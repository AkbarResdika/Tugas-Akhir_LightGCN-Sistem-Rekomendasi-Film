{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ulJnRZUXc2hb"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlnxTPumz7N8IuLDMT5PkL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkbarResdika/Tugas-Akhir_LightGCN-Sistem-Rekomendasi-Film/blob/main/Tugas_Akhir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pip Recommenders"
      ],
      "metadata": {
        "id": "ulJnRZUXc2hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fWQI-BE7E4Wh",
        "outputId": "ba071867-6bec-4a9a-96d9-883a64c541fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recommenders\n",
            "  Downloading recommenders-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting category-encoders<3,>=2.6.0 (from recommenders)\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting cornac<3,>=1.15.2 (from recommenders)\n",
            "  Downloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hyperopt<1,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.2.7)\n",
            "Requirement already satisfied: lightgbm<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (4.5.0)\n",
            "Collecting locust<3,>=2.12.2 (from recommenders)\n",
            "  Downloading locust-2.37.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting memory-profiler<1,>=0.61.0 (from recommenders)\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: nltk<4,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from recommenders) (3.9.1)\n",
            "Requirement already satisfied: notebook<8,>=6.5.5 in /usr/local/lib/python3.11/dist-packages (from recommenders) (6.5.7)\n",
            "Requirement already satisfied: numba<1,>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.60.0)\n",
            "Requirement already satisfied: pandas<3.0.0,>2.0.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (2.2.2)\n",
            "Collecting retrying<2,>=1.3.4 (from recommenders)\n",
            "  Downloading retrying-1.4.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (1.6.1)\n",
            "Collecting scikit-surprise>=1.1.3 (from recommenders)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seaborn<1,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.13.2)\n",
            "Requirement already satisfied: transformers<5,>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from recommenders) (4.52.4)\n",
            "Collecting pandera>=0.15.0 (from pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading pandera-0.24.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.4 in /usr/local/lib/python3.11/dist-packages (from recommenders) (0.14.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (2.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category-encoders<3,>=2.6.0->recommenders) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cornac<3,>=1.15.2->recommenders) (4.67.1)\n",
            "Collecting powerlaw (from cornac<3,>=1.15.2->recommenders)\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<1,>=0.2.7->recommenders) (0.10.9.7)\n",
            "Collecting configargparse>=1.7.1 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting flask-cors>=3.0.10 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting flask-login>=0.6.3 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Flask_Login-0.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: flask>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (3.1.1)\n",
            "Collecting gevent<26.0.0,>=24.10.1 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting geventhttpclient>=2.3.1 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading geventhttpclient-2.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting locust-cloud>=1.23.2 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading locust_cloud-1.24.2-py3-none-any.whl.metadata (822 bytes)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (1.1.1)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (5.9.5)\n",
            "Collecting pyzmq>=25.0.0 (from locust<3,>=2.12.2->recommenders)\n",
            "  Downloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (75.2.0)\n",
            "Requirement already satisfied: werkzeug>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from locust<3,>=2.12.2->recommenders) (3.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4,>=3.8.1->recommenders) (2024.11.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (3.1.6)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (6.4.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (25.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (5.8.1)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (6.17.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook<8,>=6.5.5->recommenders) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<1,>=0.57.0->recommenders) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>2.0.0->recommenders) (2025.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (24.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (2.11.7)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (4.4.4)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (4.14.0)\n",
            "Collecting typing_inspect>=0.6.0 (from pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting hypothesis>=6.92.7 (from pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading hypothesis-6.135.16-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1.2.0->recommenders) (3.6.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn<1,>=0.13.0->recommenders) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5,>=4.27.0->recommenders) (0.5.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.0->locust<3,>=2.12.2->recommenders) (3.0.2)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from gevent<26.0.0,>=24.10.1->locust<3,>=2.12.2->recommenders) (3.2.3)\n",
            "Collecting zope.event (from gevent<26.0.0,>=24.10.1->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope_event-5.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting zope.interface (from gevent<26.0.0,>=24.10.1->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2025.6.15)\n",
            "Collecting brotli (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from geventhttpclient>=2.3.1->locust<3,>=2.12.2->recommenders) (2.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5,>=4.27.0->recommenders) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5,>=4.27.0->recommenders) (1.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis>=6.92.7->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (2.4.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook<8,>=6.5.5->recommenders) (4.3.8)\n",
            "Collecting python-engineio>=4.12.2 (from locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading python_engineio-4.12.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting python-socketio==5.13.0 (from python-socketio[client]==5.13.0->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading python_socketio-5.13.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting bidict>=0.21.0 (from python-socketio==5.13.0->python-socketio[client]==5.13.0->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]==5.13.0->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders) (1.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn<1,>=0.13.0->recommenders) (3.2.3)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook<8,>=6.5.5->recommenders) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook<8,>=6.5.5->recommenders) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook<8,>=6.5.5->recommenders) (2.19.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook<8,>=6.5.5->recommenders) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook<8,>=6.5.5->recommenders) (4.24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->locust<3,>=2.12.2->recommenders) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->locust<3,>=2.12.2->recommenders) (3.10)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook<8,>=6.5.5->recommenders) (0.7.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing_inspect>=0.6.0->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook<8,>=6.5.5->recommenders) (21.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook<8,>=6.5.5->recommenders) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook<8,>=6.5.5->recommenders) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook<8,>=6.5.5->recommenders) (0.1.7)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from powerlaw->cornac<3,>=1.15.2->recommenders) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->pandera>=0.15.0->pandera[strategies]>=0.15.0; python_version >= \"3.9\"->recommenders) (0.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook<8,>=6.5.5->recommenders) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook<8,>=6.5.5->recommenders) (1.4.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (4.9.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook<8,>=6.5.5->recommenders) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook<8,>=6.5.5->recommenders) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook<8,>=6.5.5->recommenders) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook<8,>=6.5.5->recommenders) (1.16.0)\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.12.2->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook<8,>=6.5.5->recommenders) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook<8,>=6.5.5->recommenders) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook<8,>=6.5.5->recommenders) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (0.8.4)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook<8,>=6.5.5->recommenders) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->notebook<8,>=6.5.5->recommenders) (0.2.13)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.12.2->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook<8,>=6.5.5->recommenders) (1.3.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.12.2->locust-cloud>=1.23.2->locust<3,>=2.12.2->recommenders) (0.16.0)\n",
            "Downloading recommenders-1.2.1-py3-none-any.whl (355 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.3/355.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cornac-2.3.3-cp311-cp311-manylinux1_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locust-2.37.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Downloading pandera-0.24.0-py3-none-any.whl (267 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.0-py3-none-any.whl (11 kB)\n",
            "Downloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading Flask_Login-0.6.3-py3-none-any.whl (17 kB)\n",
            "Downloading gevent-25.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geventhttpclient-2.3.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.3/113.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypothesis-6.135.16-py3-none-any.whl (519 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locust_cloud-1.24.2-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.6/408.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_socketio-5.13.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading python_engineio-4.12.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope_event-5.1-py3-none-any.whl (6.9 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2469536 sha256=d8711b7c5fe05fcb25844b28d9cd9dd3e1eaadc3f7d49ee3924badde2c18dc87\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: brotli, zope.interface, zope.event, wsproto, retrying, pyzmq, mypy-extensions, memory-profiler, jedi, hypothesis, configargparse, bidict, typing_inspect, simple-websocket, scikit-surprise, gevent, python-engineio, powerlaw, pandera, geventhttpclient, flask-login, flask-cors, python-socketio, cornac, category-encoders, locust-cloud, locust, recommenders\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "Successfully installed bidict-0.23.1 brotli-1.1.0 category-encoders-2.8.1 configargparse-1.7.1 cornac-2.3.3 flask-cors-6.0.1 flask-login-0.6.3 gevent-25.5.1 geventhttpclient-2.3.4 hypothesis-6.135.16 jedi-0.19.2 locust-2.37.11 locust-cloud-1.24.2 memory-profiler-0.61.0 mypy-extensions-1.1.0 pandera-0.24.0 powerlaw-1.5 python-engineio-4.12.2 python-socketio-5.13.0 pyzmq-27.0.0 recommenders-1.2.1 retrying-1.4.0 scikit-surprise-1.1.4 simple-websocket-1.1.0 typing_inspect-0.9.0 wsproto-1.2.0 zope.event-5.1 zope.interface-7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "zmq"
                ]
              },
              "id": "04bb7b307bf549f5a957f3817166c6e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from recommenders.models.deeprec.deeprec_utils import prepare_hparams"
      ],
      "metadata": {
        "id": "vLoVx41LE8mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Understanding"
      ],
      "metadata": {
        "id": "79tZQQsGyhuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "WPH4gLUGyz4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_USER_COL = \"userId\"\n",
        "DEFAULT_ITEM_COL = \"movieId\"\n",
        "DEFAULT_RATING_COL = \"rating\"\n",
        "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
        "DEFAULT_PREDICTION_COL = \"prediction\"\n",
        "\n",
        "DEFAULT_HEADER = (\n",
        "    DEFAULT_USER_COL,\n",
        "    DEFAULT_ITEM_COL,\n",
        "    DEFAULT_RATING_COL,\n",
        "    DEFAULT_TIMESTAMP_COL,\n",
        ")\n",
        "\n",
        "COL_DICT = {\n",
        "    \"col_user\": DEFAULT_USER_COL,\n",
        "    \"col_item\": DEFAULT_ITEM_COL,\n",
        "    \"col_rating\": DEFAULT_RATING_COL,\n",
        "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
        "}\n",
        "\n",
        "DEFAULT_K = 50\n",
        "DEFAULT_THRESHOLD = 10\n",
        "\n",
        "# Other\n",
        "SEED = 42\n"
      ],
      "metadata": {
        "id": "yAzrhl6vC1Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Data"
      ],
      "metadata": {
        "id": "Y_w9NC_Iyt2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('/content/ratings.csv')\n",
        "movies = pd.read_csv('/content/movies.csv')"
      ],
      "metadata": {
        "id": "rl9YDvdHM35K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.head()"
      ],
      "metadata": {
        "id": "Z72t8Vey9oBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "594ca8e1-dc45-4487-c829-b666e382a179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       1        1     4.0  964982703\n",
              "1       1        3     4.0  964981247\n",
              "2       1        6     4.0  964982224\n",
              "3       1       47     5.0  964983815\n",
              "4       1       50     5.0  964982931"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87e64a6c-0036-47d6-a3bf-62801e4617fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87e64a6c-0036-47d6-a3bf-62801e4617fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87e64a6c-0036-47d6-a3bf-62801e4617fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87e64a6c-0036-47d6-a3bf-62801e4617fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-63443f74-c2da-46c5-a934-e24eb6fb404b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63443f74-c2da-46c5-a934-e24eb6fb404b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-63443f74-c2da-46c5-a934-e24eb6fb404b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ratings"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies.head()"
      ],
      "metadata": {
        "id": "CjoVhJDw9jwb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8dc32b9f-5439-44d8-fa1f-fb224594b343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movieId                               title  \\\n",
              "0        1                    Toy Story (1995)   \n",
              "1        2                      Jumanji (1995)   \n",
              "2        3             Grumpier Old Men (1995)   \n",
              "3        4            Waiting to Exhale (1995)   \n",
              "4        5  Father of the Bride Part II (1995)   \n",
              "\n",
              "                                        genres  \n",
              "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
              "1                   Adventure|Children|Fantasy  \n",
              "2                               Comedy|Romance  \n",
              "3                         Comedy|Drama|Romance  \n",
              "4                                       Comedy  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97b118c1-27c7-4b18-92fa-acd8a10280a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97b118c1-27c7-4b18-92fa-acd8a10280a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-97b118c1-27c7-4b18-92fa-acd8a10280a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-97b118c1-27c7-4b18-92fa-acd8a10280a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f54ac133-8a48-41ff-9abd-12446819d52e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f54ac133-8a48-41ff-9abd-12446819d52e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f54ac133-8a48-41ff-9abd-12446819d52e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "movies",
              "summary": "{\n  \"name\": \"movies\",\n  \"rows\": 9742,\n  \"fields\": [\n    {\n      \"column\": \"movieId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52160,\n        \"min\": 1,\n        \"max\": 193609,\n        \"num_unique_values\": 9742,\n        \"samples\": [\n          45635,\n          1373,\n          7325\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9737,\n        \"samples\": [\n          \"Teenage Mutant Ninja Turtles (2014)\",\n          \"America's Sweethearts (2001)\",\n          \"Cast Away (2000)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genres\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 951,\n        \"samples\": [\n          \"Crime|Mystery|Romance|Thriller\",\n          \"Action|Adventure|Comedy|Western\",\n          \"Crime|Drama|Musical\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_ratings = ratings.merge(movies, on='movieId', how='inner')\n",
        "movie_ratings.head(5)"
      ],
      "metadata": {
        "id": "RjONr-gVZ3TQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "bdd990f8-549e-4ac2-9931-2a0f44178833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userId  movieId  rating  timestamp                        title  \\\n",
              "0       1        1     4.0  964982703             Toy Story (1995)   \n",
              "1       1        3     4.0  964981247      Grumpier Old Men (1995)   \n",
              "2       1        6     4.0  964982224                  Heat (1995)   \n",
              "3       1       47     5.0  964983815  Seven (a.k.a. Se7en) (1995)   \n",
              "4       1       50     5.0  964982931   Usual Suspects, The (1995)   \n",
              "\n",
              "                                        genres  \n",
              "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
              "1                               Comedy|Romance  \n",
              "2                        Action|Crime|Thriller  \n",
              "3                             Mystery|Thriller  \n",
              "4                       Crime|Mystery|Thriller  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d51714fb-8b71-4848-8860-5b704faa8e00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "      <td>Heat (1995)</td>\n",
              "      <td>Action|Crime|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
              "      <td>Mystery|Thriller</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "      <td>Usual Suspects, The (1995)</td>\n",
              "      <td>Crime|Mystery|Thriller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d51714fb-8b71-4848-8860-5b704faa8e00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d51714fb-8b71-4848-8860-5b704faa8e00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d51714fb-8b71-4848-8860-5b704faa8e00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3887452-f3a7-4743-a369-8d2e503ab3f5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3887452-f3a7-4743-a369-8d2e503ab3f5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3887452-f3a7-4743-a369-8d2e503ab3f5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "movie_ratings"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_ratings = movie_ratings.drop(columns=['title','genres'])"
      ],
      "metadata": {
        "id": "Ky5Ahf3Old0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COL_USER = \"userId\"\n",
        "COL_ITEM = \"movieId\"\n",
        "COL_RATING = \"rating\"\n",
        "COL_PREDICTION = \"prediction\"\n",
        "COL_TIMESTAMP = \"timestamp\""
      ],
      "metadata": {
        "id": "XmZZD2a6r1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = movie_ratings.rename(\n",
        "    columns={\n",
        "        \"userId\": COL_USER,\n",
        "        \"movieId\": COL_ITEM,\n",
        "        \"rating\": COL_RATING,\n",
        "        \"timestamp\": COL_TIMESTAMP,\n",
        "        \"prediction\": COL_PREDICTION,\n",
        "    }\n",
        ")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "eqRs_tyfiHL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "992a366f-2644-487b-969b-5d26b556e55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userId  movieId  rating  timestamp\n",
              "0       1        1     4.0  964982703\n",
              "1       1        3     4.0  964981247\n",
              "2       1        6     4.0  964982224\n",
              "3       1       47     5.0  964983815\n",
              "4       1       50     5.0  964982931"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9f31a912-ada8-4ebc-92de-99811620668a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964981247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>964982224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964983815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>964982931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f31a912-ada8-4ebc-92de-99811620668a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9f31a912-ada8-4ebc-92de-99811620668a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9f31a912-ada8-4ebc-92de-99811620668a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-389c46ac-d289-4ea7-af0c-5316af381bcb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-389c46ac-d289-4ea7-af0c-5316af381bcb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-389c46ac-d289-4ea7-af0c-5316af381bcb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "YUzSnZeGgmgN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "2573eec7-97aa-4b66-e2c7-b176cd8cf1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              userId        movieId         rating     timestamp\n",
              "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
              "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
              "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
              "min         1.000000       1.000000       0.500000  8.281246e+08\n",
              "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
              "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
              "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
              "max       610.000000  193609.000000       5.000000  1.537799e+09"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebdc77a4-abfe-4ae9-95d1-67db75514592\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100836.000000</td>\n",
              "      <td>100836.000000</td>\n",
              "      <td>100836.000000</td>\n",
              "      <td>1.008360e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>326.127564</td>\n",
              "      <td>19435.295718</td>\n",
              "      <td>3.501557</td>\n",
              "      <td>1.205946e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>182.618491</td>\n",
              "      <td>35530.987199</td>\n",
              "      <td>1.042529</td>\n",
              "      <td>2.162610e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>8.281246e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>177.000000</td>\n",
              "      <td>1199.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.019124e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>325.000000</td>\n",
              "      <td>2991.000000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>1.186087e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>477.000000</td>\n",
              "      <td>8122.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.435994e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>610.000000</td>\n",
              "      <td>193609.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.537799e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebdc77a4-abfe-4ae9-95d1-67db75514592')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ebdc77a4-abfe-4ae9-95d1-67db75514592 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ebdc77a4-abfe-4ae9-95d1-67db75514592');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-77739dc7-9820-450a-9bf4-caed37cf1f39\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77739dc7-9820-450a-9bf4-caed37cf1f39')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-77739dc7-9820-450a-9bf4-caed37cf1f39 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"userId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35545.40345112833,\n        \"min\": 1.0,\n        \"max\": 100836.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          326.12756356856676,\n          325.0,\n          100836.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"movieId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 68674.51077210996,\n        \"min\": 1.0,\n        \"max\": 193609.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          19435.2957177992,\n          2991.0,\n          100836.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35649.87209197801,\n        \"min\": 0.5,\n        \"max\": 100836.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.501556983616962,\n          3.5,\n          100836.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 555443207.4882221,\n        \"min\": 100836.0,\n        \"max\": 1537799250.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1205946087.3684695,\n          1186086662.0,\n          100836.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=\"rating\", data=data, palette=\"viridis\")\n",
        "plt.title(\"Distribution of movie ratings\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pC33vRpih-l-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "8c21c007-f007-4214-cb94-0f2a8d9dc1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-0150dbe59df3>:1: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.countplot(x=\"rating\", data=data, palette=\"viridis\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHICAYAAABTb96uAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQk9JREFUeJzt3XlYlXX+//HXATyAC5ALW5KaC26IZYqYu/xEJScnK7dxTE0nv1ApjZozjUsbk5OmpWlNo7TomGVZLmGEiqmohZK7o4ZLKaAZ4pKAcP/+aDjjkcVbRM+Rno/rOtflue/3ue/359wHfXnfH+5jMQzDEAAAAMrk4ugGAAAAbgeEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCbgBqxfv14Wi0VTp051yP7r16+v+vXr2y2bOnWqLBaL1q9f75Cejhw5IovFoscee8wh+68I+fn5mjp1qho3bix3d3dZLBYtX77c0W1VCEd/PipSZfis4fbi5ugGAEc7cuSIGjRoYLfM09NTPj4+atasme6//34NGzZMDRs2rPB9d+3aVcnJybrdvs2oKKgdOXLEoX3cLDNmzNC0adPUuXNnPfroo6pSpYqaNm3q6LZ+kyr7Zw23F0IT8F8NGzbUH/7wB0lSbm6usrKytG3bNr3wwgt6+eWXNWHCBL300kuyWCy217Rr10779u1T7dq1HdJzUlKSQ/ZbljvvvFP79u2Tt7e3o1spt5UrV6p69epKTEyU1Wp1dDsVKiYmRgMHDtRdd93l6FZuWGX4rOH2QmgC/qtRo0YlXmbbuHGjhg4dqri4OLm6uuqFF16wratatapDz0DcjLNfN6oynJU5ceKEatWqVekCkyTVrl3bYSG/olWGzxpuL8xpAq6hY8eOSkhIkLu7u6ZPn67jx4/b1pU2p+ngwYMaPny4GjRoIHd3d9WsWVOhoaEaO3as7VKcxWJRcnKy7c9Fj6L5GVfO19i3b59+//vfq1atWrJYLLZLFSXNabrSv/71L4WEhMjDw0N33nmnxo0bp3PnztnVlDUv6+o5I0XPjx49qqNHj9r1XfT6suaZHD16VCNHjtSdd94pq9WqunXrauTIkTp27Fix2q5du8pisdjmF9WvX1/u7u5q0qSJ3nzzzVLHXJqFCxcqLCxM1atXV/Xq1RUWFqb4+Hi7mqL5Punp6XbjK+s9LmKxWNS1a1f9+OOPGjx4sGrXrq0aNWooKipK33//vSRp37596tevn2rWrKkaNWro4YcfVmZmZonbW7Fihbp16yZvb295enoqNDRUM2fO1OXLl201R48elYuLi7p3717iNvLz81W7dm0FBQWpsLDQbowlzWnauXOnBg4cqICAAFmtVtWrV09PPvmkfvrpp2uOv0jRZzI7O1sxMTEKCgqSm5ub7b1OTU1VTEyMWrZsaRtbSEiI/v73vys/P9+2nRv5rJXns3P69GmNHj1avr6+qlq1qtq2batPP/1U8fHxslgsxT4r69atU+/evRUYGCh3d3f5+fmpU6dOevvtt02/V7j9cKYJMCE4OFiPPvqo3n//fS1fvlxPPvlkqbUnTpxQu3btdOHCBUVFRWnAgAG6cOGCDh48qDfffFOvvvqq3NzcNGXKFMXHx+vo0aOaMmWK7fWtW7e2296hQ4fUvn17hYSE6LHHHtNPP/1k6gzIzJkzlZSUpAEDBigqKkpfffWVZs2apS1btmjDhg2qUqXKdb8PPj4+mjJlimbNmiVJGjt2rG1d165dy3ztf/7zH3Xs2FGnTp1S37591aJFC+3evVsLFizQihUrtHHjRjVp0qTY6wYNGqRt27apd+/ecnV11dKlSxUdHa0qVapo1KhRpvp+6qmn9MYbb+jOO+/UyJEjJUnLli3T8OHDtWPHDs2ePdtuDFePz8fHx9R+fv75Z3Xs2FH+/v4aNmyY/vOf/2jlypXav3+/PvvsM3Xq1Elt2rTRiBEjlJqaqmXLlunMmTNau3at3XZmzpypZ555RjVr1tTgwYNVrVo1ff7553rmmWf09ddf65NPPpHFYlG9evXUuXNnJScn64cfflDdunXttrN69Wr99NNPmjhxolxcyv4/8ueff65HH31ULi4uevDBBxUUFKS9e/dqzpw5WrNmjbZu3ao77rjD1PuQm5ur7t276/z58/rd734nNzc3+fn5SZL++c9/asWKFercubP69Omjixcvav369Zo0aZK++eYbLVu2zPael/ezVsTsZ+f8+fPq0qWL9u7dqw4dOqhz58764YcfNHDgQEVGRhbb7qpVq9S3b1/5+PjowQcfVEBAgE6dOqXvvvtO77//vkaPHm2qP9yGDOA3Lj093ZBkREZGlln3r3/9y5BkDB061LZs3bp1hiRjypQptmWvv/66IcmYNWtWsW389NNPds+7dOlilPZjWNSXJGPy5Mkl1tSrV8+oV6+e3bIpU6YYkgyr1Wp89913tuWFhYXG4MGDDUnGq6++WuYYru5h2LBh19zvtV7TrVs3Q5Lx1ltv2S2fO3euIcno3r273fKi9yYsLMw4e/asbfn+/fsNNzc3Izg4uMT9Xy05OdmQZDRr1szIzs62LT9z5ozRpEkTQ5KxYcMG0+MrTdGxGjdunN3yMWPGGJIMHx8fu89EYWGh0adPH0OSkZqaalt+6NAhw83NzfD19TWOHTtmW37p0iWjY8eOhiTjvffesy1/5513DEnGK6+8Uqyn/v37G5KM3bt325YVfT7WrVtnW3b69GnDy8vLuPPOO40jR47YbePf//63IcmIiYkx9T7Uq1fP9vN08eLFYuuPHj1qXL582W5ZYWGhMWLECEOSsXHjxmLbu97P2vV+dp577jlDkjF69Gi75V999ZXtuC5cuNC2/KGHHjIkGWlpacV6On36dIm9onLg8hxgUmBgoKRfT+Ob4enpWWxZzZo1r3u//v7++utf/3rdr/vjH/+oVq1a2Z5bLBa9/PLLcnV1LXap4WY7duyY1q1bp+bNmxc7O/TEE0+oadOmWrt2rd2lzyJxcXHy8vKyPQ8ODtb999+vAwcOFLvUWJJ3331X0q+Xpa6cMHzHHXfYzvBV1PtRvXp1vfjii3bLBg0aJEmqVauWnnrqKdtyi8WigQMHSpK+++472/LFixfr8uXLeuaZZxQUFGRb7u7urldeeaVYvw8//LA8PDz0wQcf2O03OztbK1euVOvWrdWiRYsy+37vvfeUk5OjuLg41atXz27dwIEDde+992rJkiXXGr6d6dOnl/gzcNddd8nV1dVumcViUXR0tCTpq6++uq79lMXsZ+eDDz6Q1WrV888/b/f6Hj16qGfPnqVuv6Tx1apVqwI6h7Pi8hxQwfr27atJkyYpOjpaSUlJ6tWrl7p06aK77767XNsLDQ0t14TkTp06FVtWr149BQUFac+ePcrLy7tlE53T0tIkSV26dLH77UNJcnFxUefOnbV//36lpaXZBQVJatOmTbHtFV2Gys7OVo0aNcrc944dOySVfEmnW7dudv3dqMaNG6tq1ap2ywICAiRJrVq1Kjb2onUnTpww1W94eLg8PDzs+vX29tbvfvc7LV26VN99951CQ0MlSR999JFyc3M1dOjQa/a9ZcsWSdLWrVt1+PDhYusvXbqk06dP6/Tp06YmkXt4eCgkJKTEdXl5eZozZ46WLFmi/fv36/z583a33LjyvbhRZj47OTk5OnLkiJo3b267hHil+++/X19++aXdsoEDB+qTTz5R+/btNXjwYPXo0UOdOnWqNBPsUTpCE2BS0V/mderUKbOufv362rJli6ZOnarVq1dr6dKlkqSmTZvq+eef1yOPPHJd+y3pL/IbeZ2fn5+OHDmic+fO3bL/Fefk5JTZU1F4KKq70pVnCoq4uf36V1dBQYGpfbu4uJR43Pz8/GSxWErcb3mU1WtZ666cAF3We2WxWOTn56cff/zRbvnQoUO1dOlSffDBB7bQ9P7778vV1VWDBw++Zt9nzpyRJM2dO7fMugsXLpgKBr6+vsUCYpGHH35YK1asUJMmTTRgwAD5+vqqSpUqys7O1uzZs5Wbm3vN7Ztl5rNT9H77+vqWuI2SjsMjjzyi5cuXa+bMmZo/f77mzp0ri8Wibt26acaMGcXmJaLy4PIcYFLRbxu1bdv2mrUtW7bUxx9/rDNnziglJUWTJ09WRkaGBgwYoE2bNl3Xfkv7x+daSvutrMzMTFksFtsZmqIJwlf+VlaRs2fPlmvfVyv6x6u0njIyMuzqKpKXl5cKCwt16tSpYuuysrJkGMZN2W95lfVeGYahzMzMYv326tVLderU0b///W8VFhbqyJEj2rhxoyIiIuTv7296n7t27ZJhGKU+rr50V5rSPrPffPONVqxYocjISO3du1f//Oc/9dJLL2nq1Km2S5W3WtHYs7KySlxf2mf2wQcfVHJysn7++Wd98cUXevzxx7V+/Xr16tVL2dnZN6tdOBihCTDhP//5j5YuXSp3d3f9/ve/N/26KlWqqH379po2bZpef/11GYahlStX2tYXze0wc8bken399dfFlh09elTHjx9XixYtbJfmin4j6uqzF9L/LhVdzdXV9bp6Lvqf94YNG4rd/dwwDG3YsMGuriLdc889klTir9gXLXOmMwNl9bt161ZdunSpWL9ubm4aOHCgfvzxR61bt06LFi2SYRi2m7VeS1hYmCQpJSXlhnq/lqJLf1FRUcXmNZX0eZWu/7N2vby8vFS/fn0dOnSoxOC0efPmMl9fo0YN9erVS2+//bYee+wxZWZmauvWrTerXTgYoQm4hk2bNikyMlK5ubl69tlndeedd5ZZn5qaWuLlnqL/sXp4eNiWFU0ML2kC9I167733tHPnTttzwzD0l7/8RQUFBXb3tQkODlaNGjX0+eef2y7TFPV79aTmK/s+ffq0Ll26ZKqXu+66S926ddOePXu0YMECu3Vvv/229u3bp+7duxebz1QRhg0bJkmaNm2a3XE5e/aspk2bZlfjDAYPHiw3NzfNnDnTbn5PXl6eJk6cKEkl3gOraO7S+++/r/fff1/VqlUzHfCHDx+uGjVq6K9//av27NlTbP3Fixdt855uRNGZqo0bN9ot37Nnj+Li4kp8zfV+1spjyJAhysvLs7v1h/RrcF2zZk2x+g0bNpQY5IpC15U/46hcmNME/NehQ4dsN83Ly8uzfY3Krl275Orqqueee67YX6olef/99/XWW2+pc+fOatiwoby8vLR3716tXr1aNWvW1PDhw2213bt318cff6z+/furd+/e8vDwUGhoqPr27XvD44mMjFR4eLgGDhyoOnXqKCkpSd9++63at29vd58pq9WqJ598Ui+//LLuvfdePfjggzp37pxWrFihLl26lDgxuHv37vr222/Vu3dvderUSVarVZ07d1bnzp1L7WfevHnq2LGjRo0apRUrVqh58+bas2ePPv/8c9WpU0fz5s274TGXpHPnznryySf1xhtvqGXLlurfv78Mw9CyZcv0ww8/6Kmnniqz71utYcOGeuWVV/TMM8+oVatWevTRR1WtWjWtWLFCBw4c0IMPPljiGaS2bdsqODhYixcvVn5+voYOHapq1aqZ2mfRpb1HHnlEoaGh6tWrl5o2barc3FwdOXJEycnJ6tChgxISEm5obO3atVO7du20dOlSnTx5Uu3bt9exY8f0+eefKyoqSh9//HGx15Tns3a9Jk6cqGXLlmn+/PnavXu3OnXqpB9++EFLly5V3759tWLFCrv7XD311FM6ceKEOnbsqPr168tisWjjxo3atm2b2rdvr44dO1ZYb3Ayt/4uB4BzufJ+SEUPT09PIyAgwOjWrZvxt7/9zTh06FCJry3pHkdbtmwx/vSnPxktW7Y0fHx8DE9PT6Nx48ZGTEyMcfToUbvX5+fnGxMmTDDuuusuw83Nze6eM6Xdg+ZKZd2nad26dcY///lPo0WLFoa7u7sREBBgPP3000ZOTk6x7RQUFBhTp041goKCDKvVajRp0sSYPXu28f3335fYw7lz54xRo0YZAQEBhqurq917UFbfR44cMYYPH24EBAQYbm5uRkBAgDF8+PBi9wYyjLLvYTVs2DBDkpGenl7qe3O1BQsWGG3btjWqVq1qVK1a1Wjbtq2xYMGCEmvLe5+mLl26FFte1vtR1j2yPvvsM6NLly5GjRo1DHd3dyMkJMSYMWOGkZ+fX2oPL774ou0zvGbNmhJrSrpPU5H9+/cbI0eONOrVq2dYrVbjjjvuMEJCQoynnnrK2LZtW6n7vdK13rusrCxjxIgRRmBgoOHh4WGEhIQYc+fOrdDPWnk+O1lZWcbIkSON2rVrGx4eHkabNm2MTz75xHj11VcNScann35qq12yZInx6KOPGg0bNjSqVq1qeHt7G6GhocYrr7xinDt3ztT7hNuTxTBus69XBwDgFvnDH/6gRYsWae/evWrWrJmj24GDMacJAPCbd/LkyWLLkpOTtWTJEgUHBxOYIIk5TQAAqE+fPvL09FTr1q1VrVo17d27VwkJCXJ1ddUbb7zh6PbgJLg8BwD4zZs1a5YWLVqkw4cP69y5c/Lx8dH999+vSZMm2W7JABCaAAAATGBOEwAAgAmEJgAAABOYCF5BCgsLdeLECdWoUaPc3xUGAABuLcMwdO7cOQUGBtrdxLQkhKYKcuLEiZvyFRAAAODmO378uOrWrVtmDaGpghR9Y/zx48ed6hvTAQBA6XJychQUFGT7d7wshKYKUnRJzsvLi9AEAMBtxszUGiaCAwAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmuDm6AQAAbtTwL2Id3UKZFvae6egWUAE40wQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwASHhqa4uDi1bdtWNWrUkK+vr/r166cDBw7Y1XTt2lUWi8Xu8cQTT9jVHDt2TFFRUapatap8fX01fvx4Xb582a5m/fr1uvfee+Xu7q5GjRopPj6+WD9z585V/fr15eHhobCwMG3btq3CxwwAAG5PDg1NycnJio6O1pYtW5SYmKj8/Hz17NlTFy5csKsbNWqUTp48aXtMnz7dtq6goEBRUVHKy8vT5s2b9e677yo+Pl6TJ0+21aSnpysqKkrdunVTWlqaxo4dq8cff1xr1qyx1Xz44YeKjY3VlClTtH37doWGhioyMlJZWVk3/40AAABOz2IYhuHoJoqcOnVKvr6+Sk5OVufOnSX9eqapdevWmjVrVomv+eKLL/TAAw/oxIkT8vPzkyTNnz9fEydO1KlTp2S1WjVx4kStWrVKu3fvtr1u4MCBys7OVkJCgiQpLCxMbdu21Zw5cyRJhYWFCgoK0pNPPqlnn332mr3n5OTI29tbZ8+elZeX1428DQCA6zT8i1hHt1Cmhb1nOroFlOJ6/v12qjlNZ8+elSTVrFnTbvmiRYtUu3ZttWzZUpMmTdLFixdt61JSUhQSEmILTJIUGRmpnJwc7dmzx1YTERFht83IyEilpKRIkvLy8pSammpX4+LiooiICFsNAAD4bXNzdANFCgsLNXbsWN1///1q2bKlbfngwYNVr149BQYGaufOnZo4caIOHDigTz75RJKUkZFhF5gk2Z5nZGSUWZOTk6NffvlFP//8swoKCkqs2b9/f4n95ubmKjc31/Y8JyennCMHAAC3A6cJTdHR0dq9e7c2btxot3z06NG2P4eEhCggIEA9evTQ4cOH1bBhw1vdpk1cXJymTZvmsP0DAIBbyykuz8XExGjlypVat26d6tatW2ZtWFiYJOnQoUOSJH9/f2VmZtrVFD339/cvs8bLy0uenp6qXbu2XF1dS6wp2sbVJk2apLNnz9oex48fNzlaAABwO3JoaDIMQzExMfr000+1du1aNWjQ4JqvSUtLkyQFBARIksLDw7Vr1y6733JLTEyUl5eXmjdvbqtJSkqy205iYqLCw8MlSVarVW3atLGrKSwsVFJSkq3mau7u7vLy8rJ7AACAysuhl+eio6O1ePFiffbZZ6pRo4ZtDpK3t7c8PT11+PBhLV68WH369FGtWrW0c+dOjRs3Tp07d1arVq0kST179lTz5s01dOhQTZ8+XRkZGXruuecUHR0td3d3SdITTzyhOXPmaMKECRoxYoTWrl2rpUuXatWqVbZeYmNjNWzYMN13331q166dZs2apQsXLmj48OG3/o0BAABOx6Ghad68eZJ+va3AlRYuXKjHHntMVqtVX331lS3ABAUFqX///nruuedsta6urlq5cqXGjBmj8PBwVatWTcOGDdPzzz9vq2nQoIFWrVqlcePGafbs2apbt67eeecdRUZG2moGDBigU6dOafLkycrIyFDr1q2VkJBQbHI4AAD4bXKq+zTdzrhPEwA4DvdpQnndtvdpAgAAcFaEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGCCQ0NTXFyc2rZtqxo1asjX11f9+vXTgQMH7GouXbqk6Oho1apVS9WrV1f//v2VmZlpV3Ps2DFFRUWpatWq8vX11fjx43X58mW7mvXr1+vee++Vu7u7GjVqpPj4+GL9zJ07V/Xr15eHh4fCwsK0bdu2Ch8zAAC4PTk0NCUnJys6OlpbtmxRYmKi8vPz1bNnT124cMFWM27cOK1YsUIfffSRkpOTdeLECT300EO29QUFBYqKilJeXp42b96sd999V/Hx8Zo8ebKtJj09XVFRUerWrZvS0tI0duxYPf7441qzZo2t5sMPP1RsbKymTJmi7du3KzQ0VJGRkcrKyro1bwYAAHBqFsMwDEc3UeTUqVPy9fVVcnKyOnfurLNnz6pOnTpavHixHn74YUnS/v371axZM6WkpKh9+/b64osv9MADD+jEiRPy8/OTJM2fP18TJ07UqVOnZLVaNXHiRK1atUq7d++27WvgwIHKzs5WQkKCJCksLExt27bVnDlzJEmFhYUKCgrSk08+qWefffaavefk5Mjb21tnz56Vl5dXRb81AIAyDP8i1tEtlGlh75mObgGluJ5/v51qTtPZs2clSTVr1pQkpaamKj8/XxEREbaapk2b6q677lJKSookKSUlRSEhIbbAJEmRkZHKycnRnj17bDVXbqOopmgbeXl5Sk1NtatxcXFRRESEreZqubm5ysnJsXsAAIDKy83RDRQpLCzU2LFjdf/996tly5aSpIyMDFmtVvn4+NjV+vn5KSMjw1ZzZWAqWl+0rqyanJwc/fLLL/r5559VUFBQYs3+/ftL7DcuLk7Tpk0r32ABVAqhr05xdAtl+u7P/B0FVCSnOdMUHR2t3bt3a8mSJY5uxZRJkybp7Nmztsfx48cd3RIAALiJnOJMU0xMjFauXKkNGzaobt26tuX+/v7Ky8tTdna23dmmzMxM+fv722qu/i23ot+uu7Lm6t+4y8zMlJeXlzw9PeXq6ipXV9cSa4q2cTV3d3e5u7uXb8AAAOC249AzTYZhKCYmRp9++qnWrl2rBg0a2K1v06aNqlSpoqSkJNuyAwcO6NixYwoPD5ckhYeHa9euXXa/5ZaYmCgvLy81b97cVnPlNopqirZhtVrVpk0bu5rCwkIlJSXZagAAwG+bQ880RUdHa/Hixfrss89Uo0YN2xwkb29veXp6ytvbWyNHjlRsbKxq1qwpLy8vPfnkkwoPD1f79u0lST179lTz5s01dOhQTZ8+XRkZGXruuecUHR1tOxP0xBNPaM6cOZowYYJGjBihtWvXaunSpVq1apWtl9jYWA0bNkz33Xef2rVrp1mzZunChQsaPnz4rX9jAACA03FoaJo3b54kqWvXrnbLFy5cqMcee0yS9Nprr8nFxUX9+/dXbm6uIiMj9eabb9pqXV1dtXLlSo0ZM0bh4eGqVq2ahg0bpueff95W06BBA61atUrjxo3T7NmzVbduXb3zzjuKjIy01QwYMECnTp3S5MmTlZGRodatWyshIaHY5HAAAPDb5FT3abqdcZ8m4LeH355zHtynCeV1296nCQAAwFkRmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMCEcoWm7t27Kzs7u9jynJwcde/e/UZ7AgAAcDrlCk3r169XXl5eseWXLl3S119/bXo7GzZsUN++fRUYGCiLxaLly5fbrX/sscdksVjsHr169bKrOXPmjIYMGSIvLy/5+Pho5MiROn/+vF3Nzp071alTJ3l4eCgoKEjTp08v1stHH32kpk2bysPDQyEhIVq9erXpcQAAgMrP7XqKd+7cafvz3r17lZGRYXteUFCghIQE3Xnnnaa3d+HCBYWGhmrEiBF66KGHSqzp1auXFi5caHvu7u5ut37IkCE6efKkEhMTlZ+fr+HDh2v06NFavHixpF/PfvXs2VMRERGaP3++du3apREjRsjHx0ejR4+WJG3evFmDBg1SXFycHnjgAS1evFj9+vXT9u3b1bJlS9PjAQAAldd1habWrVvbzviUdBnO09NTb7zxhunt9e7dW7179y6zxt3dXf7+/iWu27dvnxISEvTNN9/ovvvukyS98cYb6tOnj1599VUFBgZq0aJFysvL04IFC2S1WtWiRQulpaVp5syZttA0e/Zs9erVS+PHj5ckvfDCC0pMTNScOXM0f/580+MBAACV13VdnktPT9fhw4dlGIa2bdum9PR02+PHH39UTk6ORowYUaENrl+/Xr6+vgoODtaYMWP0008/2dalpKTIx8fHFpgkKSIiQi4uLtq6dautpnPnzrJarbaayMhIHThwQD///LOtJiIiwm6/kZGRSklJKbWv3Nxc5eTk2D0AAEDldV1nmurVqydJKiwsvCnNXK1Xr1566KGH1KBBAx0+fFh/+ctf1Lt3b6WkpMjV1VUZGRny9fW1e42bm5tq1qxpu3SYkZGhBg0a2NX4+fnZ1t1xxx3KyMiwLbuy5srLj1eLi4vTtGnTKmKYAADgNnBdoelKBw8e1Lp165SVlVUsRE2ePPmGG5OkgQMH2v4cEhKiVq1aqWHDhlq/fr169OhRIfsor0mTJik2Ntb2PCcnR0FBQQ7sCAAA3EzlCk3//Oc/NWbMGNWuXVv+/v6yWCy2dRaLpcJC09Xuvvtu1a5dW4cOHVKPHj3k7++vrKwsu5rLly/rzJkztnlQ/v7+yszMtKspen6tmtLmUkm/zrW6elI6AACovMp1y4EXX3xRL730kjIyMpSWlqYdO3bYHtu3b6/oHm1++OEH/fTTTwoICJAkhYeHKzs7W6mpqbaatWvXqrCwUGFhYbaaDRs2KD8/31aTmJio4OBg3XHHHbaapKQku30lJiYqPDz8po0FAADcXsoVmn7++Wc98sgjN7zz8+fPKy0tTWlpaZJ+nWielpamY8eO6fz58xo/fry2bNmiI0eOKCkpSQ8++KAaNWqkyMhISVKzZs3Uq1cvjRo1Stu2bdOmTZsUExOjgQMHKjAwUJI0ePBgWa1WjRw5Unv27NGHH36o2bNn211ae/rpp5WQkKAZM2Zo//79mjp1qr799lvFxMTc8BgBAEDlUK7Q9Mgjj+jLL7+84Z1/++23uueee3TPPfdIkmJjY3XPPfdo8uTJcnV11c6dO/W73/1OTZo00ciRI9WmTRt9/fXXdpfFFi1apKZNm6pHjx7q06ePOnbsqLffftu23tvbW19++aXS09PVpk0bPfPMM5o8ebLtdgOS1KFDBy1evFhvv/22QkND9fHHH2v58uXcowkAANiUa05To0aN9Le//U1btmxRSEiIqlSpYrf+qaeeMrWdrl27yjCMUtevWbPmmtuoWbOm7UaWpWnVqtU171T+yCOPVMjZMwAAUDmVKzS9/fbbql69upKTk5WcnGy3zmKxmA5NAAAAt4tyhab09PSK7gMAAMCplWtOEwAAwG9Nuc40XeurUhYsWFCuZgAAAJxVuUJT0Xe2FcnPz9fu3buVnZ1d4hf5AgAA3O7KFZo+/fTTYssKCws1ZswYNWzY8IabAgAAcDYVNqfJxcVFsbGxeu211ypqkwAAAE6jQieCHz58WJcvX67ITQIAADiFcl2eu/IrSCTJMAydPHlSq1at0rBhwyqkMQAAAGdSrtC0Y8cOu+cuLi6qU6eOZsyYcc3frAMAALgdlSs0rVu3rqL7AAAAcGrlCk1FTp06pQMHDkiSgoODVadOnQppCgAAwNmUayL4hQsXNGLECAUEBKhz587q3LmzAgMDNXLkSF28eLGiewQAAHC4coWm2NhYJScna8WKFcrOzlZ2drY+++wzJScn65lnnqnoHgEAAByuXJfnli1bpo8//lhdu3a1LevTp488PT316KOPat68eRXVHwAAgFMo15mmixcvys/Pr9hyX19fLs8BAIBKqVxnmsLDwzVlyhS999578vDwkCT98ssvmjZtmsLDwyu0QQAAcHtZsbWjo1soVd+wjeV+bblC06xZs9SrVy/VrVtXoaGhkqTvvvtO7u7u+vLLL8vdDADg1usY/1dHt1CqjY+95OgWAJtyhaaQkBAdPHhQixYt0v79+yVJgwYN0pAhQ+Tp6VmhDQIAADiDcoWmuLg4+fn5adSoUXbLFyxYoFOnTmnixIkV0hwAAICzKNdE8LfeektNmzYttrxFixaaP3/+DTcFAADgbMoVmjIyMhQQEFBseZ06dXTy5MkbbgoAAMDZlCs0BQUFadOmTcWWb9q0SYGBgTfcFAAAgLMp15ymUaNGaezYscrPz1f37t0lSUlJSZowYQJ3BAcAAJVSuULT+PHj9dNPP+n//u//lJeXJ0ny8PDQxIkTNWnSpAptEAAAwBmUKzRZLBa98sor+tvf/qZ9+/bJ09NTjRs3lru7e0X3BwAA4BTKFZqKVK9eXW3btq2oXgAAAJxWuSaCAwAA/NYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJjg0NC0YcMG9e3bV4GBgbJYLFq+fLndesMwNHnyZAUEBMjT01MRERE6ePCgXc2ZM2c0ZMgQeXl5ycfHRyNHjtT58+ftanbu3KlOnTrJw8NDQUFBmj59erFePvroIzVt2lQeHh4KCQnR6tWrK3y8AADg9uXQ0HThwgWFhoZq7ty5Ja6fPn26Xn/9dc2fP19bt25VtWrVFBkZqUuXLtlqhgwZoj179igxMVErV67Uhg0bNHr0aNv6nJwc9ezZU/Xq1VNqaqr+8Y9/aOrUqXr77bdtNZs3b9agQYM0cuRI7dixQ/369VO/fv20e/fumzd4AABwW3Fz5M579+6t3r17l7jOMAzNmjVLzz33nB588EFJ0nvvvSc/Pz8tX75cAwcO1L59+5SQkKBvvvlG9913nyTpjTfeUJ8+ffTqq68qMDBQixYtUl5enhYsWCCr1aoWLVooLS1NM2fOtIWr2bNnq1evXho/frwk6YUXXlBiYqLmzJmj+fPn34J3AgAAODunndOUnp6ujIwMRURE2JZ5e3srLCxMKSkpkqSUlBT5+PjYApMkRUREyMXFRVu3brXVdO7cWVar1VYTGRmpAwcO6Oeff7bVXLmfopqi/ZQkNzdXOTk5dg8AAFB5OW1oysjIkCT5+fnZLffz87Oty8jIkK+vr916Nzc31axZ066mpG1cuY/SaorWlyQuLk7e3t62R1BQ0PUOEQAA3EacNjQ5u0mTJuns2bO2x/Hjxx3dEgAAuImcNjT5+/tLkjIzM+2WZ2Zm2tb5+/srKyvLbv3ly5d15swZu5qStnHlPkqrKVpfEnd3d3l5edk9AABA5eW0oalBgwby9/dXUlKSbVlOTo62bt2q8PBwSVJ4eLiys7OVmppqq1m7dq0KCwsVFhZmq9mwYYPy8/NtNYmJiQoODtYdd9xhq7lyP0U1RfsBAABwaGg6f/680tLSlJaWJunXyd9paWk6duyYLBaLxo4dqxdffFGff/65du3apT/+8Y8KDAxUv379JEnNmjVTr169NGrUKG3btk2bNm1STEyMBg4cqMDAQEnS4MGDZbVaNXLkSO3Zs0cffvihZs+erdjYWFsfTz/9tBISEjRjxgzt379fU6dO1bfffquYmJhb/ZYAAAAn5dBbDnz77bfq1q2b7XlRkBk2bJji4+M1YcIEXbhwQaNHj1Z2drY6duyohIQEeXh42F6zaNEixcTEqEePHnJxcVH//v31+uuv29Z7e3vryy+/VHR0tNq0aaPatWtr8uTJdvdy6tChgxYvXqznnntOf/nLX9S4cWMtX75cLVu2vAXvAgAAuB04NDR17dpVhmGUut5isej555/X888/X2pNzZo1tXjx4jL306pVK3399ddl1jzyyCN65JFHym4YAAD8ZjntnCYAAABnQmgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwASHfmEvAAD4n1c2/sHRLZRpYscPHN2CQ3GmCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMcHN0AwCuT4eYFxzdQpk2z/mbo1sAgJuCM00AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmuDm6AQC/TW3++ryjWyhV6kuTHd0CACfEmSYAAAATCE0AAAAmOHVomjp1qiwWi92jadOmtvWXLl1SdHS0atWqperVq6t///7KzMy028axY8cUFRWlqlWrytfXV+PHj9fly5ftatavX697771X7u7uatSokeLj42/F8AAAwG3EqUOTJLVo0UInT560PTZu3GhbN27cOK1YsUIfffSRkpOTdeLECT300EO29QUFBYqKilJeXp42b96sd999V/Hx8Zo8+X/zFdLT0xUVFaVu3bopLS1NY8eO1eOPP641a9bc0nECAADn5vQTwd3c3OTv719s+dmzZ/Wvf/1LixcvVvfu3SVJCxcuVLNmzbRlyxa1b99eX375pfbu3auvvvpKfn5+at26tV544QVNnDhRU6dOldVq1fz589WgQQPNmDFDktSsWTNt3LhRr732miIjI2/pWAEAgPNy+jNNBw8eVGBgoO6++24NGTJEx44dkySlpqYqPz9fERERttqmTZvqrrvuUkpKiiQpJSVFISEh8vPzs9VERkYqJydHe/bssdVcuY2imqJtlCY3N1c5OTl2DwAAUHk5dWgKCwtTfHy8EhISNG/ePKWnp6tTp046d+6cMjIyZLVa5ePjY/caPz8/ZWRkSJIyMjLsAlPR+qJ1ZdXk5OTol19+KbW3uLg4eXt72x5BQUE3OlwAAODEnPryXO/evW1/btWqlcLCwlSvXj0tXbpUnp6eDuxMmjRpkmJjY23Pc3JyCE4AAFRiTn2m6Wo+Pj5q0qSJDh06JH9/f+Xl5Sk7O9uuJjMz0zYHyt/fv9hv0xU9v1aNl5dXmcHM3d1dXl5edg8AAFB53Vah6fz58zp8+LACAgLUpk0bValSRUlJSbb1Bw4c0LFjxxQeHi5JCg8P165du5SVlWWrSUxMlJeXl5o3b26ruXIbRTVF2wAAAJCcPDT9+c9/VnJyso4cOaLNmzfr97//vVxdXTVo0CB5e3tr5MiRio2N1bp165Samqrhw4crPDxc7du3lyT17NlTzZs319ChQ/Xdd99pzZo1eu655xQdHS13d3dJ0hNPPKHvv/9eEyZM0P79+/Xmm29q6dKlGjdunCOHDgAAnIxTz2n64YcfNGjQIP3000+qU6eOOnbsqC1btqhOnTqSpNdee00uLi7q37+/cnNzFRkZqTfffNP2eldXV61cuVJjxoxReHi4qlWrpmHDhun55//3nVcNGjTQqlWrNG7cOM2ePVt169bVO++8w+0GAACAHacOTUuWLClzvYeHh+bOnau5c+eWWlOvXj2tXr26zO107dpVO3bsKFePAADgt8GpL88BAAA4C0ITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwAAgAmEJgAAABMITQAAACYQmgAAAEwgNAEAAJhAaAIAADCB0AQAAGACoQkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACY4OboBnB7iOr1N0e3UKpVCS84ugUAwG8AZ5oAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAJf2IvfjIghzv3Fvl8tct4vRQYAcKYJAADAFEITAACACYQmAAAAEwhNAAAAJhCaAAAATOC3526yPiFjHN1CmVbvmufoFgAAuC1wpgkAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJoAAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCarjJ37lzVr19fHh4eCgsL07Zt2xzdEgAAcAKEpit8+OGHio2N1ZQpU7R9+3aFhoYqMjJSWVlZjm4NAAA4GKHpCjNnztSoUaM0fPhwNW/eXPPnz1fVqlW1YMECR7cGAAAcjND0X3l5eUpNTVVERIRtmYuLiyIiIpSSkuLAzgAAgDNwc3QDzuL06dMqKCiQn5+f3XI/Pz/t37+/WH1ubq5yc3Ntz8+ePStJysnJsavLL8i7Cd1WnKv7LU3+5dxrFzmI2TFczr90kzu5MabHkVc5xlGQ67zjMD2GS877cyFdx2fqF+cdh9kx5F103jFI5sdx6UL+Te7kxpgdx8ULl29yJ+V39RiKnhuGce0XGzAMwzB+/PFHQ5KxefNmu+Xjx4832rVrV6x+ypQphiQePHjw4MGDRyV4HD9+/JpZgTNN/1W7dm25uroqMzPTbnlmZqb8/f2L1U+aNEmxsbG254WFhTpz5oxq1aoli8VyU3rMyclRUFCQjh8/Li8vr5uyj1uhMoyjMoxBqhzjqAxjkBiHM6kMY5AqxzhuxRgMw9C5c+cUGBh4zVpC039ZrVa1adNGSUlJ6tevn6Rfg1BSUpJiYmKK1bu7u8vd3d1umY+Pzy3oVPLy8rptfwCuVBnGURnGIFWOcVSGMUiMw5lUhjFIlWMcN3sM3t7epuoITVeIjY3VsGHDdN9996ldu3aaNWuWLly4oOHDhzu6NQAA4GCEpisMGDBAp06d0uTJk5WRkaHWrVsrISGh2ORwAADw20NoukpMTEyJl+Ocgbu7u6ZMmVLssuDtpjKMozKMQaoc46gMY5AYhzOpDGOQKsc4nG0MFsMw8zt2AAAAv23c3BIAAMAEQhMAAIAJhCYAAAATCE0AAAAmEJqczNy5c1W/fn15eHgoLCxM27ZtK7U2Pj5eFovF7uHh4XELuy1uw4YN6tu3rwIDA2WxWLR8+fJrvmb9+vW699575e7urkaNGik+Pv6m93kt1zuO9evXFzsWFotFGRkZt6bhEsTFxalt27aqUaOGfH191a9fPx04cOCar/voo4/UtGlTeXh4KCQkRKtXr74F3ZasPGNwxp+LefPmqVWrVrYb9IWHh+uLL74o8zXOdByKXO84nPFYXO3vf/+7LBaLxo4dW2adMx6PK5kZh7Mdj6lTpxbrp2nTpmW+xtHHgdDkRD788EPFxsZqypQp2r59u0JDQxUZGamsrKxSX+Pl5aWTJ0/aHkePHr2FHRd34cIFhYaGau7cuabq09PTFRUVpW7duiktLU1jx47V448/rjVr1tzkTst2veMocuDAAbvj4evre5M6vLbk5GRFR0dry5YtSkxMVH5+vnr27KkLFy6U+prNmzdr0KBBGjlypHbs2KF+/fqpX79+2r179y3s/H/KMwbJ+X4u6tatq7///e9KTU3Vt99+q+7du+vBBx/Unj17Sqx3tuNQ5HrHITnfsbjSN998o7feekutWrUqs85Zj0cRs+OQnO94tGjRwq6fjRs3llrrFMehYr7uFhWhXbt2RnR0tO15QUGBERgYaMTFxZVYv3DhQsPb2/sWdXf9JBmffvppmTUTJkwwWrRoYbdswIABRmRk5E3s7PqYGce6desMScbPP/98S3oqj6ysLEOSkZycXGrNo48+akRFRdktCwsLM/70pz/d7PZMMTMGZ/+5KHLHHXcY77zzTonrnP04XKmscTjzsTh37pzRuHFjIzEx0ejSpYvx9NNPl1rrzMfjesbhbMdjypQpRmhoqOl6ZzgOnGlyEnl5eUpNTVVERIRtmYuLiyIiIpSSklLq686fP6969eopKCjomv/jc0YpKSl2Y5akyMjIMsfszFq3bq2AgAD9v//3/7Rp0yZHt2Pn7NmzkqSaNWuWWuPsx8PMGCTn/rkoKCjQkiVLdOHCBYWHh5dY4+zHQTI3Dsl5j0V0dLSioqKKvc8lcebjcT3jkJzveBw8eFCBgYG6++67NWTIEB07dqzUWmc4DoQmJ3H69GkVFBQU+8oWPz+/UufFBAcHa8GCBfrss8/0wQcfqLCwUB06dNAPP/xwK1quEBkZGSWOOScnR7/88ouDurp+AQEBmj9/vpYtW6Zly5YpKChIXbt21fbt2x3dmqRfv3x67Nixuv/++9WyZctS60o7Ho6cm1XE7Bic9edi165dql69utzd3fXEE0/o008/VfPmzUusdebjcD3jcNZjsWTJEm3fvl1xcXGm6p31eFzvOJzteISFhSk+Pl4JCQmaN2+e0tPT1alTJ507d67Eemc4DnyNym0sPDzc7n94HTp0ULNmzfTWW2/phRdecGBnvz3BwcEKDg62Pe/QoYMOHz6s1157Te+//74DO/tVdHS0du/eXeZ8AWdndgzO+nMRHBystLQ0nT17Vh9//LGGDRum5OTkUgOHs7qecTjjsTh+/LiefvppJSYmOt2k9OtRnnE42/Ho3bu37c+tWrVSWFiY6tWrp6VLl2rkyJG3vB8zCE1Oonbt2nJ1dVVmZqbd8szMTPn7+5vaRpUqVXTPPffo0KFDN6PFm8Lf37/EMXt5ecnT09NBXVWMdu3aOUVIiYmJ0cqVK7VhwwbVrVu3zNrSjofZz+DNcj1juJqz/FxYrVY1atRIktSmTRt98803mj17tt56661itc56HKTrG8fVnOFYpKamKisrS/fee69tWUFBgTZs2KA5c+YoNzdXrq6udq9xxuNRnnFczRmOx5V8fHzUpEmTUvtxhuPA5TknYbVa1aZNGyUlJdmWFRYWKikpqcz5AlcqKCjQrl27FBAQcLParHDh4eF2Y5akxMRE02N2ZmlpaQ49FoZhKCYmRp9++qnWrl2rBg0aXPM1znY8yjOGqznrz0VhYaFyc3NLXOdsx6EsZY3jas5wLHr06KFdu3YpLS3N9rjvvvs0ZMgQpaWllRg0nPF4lGccV3OG43Gl8+fP6/Dhw6X24xTH4ZZNOcc1LVmyxHB3dzfi4+ONvXv3GqNHjzZ8fHyMjIwMwzAMY+jQocazzz5rq582bZqxZs0a4/Dhw0ZqaqoxcOBAw8PDw9izZ4+jhmCcO3fO2LFjh7Fjxw5DkjFz5kxjx44dxtGjRw3DMIxnn33WGDp0qK3++++/N6pWrWqMHz/e2LdvnzF37lzD1dXVSEhIcNQQDMO4/nG89tprxvLly42DBw8au3btMp5++mnDxcXF+Oqrrxw1BGPMmDGGt7e3sX79euPkyZO2x8WLF201V3+mNm3aZLi5uRmvvvqqsW/fPmPKlClGlSpVjF27djliCOUagzP+XDz77LNGcnKykZ6ebuzcudN49tlnDYvFYnz55ZeGYTj/cShyveNwxmNRkqt/6+x2OR5Xu9Y4nO14PPPMM8b69euN9PR0Y9OmTUZERIRRu3ZtIysrq8T+neE4EJqczBtvvGHcddddhtVqNdq1a2ds2bLFtq5Lly7GsGHDbM/Hjh1rq/Xz8zP69OljbN++3QFd/0/Rr95f/Sjqe9iwYUaXLl2KvaZ169aG1Wo17r77bmPhwoW3vO+rXe84XnnlFaNhw4aGh4eHUbNmTaNr167G2rVrHdP8f5XUvyS79/fqz5RhGMbSpUuNJk2aGFar1WjRooWxatWqW9v4FcozBmf8uRgxYoRRr149w2q1GnXq1DF69OhhCxqG4fzHocj1jsMZj0VJrg4bt8vxuNq1xuFsx2PAgAFGQECAYbVajTvvvNMYMGCAcejQIdt6ZzwOFsMwjFt3XgsAAOD2xJwmAAAAEwhNAAAAJhCaAAAATCA0AQAAmEBoAgAAMIHQBAAAYAKhCQAAwARCEwCYUL9+fc2aNcvRbQBwIEITAFwhPj5ePj4+xZZ/8803Gj169K1vCIDTcHN0AwBwq+Tl5clqtZbrtXXq1KngbgDcbjjTBKDS6tq1q2JiYjR27FjVrl1bkZGRmjlzpkJCQlStWjUFBQXp//7v/3T+/HlJ0vr16zV8+HCdPXtWFotFFotFU6dOlVT88pzFYtE777yj3//+96pataoaN26szz//3G7/n3/+uRo3biwPDw9169ZN7777riwWi7Kzs2/ROwCgIhGaAFRq7777rqxWqzZt2qT58+fLxcVFr7/+uvbs2aN3331Xa9eu1YQJEyRJHTp00KxZs+Tl5aWTJ0/q5MmT+vOf/1zqtqdNm6ZHH31UO3fuVJ8+fTRkyBCdOXNGkpSenq6HH35Y/fr103fffac//elP+utf/3pLxgzg5uDyHIBKrXHjxpo+fbrteXBwsO3P9evX14svvqgnnnhCb775pqxWq7y9vWWxWOTv73/NbT/22GMaNGiQJOnll1/W66+/rm3btqlXr1566623FBwcrH/84x+2/e7evVsvvfRSBY8QwK1CaAJQqbVp08bu+VdffaW4uDjt379fOTk5unz5si5duqSLFy+qatWq17XtVq1a2f5crVo1eXl5KSsrS5J04MABtW3b1q6+Xbt25RwFAGfA5TkAlVq1atVsfz5y5IgeeOABtWrVSsuWLVNqaqrmzp0r6ddJ4terSpUqds8tFosKCwtvrGEAToszTQB+M1JTU1VYWKgZM2bIxeXX/zMuXbrUrsZqtaqgoOCG9xUcHKzVq1fbLfvmm29ueLsAHIczTQB+Mxo1aqT8/Hy98cYb+v777/X+++9r/vz5djX169fX+fPnlZSUpNOnT+vixYvl2tef/vQn7d+/XxMnTtR//vMfLV26VPHx8ZJ+PSMF4PZDaALwmxEaGqqZM2fqlVdeUcuWLbVo0SLFxcXZ1XTo0EFPPPGEBgwYoDp16thNIr8eDRo00Mcff6xPPvlErVq10rx582y/Pefu7n7DYwFw61kMwzAc3QQA/Ba89NJLmj9/vo4fP+7oVgCUA3OaAOAmefPNN9W2bVvVqlVLmzZt0j/+8Q/FxMQ4ui0A5URoAoCb5ODBg3rxxRd15swZ3XXXXXrmmWc0adIkR7cFoJy4PAcAAGACE8EBAABMIDQBAACYQGgCAAAwgdAEAABgAqEJAADABEITAACACYQmAAAAEwhNAAAAJhCaAAAATPj/T0HE2RpoOaIAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean global rating: {round(ratings['rating'].mean(),2)}.\")\n",
        "\n",
        "mean_ratings = ratings.groupby('userId')['rating'].mean()\n",
        "print(f\"Mean rating per user: {round(mean_ratings.mean(),2)}.\")"
      ],
      "metadata": {
        "id": "uCoVyBWpiOQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332d7802-b95a-4c65-bd51-5c1fec6d8fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean global rating: 3.5.\n",
            "Mean rating per user: 3.66.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung total interaksi antar user dan item\n",
        "total_interactions = data.shape[0]\n",
        "\n",
        "# Menghitung total jumlah pengguna dan item\n",
        "num_users = data[COL_USER].nunique()\n",
        "num_items = data[COL_ITEM].nunique()\n",
        "\n",
        "# Menghitung densitas interaksi\n",
        "interaction_density = (total_interactions / (num_users * num_items)) * 100\n",
        "\n",
        "print(\n",
        "    f\"Total Interactions: {total_interactions}\",\n",
        "    f\"Number of Users: {num_users}\",\n",
        "    f\"Number of Items: {num_items}\",\n",
        "    f\"Interaction Density: {round(interaction_density, 4)}%\",\n",
        "    sep=\"\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "8Q-htasejSWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c412539-657a-4434-8e68-3efe7bbe8e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Interactions: 100836\n",
            "Number of Users: 610\n",
            "Number of Items: 9724\n",
            "Interaction Density: 1.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "_scBRfZ0fnX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Transformation"
      ],
      "metadata": {
        "id": "9kgxgqrSs0Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "data[COL_TIMESTAMP]= data.apply(\n",
        "    lambda x: datetime.strftime(datetime(1970, 1, 1, 0, 0, 0) + timedelta(seconds=x[COL_TIMESTAMP].item()), \"%Y-%m-%d %H:%M:%S\"),\n",
        "    axis=1\n",
        ")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "s2bMgUeesTXp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "64443a5e-5945-414b-b7f1-427573b56861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   userId  movieId  rating            timestamp\n",
              "0       1        1     4.0  2000-07-30 18:45:03\n",
              "1       1        3     4.0  2000-07-30 18:20:47\n",
              "2       1        6     4.0  2000-07-30 18:37:04\n",
              "3       1       47     5.0  2000-07-30 19:03:35\n",
              "4       1       50     5.0  2000-07-30 18:48:51"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cefcac01-7650-483e-aaeb-1486389e4725\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-07-30 18:45:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-07-30 18:20:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2000-07-30 18:37:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2000-07-30 19:03:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2000-07-30 18:48:51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cefcac01-7650-483e-aaeb-1486389e4725')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cefcac01-7650-483e-aaeb-1486389e4725 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cefcac01-7650-483e-aaeb-1486389e4725');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1844e1fd-ae0b-4419-b3b8-ffa86dc0e4a5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1844e1fd-ae0b-4419-b3b8-ffa86dc0e4a5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1844e1fd-ae0b-4419-b3b8-ffa86dc0e4a5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Test Split Data"
      ],
      "metadata": {
        "id": "KFaWYCQmkw5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 50\n",
        "\n",
        "# Model parameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "yaml3_file = \"/content/lightgcn3.yaml\"\n",
        "yaml4_file = \"/content/lightgcn4.yaml\"\n",
        "yaml5_file = \"/content/lightgcn5.yaml\"\n",
        "user_file = \"/content/user_embeddings.csv\"\n",
        "item_file = \"/content/item_embeddings.csv\"\n"
      ],
      "metadata": {
        "id": "kXw1CTTNdJML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def process_split_ratio(ratio):\n",
        "    \"\"\".Menghasilkan daftar rasio pembagian.\n",
        "\n",
        "    Argumen:\n",
        "        ratio (float atau list): Sebuah angka desimal (float) yang menunjukkan rasio pembagian atau sebuah daftar angka desimal (list) yang menunjukkan rasio pembagian\n",
        "                                 (jika pembagian dilakukan dalam beberapa bagian atau multi-split).\n",
        "\n",
        "    Mengembalikan:\n",
        "        tuple:\n",
        "        - bool: Sebuah variabel boolean `multi` yang menunjukkan apakah pembagian dilakukan\n",
        "          dalam beberapa bagian (multi-split) atau satu bagian (single-split).\n",
        "        - list: Daftar rasio pembagian yang telah dinormalisasi.\n",
        "    \"\"\"\n",
        "    if isinstance(ratio, float):\n",
        "        if ratio <= 0 or ratio >= 1:\n",
        "            raise ValueError(\"Split ratio has to be between 0 and 1\")\n",
        "\n",
        "        multi = False\n",
        "    elif isinstance(ratio, list):\n",
        "        if any([x <= 0 for x in ratio]):\n",
        "            raise ValueError(\n",
        "                \"All split ratios in the ratio list should be larger than 0.\"\n",
        "            )\n",
        "\n",
        "        # normalize split ratios if they are not summed to 1\n",
        "        if math.fsum(ratio) != 1.0:\n",
        "            ratio = [x / math.fsum(ratio) for x in ratio]\n",
        "\n",
        "        multi = True\n",
        "    else:\n",
        "        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n",
        "\n",
        "    return multi, ratio\n",
        "\n",
        "def _get_column_name(name, col_user, col_item):\n",
        "    if name == \"user\":\n",
        "        return col_user\n",
        "    elif name == \"item\":\n",
        "        return col_item\n",
        "    else:\n",
        "        raise ValueError(\"name should be either 'user' or 'item'.\")\n",
        "\n",
        "def min_rating_filter_pandas(\n",
        "    data,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "):\n",
        "    \"\"\"Memfilter DataFrame rating untuk setiap pengguna dengan jumlah rating minimum.\n",
        "\n",
        "      Memfilter data frame rating dengan jumlah rating minimum untuk pengguna/item biasanya berguna untuk menghasilkan data frame baru dengan pengguna/item hangat. Kehangatan didefinisikan oleh\n",
        "      argumen min_rating. Sebagai contoh, seorang pengguna disebut hangat jika dia telah memberikan rating pada setidaknya 4 item.\n",
        "\n",
        "      Argumen:\n",
        "          data (pandas.DataFrame): DataFrame dari pasangan user-item. Kolom untuk pengguna dan item harus ada dalam DataFrame, sedangkan kolom lain seperti rating bersifat opsional.\n",
        "          min_rating (int): Jumlah minimum rating untuk pengguna atau item.\n",
        "          filter_by (str): Pilihan \"user\" atau \"item\", tergantung pada mana yang akan difilter berdasarkan min_rating.\n",
        "          col_user (str): Nama kolom untuk ID pengguna.\n",
        "          col_item (str): Nama kolom untuk ID item.\n",
        "\n",
        "      Mengembalikan:\n",
        "          pandas.DataFrame: DataFrame dengan setidaknya kolom pengguna dan item yang telah difilter berdasarkan spesifikasi yang diberikan.\n",
        "    \"\"\"\n",
        "    split_by_column = _get_column_name(filter_by, col_user, col_item)\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    return data.groupby(split_by_column).filter(lambda x: len(x) >= min_rating)\n",
        "\n",
        "def _do_stratification(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    is_random=True,\n",
        "    seed=42,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
        "):\n",
        "    # A few preliminary checks.\n",
        "    if not (filter_by == \"user\" or filter_by == \"item\"):\n",
        "        raise ValueError(\"filter_by should be either 'user' or 'item'.\")\n",
        "\n",
        "    if min_rating < 1:\n",
        "        raise ValueError(\"min_rating should be integer and larger than or equal to 1.\")\n",
        "\n",
        "    if col_user not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing User Col\")\n",
        "\n",
        "    if col_item not in data.columns:\n",
        "        raise ValueError(\"Schema of data not valid. Missing Item Col\")\n",
        "\n",
        "    if not is_random:\n",
        "        if col_timestamp not in data.columns:\n",
        "            raise ValueError(\"Schema of data not valid. Missing Timestamp Col\")\n",
        "\n",
        "    multi_split, ratio = process_split_ratio(ratio)\n",
        "\n",
        "    split_by_column = col_user if filter_by == \"user\" else col_item\n",
        "\n",
        "    ratio = ratio if multi_split else [ratio, 1 - ratio]\n",
        "\n",
        "    if min_rating > 1:\n",
        "        data = min_rating_filter_pandas(\n",
        "            data,\n",
        "            min_rating=min_rating,\n",
        "            filter_by=filter_by,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "        )\n",
        "\n",
        "    if is_random:\n",
        "        np.random.seed(seed)\n",
        "        data[\"random\"] = np.random.rand(data.shape[0])\n",
        "        order_by = \"random\"\n",
        "    else:\n",
        "        order_by = col_timestamp\n",
        "\n",
        "    data = data.sort_values([split_by_column, order_by])\n",
        "\n",
        "    groups = data.groupby(split_by_column)\n",
        "\n",
        "    data[\"count\"] = groups[split_by_column].transform(\"count\")\n",
        "    data[\"rank\"] = groups.cumcount() + 1\n",
        "\n",
        "    if is_random:\n",
        "        data = data.drop(\"random\", axis=1)\n",
        "\n",
        "    splits = []\n",
        "    prev_threshold = None\n",
        "    for threshold in np.cumsum(ratio):\n",
        "        condition = data[\"rank\"] <= round(threshold * data[\"count\"])\n",
        "        if prev_threshold is not None:\n",
        "            condition &= data[\"rank\"] > round(prev_threshold * data[\"count\"])\n",
        "        splits.append(data[condition].drop([\"rank\", \"count\"], axis=1))\n",
        "        prev_threshold = threshold\n",
        "\n",
        "    return splits\n",
        "\n",
        "def python_stratified_split(\n",
        "    data,\n",
        "    ratio=0.75,\n",
        "    min_rating=1,\n",
        "    filter_by=\"user\",\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\"Pembagi stratifikasi Pandas.\n",
        "\n",
        "        Untuk setiap pengguna/item, fungsi split mengambil proporsi rating yang ditentukan oleh rasio pembagian (split ratio). Pembagian dilakukan secara stratifikasi.\n",
        "\n",
        "        Argumen:\n",
        "            data (pandas.DataFrame): DataFrame Pandas yang akan dibagi.\n",
        "            ratio (float atau list): Rasio untuk membagi data. Jika merupakan angka float tunggal, data dibagi menjadi dua bagian dan argumen rasio menunjukkan proporsi data pelatihan;\n",
        "                                     jika merupakan daftar angka float, pembagi membagi data menjadi beberapa bagian yang sesuai dengan rasio pembagian. Jika daftar diberikan dan\n",
        "                                     rasio tidak dijumlahkan menjadi 1, rasio akan dinormalisasi.\n",
        "            seed (int): Seed.\n",
        "            min_rating (int): Jumlah minimum rating untuk pengguna atau item.\n",
        "            filter_by (str): Pilihan \"user\" atau \"item\", tergantung pada mana yang akan difilter\n",
        "                berdasarkan min_rating.\n",
        "            col_user (str): Nama kolom untuk ID pengguna.\n",
        "            col_item (str): Nama kolom untuk ID item.\n",
        "\n",
        "        Mengembalikan:\n",
        "            list: Pembagian data input sebagai pandas.DataFrame.\n",
        "    \"\"\"\n",
        "    return _do_stratification(\n",
        "        data,\n",
        "        ratio=ratio,\n",
        "        min_rating=min_rating,\n",
        "        filter_by=filter_by,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        is_random=True,\n",
        "        seed=seed,\n",
        "    )"
      ],
      "metadata": {
        "id": "WifnFT2fDNNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = python_stratified_split(data, ratio=0.75)"
      ],
      "metadata": {
        "id": "fork3dV9dKVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model LightGCN"
      ],
      "metadata": {
        "id": "0X2ogiVzhKJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Proses Data"
      ],
      "metadata": {
        "id": "8eyelZAxnaec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ImplicitCF"
      ],
      "metadata": {
        "id": "q6QNVwjxCjvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the MIT License.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "class ImplicitCF(object):\n",
        "    \"\"\"Kelas pemrosesan data untuk model GCN yang menggunakan umpan balik implisit.\n",
        "\n",
        "      Menginisialisasi set pelatihan dan pengujian, membuat normalized adjacency matrix,\n",
        "      dan mengambil sampel data untuk epoch pelatihan.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train,\n",
        "        test=None,\n",
        "        adj_dir=None,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_rating=DEFAULT_RATING_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        seed=None,\n",
        "    ):\n",
        "        \"\"\"Konstruktor\n",
        "\n",
        "          Argumen:\n",
        "              adj_dir (str): Direktori untuk menyimpan/memuat adjacency matrices. Jika None, adjacency matrices akan dibuat dan tidak disimpan.\n",
        "              train (pandas.DataFrame): Data pelatihan dengan setidaknya kolom (col_user, col_item, col_rating).\n",
        "              test (pandas.DataFrame): Data pengujian dengan setidaknya kolom (col_user, col_item, col_rating). Test dapat bernilai None, jika demikian, hanya data pelatihan yang akan diproses.\n",
        "              col_user (str): Nama kolom untuk ID pengguna.\n",
        "              col_item (str): Nama kolom untuk ID item.\n",
        "              col_rating (str): Nama kolom untuk rating.\n",
        "              seed (int): Seed.\n",
        "\n",
        "        \"\"\"\n",
        "        self.user_idx = None\n",
        "        self.item_idx = None\n",
        "        self.adj_dir = adj_dir\n",
        "        self.col_user = col_user\n",
        "        self.col_item = col_item\n",
        "        self.col_rating = col_rating\n",
        "        self.col_prediction = col_prediction\n",
        "        self.train, self.test = self._data_processing(train, test)\n",
        "        self._init_train_data()\n",
        "\n",
        "        random.seed(seed)\n",
        "\n",
        "    def _data_processing(self, train, test):\n",
        "        \"\"\"Memproses dataset untuk mengindeks ulang userID dan itemID dan hanya menyimpan data dengan rating lebih besar dari 0.\n",
        "\n",
        "            Argumen:\n",
        "                train (pandas.DataFrame): Data pelatihan dengan setidaknya kolom (col_user, col_item, col_rating).\n",
        "                test (pandas.DataFrame): Data pengujian dengan setidaknya kolom (col_user, col_item, col_rating).\n",
        "                    Test dapat bernilai None, jika demikian, hanya data pelatihan yang akan diproses.\n",
        "\n",
        "            Mengembalikan:\n",
        "                list: Dataset train dan test pandas.DataFrame, yang telah diindeks ulang dan difilter.\n",
        "\n",
        "        \"\"\"\n",
        "        df = (\n",
        "            train\n",
        "            if test is None\n",
        "            else pd.concat([train, test], axis=0, ignore_index=True)\n",
        "        )\n",
        "\n",
        "        if self.user_idx is None:\n",
        "            user_idx = df[[self.col_user]].drop_duplicates().reindex()\n",
        "            user_idx[self.col_user + \"_idx\"] = np.arange(len(user_idx))\n",
        "            self.n_users = len(user_idx)\n",
        "            self.n_users_in_train = train[self.col_user].nunique()\n",
        "            self.user_idx = user_idx\n",
        "\n",
        "            self.user2id = dict(\n",
        "                zip(user_idx[self.col_user], user_idx[self.col_user + \"_idx\"])\n",
        "            )\n",
        "            self.id2user = dict(\n",
        "                zip(user_idx[self.col_user + \"_idx\"], user_idx[self.col_user])\n",
        "            )\n",
        "\n",
        "        if self.item_idx is None:\n",
        "            item_idx = df[[self.col_item]].drop_duplicates()\n",
        "            item_idx[self.col_item + \"_idx\"] = np.arange(len(item_idx))\n",
        "            self.n_items = len(item_idx)\n",
        "            self.item_idx = item_idx\n",
        "\n",
        "            self.item2id = dict(\n",
        "                zip(item_idx[self.col_item], item_idx[self.col_item + \"_idx\"])\n",
        "            )\n",
        "            self.id2item = dict(\n",
        "                zip(item_idx[self.col_item + \"_idx\"], item_idx[self.col_item])\n",
        "            )\n",
        "\n",
        "        return self._reindex(train), self._reindex(test)\n",
        "\n",
        "    def _reindex(self, df):\n",
        "\n",
        "        if df is None:\n",
        "            return None\n",
        "\n",
        "        df = pd.merge(df, self.user_idx, on=self.col_user, how=\"left\")\n",
        "        df = pd.merge(df, self.item_idx, on=self.col_item, how=\"left\")\n",
        "\n",
        "        df = df[df[self.col_rating] > 0]\n",
        "\n",
        "        df_reindex = df[\n",
        "            [self.col_user + \"_idx\", self.col_item + \"_idx\", self.col_rating]\n",
        "        ]\n",
        "        df_reindex.columns = [self.col_user, self.col_item, self.col_rating]\n",
        "\n",
        "        return df_reindex\n",
        "\n",
        "    def _init_train_data(self):\n",
        "        \"\"\"Mencatat item yang diinteraksikan dengan setiap pengguna dalam dataframe self.interact_status, dan membuat  adjacency\n",
        "        matrix self.R.\n",
        "\n",
        "        \"\"\"\n",
        "        self.interact_status = (\n",
        "            self.train.groupby(self.col_user)[self.col_item]\n",
        "            .apply(set)\n",
        "            .reset_index()\n",
        "            .rename(columns={self.col_item: self.col_item + \"_interacted\"})\n",
        "        )\n",
        "        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
        "        self.R[self.train[self.col_user], self.train[self.col_item]] = 1.0\n",
        "\n",
        "    def get_norm_adj_mat(self):\n",
        "        \"\"\"Memuat Load normalized adjacency matrix ternormalisasi jika ada, jika tidak, buat (dan simpan) matriks tersebut.\n",
        "\n",
        "            Mengembalikan:\n",
        "                scipy.sparse.csr_matrix: Matriks kedekatan ternormalisasi.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.adj_dir is None:\n",
        "                raise FileNotFoundError\n",
        "            norm_adj_mat = sp.load_npz(self.adj_dir + \"/norm_adj_mat.npz\")\n",
        "            print(\"Already load norm adj matrix.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            norm_adj_mat = self.create_norm_adj_mat()\n",
        "            if self.adj_dir is not None:\n",
        "                sp.save_npz(self.adj_dir + \"/norm_adj_mat.npz\", norm_adj_mat)\n",
        "        return norm_adj_mat\n",
        "\n",
        "    def create_norm_adj_mat(self):\n",
        "        \"\"\"Create normalized adjacency matrix.\n",
        "\n",
        "        Returns:\n",
        "            scipy.sparse.csr_matrix: Normalized adjacency matrix.\n",
        "\n",
        "        \"\"\"\n",
        "        adj_mat = sp.dok_matrix(\n",
        "            (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n",
        "        )\n",
        "        adj_mat = adj_mat.tolil()\n",
        "        R = self.R.tolil()\n",
        "\n",
        "        adj_mat[: self.n_users, self.n_users :] = R\n",
        "        adj_mat[self.n_users :, : self.n_users] = R.T\n",
        "        adj_mat = adj_mat.todok()\n",
        "        print(\"Already create adjacency matrix.\")\n",
        "\n",
        "        rowsum = np.array(adj_mat.sum(1))\n",
        "        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n",
        "        d_inv[np.isinf(d_inv)] = 0.0\n",
        "        d_mat_inv = sp.diags(d_inv)\n",
        "        norm_adj_mat = d_mat_inv.dot(adj_mat)\n",
        "        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n",
        "        print(\"Already normalize adjacency matrix.\")\n",
        "\n",
        "        return norm_adj_mat.tocsr()\n",
        "\n",
        "    def train_loader(self, batch_size):\n",
        "        \"\"\"Mengambil sampel data pelatihan setiap batch. Satu item positif dan satu item negatif diambil untuk setiap pengguna.\n",
        "\n",
        "          Argumen:\n",
        "              batch_size (int): Ukuran batch pengguna.\n",
        "\n",
        "          Mengembalikan:\n",
        "              numpy.ndarray, numpy.ndarray, numpy.ndarray:\n",
        "              - Pengguna yang diambil sampelnya.\n",
        "              - Item positif yang diambil sampelnya.\n",
        "              - Item negatif yang diambil sampelnya.\n",
        "        \"\"\"\n",
        "\n",
        "        def sample_neg(x):\n",
        "            if len(x) >= self.n_items:\n",
        "                raise ValueError(\"A user has voted in every item. Can't find a negative sample.\")\n",
        "            while True:\n",
        "                neg_id = random.randint(0, self.n_items - 1)\n",
        "                if neg_id not in x:\n",
        "                    return neg_id\n",
        "\n",
        "        indices = range(self.n_users_in_train)\n",
        "        if self.n_users < batch_size:\n",
        "            users = [random.choice(indices) for _ in range(batch_size)]\n",
        "        else:\n",
        "            users = random.sample(indices, batch_size)\n",
        "\n",
        "        interact = self.interact_status.iloc[users]\n",
        "        pos_items = interact[self.col_item + \"_interacted\"].apply(\n",
        "            lambda x: random.choice(list(x))\n",
        "        )\n",
        "        neg_items = interact[self.col_item + \"_interacted\"].apply(\n",
        "            lambda x: sample_neg(x)\n",
        "        )\n",
        "\n",
        "        return np.array(users), np.array(pos_items), np.array(neg_items)\n"
      ],
      "metadata": {
        "id": "CSQDx2AcCl4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ImplicitCF(train=train, test=test, seed=SEED)"
      ],
      "metadata": {
        "id": "bkIFB2ALmfnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prepere hyper-parameters"
      ],
      "metadata": {
        "id": "xlVmrTcLoeTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = prepare_hparams(yaml5_file,\n",
        "                          n_layers=5,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS,\n",
        "                          learning_rate=0.005,\n",
        "                          eval_epoch=5,\n",
        "                          top_k=TOP_K,\n",
        "                         )"
      ],
      "metadata": {
        "id": "RIlh9UfWodYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create and Train Model"
      ],
      "metadata": {
        "id": "65xDoQPBdgDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lightgcn"
      ],
      "metadata": {
        "id": "sp_jLUIuBukx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_rating=DEFAULT_RATING_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    score_type=\"binary\",\n",
        "    discfun_type=\"loge\",\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Normalized Discounted Cumulative Gain (nDCG).\n",
        "\n",
        "    Info: https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_rating (str): column name for rating\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "        score_type (str): type of relevance scores ['binary', 'raw', 'exp']. With the default option 'binary', the\n",
        "            relevance score is reduced to either 1 (hit) or 0 (miss). Option 'raw' uses the raw relevance score.\n",
        "            Option 'exp' uses (2 ** RAW_RELEVANCE - 1) as the relevance score\n",
        "        discfun_type (str): type of discount function ['loge', 'log2'] used to calculate DCG.\n",
        "\n",
        "    Returns:\n",
        "        float: nDCG at k (min=0, max=1).\n",
        "    \"\"\"\n",
        "    df_hit, _, _ = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    df_dcg = df_hit.merge(rating_pred, on=[col_user, col_item]).merge(\n",
        "        rating_true, on=[col_user, col_item], how=\"outer\", suffixes=(\"_left\", None)\n",
        "    )\n",
        "\n",
        "    if score_type == \"binary\":\n",
        "        df_dcg[\"rel\"] = 1\n",
        "    elif score_type == \"raw\":\n",
        "        df_dcg[\"rel\"] = df_dcg[col_rating]\n",
        "    elif score_type == \"exp\":\n",
        "        df_dcg[\"rel\"] = 2 ** df_dcg[col_rating] - 1\n",
        "    else:\n",
        "        raise ValueError(\"score_type must be one of 'binary', 'raw', 'exp'\")\n",
        "\n",
        "    if discfun_type == \"loge\":\n",
        "        discfun = np.log\n",
        "    elif discfun_type == \"log2\":\n",
        "        discfun = np.log2\n",
        "    else:\n",
        "        raise ValueError(\"discfun_type must be one of 'loge', 'log2'\")\n",
        "\n",
        "    # Calculate the actual discounted gain for each record\n",
        "    df_dcg[\"dcg\"] = df_dcg[\"rel\"] / discfun(1 + df_dcg[\"rank\"])\n",
        "\n",
        "    # Calculate the ideal discounted gain for each record\n",
        "    df_idcg = df_dcg.sort_values([col_user, col_rating], ascending=False)\n",
        "    df_idcg[\"irank\"] = df_idcg.groupby(col_user, as_index=False, sort=False)[\n",
        "        col_rating\n",
        "    ].rank(\"first\", ascending=False)\n",
        "    df_idcg[\"idcg\"] = df_idcg[\"rel\"] / discfun(1 + df_idcg[\"irank\"])\n",
        "\n",
        "    # Calculate the actual DCG for each user\n",
        "    df_user = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
        "\n",
        "    # Calculate the ideal DCG for each user\n",
        "    df_user = df_user.merge(\n",
        "        df_idcg.groupby(col_user, as_index=False, sort=False)\n",
        "        .head(k)\n",
        "        .groupby(col_user, as_index=False, sort=False)\n",
        "        .agg({\"idcg\": \"sum\"}),\n",
        "        on=col_user,\n",
        "    )\n",
        "\n",
        "    # DCG over IDCG is the normalized DCG\n",
        "    df_user[\"ndcg\"] = df_user[\"dcg\"] / df_user[\"idcg\"]\n",
        "    return df_user[\"ndcg\"].mean()"
      ],
      "metadata": {
        "id": "YB2kFe0UHYT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Recall at K.\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        float: recall at k (min=0, max=1). The maximum value is 1 even when fewer than\n",
        "        k items exist for a user in rating_true.\n",
        "    \"\"\"\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users"
      ],
      "metadata": {
        "id": "MeMb3SCBHvB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_scored_items(scores, top_k, sort_top_k=False):\n",
        "    \"\"\"Extract top K items from a matrix of scores for each user-item pair, optionally sort results per user.\n",
        "\n",
        "    Args:\n",
        "        scores (numpy.ndarray): Score matrix (users x items).\n",
        "        top_k (int): Number of top items to recommend.\n",
        "        sort_top_k (bool): Flag to sort top k results.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray, numpy.ndarray:\n",
        "        - Indices into score matrix for each user's top items.\n",
        "        - Scores corresponding to top items.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # ensure we're working with a dense ndarray\n",
        "    if isinstance(scores, sparse.spmatrix):\n",
        "        scores = scores.todense()\n",
        "\n",
        "    if scores.shape[1] < top_k:\n",
        "        logger.warning(\n",
        "            \"Number of items is less than top_k, limiting top_k to number of items\"\n",
        "        )\n",
        "    k = min(top_k, scores.shape[1])\n",
        "\n",
        "    test_user_idx = np.arange(scores.shape[0])[:, None]\n",
        "\n",
        "    # get top K items and scores\n",
        "    # this determines the un-ordered top-k item indices for each user\n",
        "    top_items = np.argpartition(scores, -k, axis=1)[:, -k:]\n",
        "    top_scores = scores[test_user_idx, top_items]\n",
        "\n",
        "    if sort_top_k:\n",
        "        sort_ind = np.argsort(-top_scores)\n",
        "        top_items = top_items[test_user_idx, sort_ind]\n",
        "        top_scores = top_scores[test_user_idx, sort_ind]\n",
        "\n",
        "    return np.array(top_items), np.array(top_scores)\n"
      ],
      "metadata": {
        "id": "Dqm-GkmLIiTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the MIT License.\n",
        "\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "tf.compat.v1.disable_eager_execution()  # need to disable eager in TF2.x\n",
        "\n",
        "\n",
        "class LightGCN(object):\n",
        "    \"\"\"LightGCN model\n",
        "\n",
        "    :Citation:\n",
        "\n",
        "        He, Xiangnan, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang.\n",
        "        \"LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation.\" arXiv\n",
        "        preprint arXiv:2002.02126, 2020.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams, data, seed=None):\n",
        "        \"\"\"Initializing the model. Create parameters, placeholders, embeddings and loss function.\n",
        "\n",
        "        Args:\n",
        "            hparams (HParams): A HParams object, hold the entire set of hyperparameters.\n",
        "            data (object): A recommenders.models.deeprec.DataModel.ImplicitCF object, load and process data.\n",
        "            seed (int): Seed.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        tf.compat.v1.set_random_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.data = data\n",
        "        self.epochs = hparams.epochs\n",
        "        self.lr = hparams.learning_rate\n",
        "        self.emb_dim = hparams.embed_size\n",
        "        self.batch_size = hparams.batch_size\n",
        "        self.n_layers = hparams.n_layers\n",
        "        self.decay = hparams.decay\n",
        "        self.eval_epoch = hparams.eval_epoch\n",
        "        self.top_k = hparams.top_k\n",
        "        self.save_model = hparams.save_model\n",
        "        self.save_epoch = hparams.save_epoch\n",
        "        self.metrics = hparams.metrics\n",
        "        self.model_dir = hparams.MODEL_DIR\n",
        "\n",
        "        valid_metrics = [\"ndcg\", \"recall\"]\n",
        "        self.metrics = [metric for metric in hparams.metrics if metric in valid_metrics]\n",
        "        if not self.metrics:\n",
        "            raise ValueError(\"Please select at least one valid metric: {}\".format(valid_metrics))\n",
        "\n",
        "\n",
        "        self.norm_adj = data.get_norm_adj_mat()\n",
        "\n",
        "        self.n_users = data.n_users\n",
        "        self.n_items = data.n_items\n",
        "\n",
        "        self.users = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "        self.pos_items = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "        self.neg_items = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "\n",
        "        self.weights = self._init_weights()\n",
        "        self.ua_embeddings, self.ia_embeddings = self._create_lightgcn_embed()\n",
        "\n",
        "        self.u_g_embeddings = tf.nn.embedding_lookup(\n",
        "            params=self.ua_embeddings, ids=self.users\n",
        "        )\n",
        "        self.pos_i_g_embeddings = tf.nn.embedding_lookup(\n",
        "            params=self.ia_embeddings, ids=self.pos_items\n",
        "        )\n",
        "        self.neg_i_g_embeddings = tf.nn.embedding_lookup(\n",
        "            params=self.ia_embeddings, ids=self.neg_items\n",
        "        )\n",
        "        self.u_g_embeddings_pre = tf.nn.embedding_lookup(\n",
        "            params=self.weights[\"user_embedding\"], ids=self.users\n",
        "        )\n",
        "        self.pos_i_g_embeddings_pre = tf.nn.embedding_lookup(\n",
        "            params=self.weights[\"item_embedding\"], ids=self.pos_items\n",
        "        )\n",
        "        self.neg_i_g_embeddings_pre = tf.nn.embedding_lookup(\n",
        "            params=self.weights[\"item_embedding\"], ids=self.neg_items\n",
        "        )\n",
        "\n",
        "        self.batch_ratings = tf.matmul(\n",
        "            self.u_g_embeddings,\n",
        "            self.pos_i_g_embeddings,\n",
        "            transpose_a=False,\n",
        "            transpose_b=True,\n",
        "        )\n",
        "\n",
        "        self.mf_loss, self.emb_loss = self._create_bpr_loss(\n",
        "            self.u_g_embeddings, self.pos_i_g_embeddings, self.neg_i_g_embeddings\n",
        "        )\n",
        "        self.loss = self.mf_loss + self.emb_loss\n",
        "\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr).minimize(\n",
        "            self.loss\n",
        "        )\n",
        "        self.saver = tf.compat.v1.train.Saver(max_to_keep=1)\n",
        "\n",
        "        gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "        self.sess = tf.compat.v1.Session(\n",
        "            config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
        "        )\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize user and item embeddings.\n",
        "\n",
        "        Returns:\n",
        "            dict: With keys `user_embedding` and `item_embedding`, embeddings of all users and items.\n",
        "\n",
        "        \"\"\"\n",
        "        all_weights = dict()\n",
        "        initializer = tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "            scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"\n",
        "        )\n",
        "\n",
        "        all_weights[\"user_embedding\"] = tf.Variable(\n",
        "            initializer([self.n_users, self.emb_dim]), name=\"user_embedding\"\n",
        "        )\n",
        "        all_weights[\"item_embedding\"] = tf.Variable(\n",
        "            initializer([self.n_items, self.emb_dim]), name=\"item_embedding\"\n",
        "        )\n",
        "        print(\"Using xavier initialization.\")\n",
        "\n",
        "        return all_weights\n",
        "\n",
        "    def _create_lightgcn_embed(self):\n",
        "        \"\"\"Calculate the average embeddings of users and items after every layer of the model.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor: Average user embeddings. Average item embeddings.\n",
        "\n",
        "        \"\"\"\n",
        "        A_hat = self._convert_sp_mat_to_sp_tensor(self.norm_adj)\n",
        "\n",
        "        ego_embeddings = tf.concat(\n",
        "            [self.weights[\"user_embedding\"], self.weights[\"item_embedding\"]], axis=0\n",
        "        )\n",
        "        all_embeddings = [ego_embeddings]\n",
        "\n",
        "        for k in range(0, self.n_layers):\n",
        "            ego_embeddings = tf.sparse.sparse_dense_matmul(A_hat, ego_embeddings)\n",
        "            all_embeddings += [ego_embeddings]\n",
        "\n",
        "        all_embeddings = tf.stack(all_embeddings, 1)\n",
        "        all_embeddings = tf.reduce_mean(\n",
        "            input_tensor=all_embeddings, axis=1, keepdims=False\n",
        "        )\n",
        "        u_g_embeddings, i_g_embeddings = tf.split(\n",
        "            all_embeddings, [self.n_users, self.n_items], 0\n",
        "        )\n",
        "        return u_g_embeddings, i_g_embeddings\n",
        "\n",
        "    def _create_bpr_loss(self, users, pos_items, neg_items):\n",
        "        \"\"\"Calculate BPR loss.\n",
        "\n",
        "        Args:\n",
        "            users (tf.Tensor): User embeddings to calculate loss.\n",
        "            pos_items (tf.Tensor): Positive item embeddings to calculate loss.\n",
        "            neg_items (tf.Tensor): Negative item embeddings to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor, tf.Tensor: Matrix factorization loss. Embedding regularization loss.\n",
        "\n",
        "        \"\"\"\n",
        "        pos_scores = tf.reduce_sum(input_tensor=tf.multiply(users, pos_items), axis=1)\n",
        "        neg_scores = tf.reduce_sum(input_tensor=tf.multiply(users, neg_items), axis=1)\n",
        "\n",
        "        regularizer = (\n",
        "            tf.nn.l2_loss(self.u_g_embeddings_pre)\n",
        "            + tf.nn.l2_loss(self.pos_i_g_embeddings_pre)\n",
        "            + tf.nn.l2_loss(self.neg_i_g_embeddings_pre)\n",
        "        )\n",
        "        regularizer = regularizer / self.batch_size\n",
        "        mf_loss = tf.reduce_mean(\n",
        "            input_tensor=tf.nn.softplus(-(pos_scores - neg_scores))\n",
        "        )\n",
        "        emb_loss = self.decay * regularizer\n",
        "        return mf_loss, emb_loss\n",
        "\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        \"\"\"Convert a scipy sparse matrix to tf.SparseTensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.SparseTensor: SparseTensor after conversion.\n",
        "\n",
        "        \"\"\"\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        indices = np.asmatrix([coo.row, coo.col]).transpose()\n",
        "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the model on self.data.train. If eval_epoch is not -1, evaluate the model on `self.data.test`\n",
        "        every `eval_epoch` epoch to observe the training status.\n",
        "\n",
        "        \"\"\"\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            train_start = time.time()\n",
        "            loss, mf_loss, emb_loss = 0.0, 0.0, 0.0\n",
        "            n_batch = self.data.train.shape[0] // self.batch_size + 1\n",
        "            for idx in range(n_batch):\n",
        "                users, pos_items, neg_items = self.data.train_loader(self.batch_size)\n",
        "                _, batch_loss, batch_mf_loss, batch_emb_loss = self.sess.run(\n",
        "                    [self.opt, self.loss, self.mf_loss, self.emb_loss],\n",
        "                    feed_dict={\n",
        "                        self.users: users,\n",
        "                        self.pos_items: pos_items,\n",
        "                        self.neg_items: neg_items,\n",
        "                    },\n",
        "                )\n",
        "                loss += batch_loss / n_batch\n",
        "                mf_loss += batch_mf_loss / n_batch\n",
        "                emb_loss += batch_emb_loss / n_batch\n",
        "\n",
        "            if np.isnan(loss):\n",
        "                print(\"ERROR: loss is nan.\")\n",
        "                sys.exit()\n",
        "            train_end = time.time()\n",
        "            train_time = train_end - train_start\n",
        "\n",
        "            if self.save_model and epoch % self.save_epoch == 0:\n",
        "                save_path_str = os.path.join(self.model_dir, \"epoch_\" + str(epoch))\n",
        "                if not os.path.exists(save_path_str):\n",
        "                    os.makedirs(save_path_str)\n",
        "                checkpoint_path = self.saver.save(  # noqa: F841\n",
        "                    sess=self.sess, save_path=save_path_str\n",
        "                )\n",
        "                print(\"Save model to path {0}\".format(os.path.abspath(save_path_str)))\n",
        "\n",
        "            if self.eval_epoch == -1 or epoch % self.eval_epoch != 0:\n",
        "                print(\n",
        "                    \"Epoch %d (train)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f\"\n",
        "                    % (epoch, train_time, loss, mf_loss, emb_loss)\n",
        "                )\n",
        "            else:\n",
        "                eval_start = time.time()\n",
        "                ret = self.run_eval()\n",
        "                eval_end = time.time()\n",
        "                eval_time = eval_end - eval_start\n",
        "\n",
        "                print(\n",
        "                    \"Epoch %d (train)%.1fs + (eval)%.1fs: train loss = %.5f = (mf)%.5f + (embed)%.5f, %s\"\n",
        "                    % (\n",
        "                        epoch,\n",
        "                        train_time,\n",
        "                        eval_time,\n",
        "                        loss,\n",
        "                        mf_loss,\n",
        "                        emb_loss,\n",
        "                        \", \".join(\n",
        "                            metric + \" = %.5f\" % (r)\n",
        "                            for metric, r in zip(self.metrics, ret)\n",
        "                        ),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def load(self, model_path=None):\n",
        "        \"\"\"Load an existing model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Model path.\n",
        "\n",
        "        Raises:\n",
        "            IOError: if the restore operation failed.\n",
        "\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.saver.restore(self.sess, model_path)\n",
        "        except Exception:\n",
        "            raise IOError(\n",
        "                \"Failed to find any matching files for {0}\".format(model_path)\n",
        "            )\n",
        "\n",
        "    def run_eval(self):\n",
        "        \"\"\"Run evaluation on self.data.test.\n",
        "\n",
        "        Returns:\n",
        "            dict: Results of all metrics in `self.metrics`.\n",
        "        \"\"\"\n",
        "        topk_scores = self.recommend_k_items(\n",
        "            self.data.test, top_k=self.top_k, use_id=True\n",
        "        )\n",
        "        ret = []\n",
        "        for metric in self.metrics:\n",
        "            if metric == \"ndcg\":\n",
        "                ret.append(ndcg_at_k(self.data.test, topk_scores, k=self.top_k))\n",
        "            elif metric == \"recall\":\n",
        "                ret.append(recall_at_k(self.data.test, topk_scores, k=self.top_k))\n",
        "        return ret\n",
        "\n",
        "    def score(self, user_ids, remove_seen=True):\n",
        "        \"\"\"Score all items for test users.\n",
        "\n",
        "        Args:\n",
        "            user_ids (np.array): Users to test.\n",
        "            remove_seen (bool): Flag to remove items seen in training from recommendation.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Value of interest of all items for the users.\n",
        "\n",
        "        \"\"\"\n",
        "        if any(np.isnan(user_ids)):\n",
        "            raise ValueError(\n",
        "                \"LightGCN cannot score users that are not in the training set\"\n",
        "            )\n",
        "        u_batch_size = self.batch_size\n",
        "        n_user_batchs = len(user_ids) // u_batch_size + 1\n",
        "        test_scores = []\n",
        "        for u_batch_id in range(n_user_batchs):\n",
        "            start = u_batch_id * u_batch_size\n",
        "            end = (u_batch_id + 1) * u_batch_size\n",
        "            user_batch = user_ids[start:end]\n",
        "            item_batch = range(self.data.n_items)\n",
        "            rate_batch = self.sess.run(\n",
        "                self.batch_ratings, {self.users: user_batch, self.pos_items: item_batch}\n",
        "            )\n",
        "            test_scores.append(np.array(rate_batch))\n",
        "        test_scores = np.concatenate(test_scores, axis=0)\n",
        "        if remove_seen:\n",
        "            test_scores += self.data.R.tocsr()[user_ids, :] * -np.inf\n",
        "        return test_scores\n",
        "\n",
        "    def recommend_k_items(\n",
        "        self, test, top_k=10, sort_top_k=True, remove_seen=True, use_id=False\n",
        "    ):\n",
        "        \"\"\"Recommend top K items for all users in the test set.\n",
        "\n",
        "        Args:\n",
        "            test (pandas.DataFrame): Test data.\n",
        "            top_k (int): Number of top items to recommend.\n",
        "            sort_top_k (bool): Flag to sort top k results.\n",
        "            remove_seen (bool): Flag to remove items seen in training from recommendation.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: Top k recommendation items for each user.\n",
        "\n",
        "        \"\"\"\n",
        "        data = self.data\n",
        "        if not use_id:\n",
        "            user_ids = np.array([data.user2id[x] for x in test[data.col_user].unique()])\n",
        "        else:\n",
        "            user_ids = np.array(test[data.col_user].unique())\n",
        "\n",
        "        test_scores = self.score(user_ids, remove_seen=remove_seen)\n",
        "\n",
        "        top_items, top_scores = get_top_k_scored_items(\n",
        "            scores=test_scores, top_k=top_k, sort_top_k=sort_top_k\n",
        "        )\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                data.col_user: np.repeat(\n",
        "                    test[data.col_user].drop_duplicates().values, top_items.shape[1]\n",
        "                ),\n",
        "                data.col_item: top_items.flatten()\n",
        "                if use_id\n",
        "                else [data.id2item[item] for item in top_items.flatten()],\n",
        "                data.col_prediction: top_scores.flatten(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return df.replace(-np.inf, np.nan).dropna()\n",
        "\n",
        "    def output_embeddings(self, idmapper, n, target, user_file):\n",
        "        embeddings = list(target.eval(session=self.sess))\n",
        "        with open(user_file, \"w\") as wt:\n",
        "            for i in range(n):\n",
        "                wt.write(\n",
        "                    \"{0}\\t{1}\\n\".format(\n",
        "                        idmapper[i], \" \".join([str(a) for a in embeddings[i]])\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def infer_embedding(self, user_file, item_file):\n",
        "        \"\"\"Export user and item embeddings to csv files.\n",
        "\n",
        "        Args:\n",
        "            user_file (str): Path of file to save user embeddings.\n",
        "            item_file (str): Path of file to save item embeddings.\n",
        "\n",
        "        \"\"\"\n",
        "        # create output directories if they do not exist\n",
        "        dirs, _ = os.path.split(user_file)\n",
        "        if not os.path.exists(dirs):\n",
        "            os.makedirs(dirs)\n",
        "        dirs, _ = os.path.split(item_file)\n",
        "        if not os.path.exists(dirs):\n",
        "            os.makedirs(dirs)\n",
        "\n",
        "        data = self.data\n",
        "\n",
        "        self.output_embeddings(\n",
        "            data.id2user, self.n_users, self.ua_embeddings, user_file\n",
        "        )\n",
        "        self.output_embeddings(\n",
        "            data.id2item, self.n_items, self.ia_embeddings, item_file\n",
        "        )\n"
      ],
      "metadata": {
        "id": "MDQ618w9AYai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightGCN(hparams, data, seed=SEED)"
      ],
      "metadata": {
        "id": "ac2nM1o8di4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14735b95-a6fa-4a31-a1af-492014c0926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Timer"
      ],
      "metadata": {
        "id": "JT46qlTsADkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import lru_cache, wraps\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "V1FAPBRWtefy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _check_column_dtypes(func):\n",
        "    \"\"\"Checks columns of DataFrame inputs\n",
        "\n",
        "    This includes the checks on:\n",
        "\n",
        "    * whether the input columns exist in the input DataFrames\n",
        "    * whether the data types of col_user as well as col_item are matched in the two input DataFrames.\n",
        "\n",
        "    Args:\n",
        "        func (function): function that will be wrapped\n",
        "\n",
        "    Returns:\n",
        "        function: Wrapper function for checking dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def check_column_dtypes_wrapper(\n",
        "        rating_true,\n",
        "        rating_pred,\n",
        "        col_user=DEFAULT_USER_COL,\n",
        "        col_item=DEFAULT_ITEM_COL,\n",
        "        col_prediction=DEFAULT_PREDICTION_COL,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Check columns of DataFrame inputs\n",
        "\n",
        "        Args:\n",
        "            rating_true (pandas.DataFrame): True data\n",
        "            rating_pred (pandas.DataFrame): Predicted data\n",
        "            col_user (str): column name for user\n",
        "            col_item (str): column name for item\n",
        "            col_rating (str): column name for rating\n",
        "            col_prediction (str): column name for prediction\n",
        "        \"\"\"\n",
        "        # Some ranking metrics don't have the rating column, so we don't need to check.\n",
        "        expected_true_columns = {col_user, col_item}\n",
        "        if \"col_rating\" in kwargs:\n",
        "            expected_true_columns.add(kwargs[\"col_rating\"])\n",
        "        if not has_columns(rating_true, expected_true_columns):\n",
        "            raise ColumnMismatchError(\"Missing columns in true rating DataFrame\")\n",
        "\n",
        "        if not has_columns(rating_pred, {col_user, col_item, col_prediction}):\n",
        "            raise ColumnMismatchError(\"Missing columns in predicted rating DataFrame\")\n",
        "\n",
        "        if not has_same_base_dtype(\n",
        "            rating_true, rating_pred, columns=[col_user, col_item]\n",
        "        ):\n",
        "            raise ColumnTypeMismatchError(\n",
        "                \"Columns in provided DataFrames are not the same datatype\"\n",
        "            )\n",
        "\n",
        "        return func(\n",
        "            rating_true=rating_true,\n",
        "            rating_pred=rating_pred,\n",
        "            col_user=col_user,\n",
        "            col_item=col_item,\n",
        "            col_prediction=col_prediction,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    return check_column_dtypes_wrapper"
      ],
      "metadata": {
        "id": "-znoBbp5tCbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lru_cache_df(maxsize, typed=False):\n",
        "    \"\"\"Least-recently-used cache decorator for pandas Dataframes.\n",
        "\n",
        "    Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can\n",
        "    save time when an expensive or I/O bound function is periodically called with the same arguments.\n",
        "\n",
        "    Inspired in the `lru_cache function <https://docs.python.org/3/library/functools.html#functools.lru_cache>`_.\n",
        "\n",
        "    Args:\n",
        "        maxsize (int|None): max size of cache, if set to None cache is boundless\n",
        "        typed (bool): arguments of different types are cached separately\n",
        "    \"\"\"\n",
        "\n",
        "    def to_pandas_hash(val):\n",
        "        \"\"\"Return PandaHash object if input is a DataFrame otherwise return input unchanged\"\"\"\n",
        "        return PandasHash(val) if isinstance(val, pd.DataFrame) else val\n",
        "\n",
        "    def from_pandas_hash(val):\n",
        "        \"\"\"Extract DataFrame if input is PandaHash object otherwise return input unchanged\"\"\"\n",
        "        return val.pandas_object if isinstance(val, PandasHash) else val\n",
        "\n",
        "    def decorating_function(user_function):\n",
        "        @wraps(user_function)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            # convert DataFrames in args and kwargs to PandaHash objects\n",
        "            args = tuple([to_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: to_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return cached_wrapper(*args, **kwargs)\n",
        "\n",
        "        @lru_cache(maxsize=maxsize, typed=typed)\n",
        "        def cached_wrapper(*args, **kwargs):\n",
        "            # get DataFrames from PandaHash objects in args and kwargs\n",
        "            args = tuple([from_pandas_hash(a) for a in args])\n",
        "            kwargs = {k: from_pandas_hash(v) for k, v in kwargs.items()}\n",
        "            return user_function(*args, **kwargs)\n",
        "\n",
        "        # retain lru_cache attributes\n",
        "        wrapper.cache_info = cached_wrapper.cache_info\n",
        "        wrapper.cache_clear = cached_wrapper.cache_clear\n",
        "\n",
        "        return wrapper\n",
        "\n",
        "    return decorating_function"
      ],
      "metadata": {
        "id": "XATlE1YBtSQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@_check_column_dtypes\n",
        "@lru_cache_df(maxsize=1)\n",
        "def merge_ranking_true_pred(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user,\n",
        "    col_item,\n",
        "    col_prediction,\n",
        "    relevancy_method,\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "    **_,\n",
        "):\n",
        "    \"\"\"Filter truth and prediction data frames on common users\n",
        "\n",
        "    Args:\n",
        "        rating_true (pandas.DataFrame): True DataFrame\n",
        "        rating_pred (pandas.DataFrame): Predicted DataFrame\n",
        "        col_user (str): column name for user\n",
        "        col_item (str): column name for item\n",
        "        col_prediction (str): column name for prediction\n",
        "        relevancy_method (str): method for determining relevancy ['top_k', 'by_threshold', None]. None means that the\n",
        "            top k items are directly provided, so there is no need to compute the relevancy operation.\n",
        "        k (int): number of top k items per user (optional)\n",
        "        threshold (float): threshold of top items per user (optional)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame, pandas.DataFrame, int: DataFrame of recommendation hits, sorted by `col_user` and `rank`\n",
        "        DataFrame of hit counts vs actual relevant items per user number of unique user ids\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure the prediction and true data frames have the same set of users\n",
        "    common_users = set(rating_true[col_user]).intersection(set(rating_pred[col_user]))\n",
        "    rating_true_common = rating_true[rating_true[col_user].isin(common_users)]\n",
        "    rating_pred_common = rating_pred[rating_pred[col_user].isin(common_users)]\n",
        "    n_users = len(common_users)\n",
        "\n",
        "    # Return hit items in prediction data frame with ranking information. This is used for calculating NDCG and MAP.\n",
        "    # Use first to generate unique ranking values for each item. This is to align with the implementation in\n",
        "    # Spark evaluation metrics, where index of each recommended items (the indices are unique to items) is used\n",
        "    # to calculate penalized precision of the ordered items.\n",
        "    if relevancy_method == \"top_k\":\n",
        "        top_k = k\n",
        "    elif relevancy_method == \"by_threshold\":\n",
        "        top_k = threshold\n",
        "    elif relevancy_method is None:\n",
        "        top_k = None\n",
        "    else:\n",
        "        raise NotImplementedError(\"Invalid relevancy_method\")\n",
        "    df_hit = get_top_k_items(\n",
        "        dataframe=rating_pred_common,\n",
        "        col_user=col_user,\n",
        "        col_rating=col_prediction,\n",
        "        k=top_k,\n",
        "    )\n",
        "    df_hit = pd.merge(df_hit, rating_true_common, on=[col_user, col_item])[\n",
        "        [col_user, col_item, \"rank\"]\n",
        "    ]\n",
        "\n",
        "    # count the number of hits vs actual relevant items per user\n",
        "    df_hit_count = pd.merge(\n",
        "        df_hit.groupby(col_user, as_index=False)[col_user].agg(hit=\"count\"),\n",
        "        rating_true_common.groupby(col_user, as_index=False)[col_user].agg(\n",
        "            actual=\"count\",\n",
        "        ),\n",
        "        on=col_user,\n",
        "    )\n",
        "\n",
        "    return df_hit, df_hit_count, n_users"
      ],
      "metadata": {
        "id": "PfDT5nEIsX6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_columns(df, columns):\n",
        "    \"\"\"Check if DataFrame has necessary columns\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): DataFrame\n",
        "        columns (iterable(str)): columns to check for\n",
        "\n",
        "    Returns:\n",
        "        bool: True if DataFrame has specified columns.\n",
        "    \"\"\"\n",
        "    if not isinstance(columns, set):\n",
        "        columns = set(columns)\n",
        "    return columns.issubset(df.columns)"
      ],
      "metadata": {
        "id": "qjCvOEw2uB15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"Timer class.\n",
        "\n",
        "    `Original code <https://github.com/miguelgfierro/pybase/blob/2298172a13fb4a243754acbc6029a4a2dcf72c20/log_base/timer.py>`_.\n",
        "\n",
        "    Examples:\n",
        "        >>> import time\n",
        "        >>> t = Timer()\n",
        "        >>> t.start()\n",
        "        >>> time.sleep(1)\n",
        "        >>> t.stop()\n",
        "        >>> t.interval < 1\n",
        "        True\n",
        "        >>> with Timer() as t:\n",
        "        ...   time.sleep(1)\n",
        "        >>> t.interval < 1\n",
        "        True\n",
        "        >>> \"Time elapsed {}\".format(t) #doctest: +ELLIPSIS\n",
        "        'Time elapsed 1...'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._timer = default_timer\n",
        "        self._interval = 0\n",
        "        self.running = False\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.stop()\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"{:0.4f}\".format(self.interval)\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.init = self._timer()\n",
        "        self.running = True\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer. Calculate the interval in seconds.\"\"\"\n",
        "        self.end = self._timer()\n",
        "        try:\n",
        "            self._interval = self.end - self.init\n",
        "            self.running = False\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Timer has not been initialized: use start() or the contextual form with Timer() as t:\"\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def interval(self):\n",
        "        \"\"\"Get time interval in seconds.\n",
        "\n",
        "        Returns:\n",
        "            float: Seconds.\n",
        "        \"\"\"\n",
        "        if self.running:\n",
        "            raise ValueError(\"Timer has not been stopped, please use stop().\")\n",
        "        else:\n",
        "            return self._interval\n"
      ],
      "metadata": {
        "id": "rcY87Mgh9MhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_same_base_dtype(df_1, df_2, columns=None):\n",
        "    \"\"\"Check if specified columns have the same base dtypes across both DataFrames\n",
        "\n",
        "    Args:\n",
        "        df_1 (pandas.DataFrame): first DataFrame\n",
        "        df_2 (pandas.DataFrame): second DataFrame\n",
        "        columns (list(str)): columns to check, None checks all columns\n",
        "\n",
        "    Returns:\n",
        "        bool: True if DataFrames columns have the same base dtypes.\n",
        "    \"\"\"\n",
        "\n",
        "    if columns is None:\n",
        "        if any(set(df_1.columns).symmetric_difference(set(df_2.columns))):\n",
        "            logger.error(\n",
        "                \"Cannot test all columns because they are not all shared across DataFrames\"\n",
        "            )\n",
        "            return False\n",
        "        columns = df_1.columns\n",
        "\n",
        "    if not (\n",
        "        has_columns(df=df_1, columns=columns) and has_columns(df=df_2, columns=columns)\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    result = True\n",
        "    for column in columns:\n",
        "        if df_1[column].dtype.type.__base__ != df_2[column].dtype.type.__base__:\n",
        "            logger.error(\"Columns {} do not have the same base datatype\".format(column))\n",
        "            result = False\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "fJEIDVPOuOeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PandasHash:\n",
        "    \"\"\"Wrapper class to allow pandas objects (DataFrames or Series) to be hashable\"\"\"\n",
        "\n",
        "    # reserve space just for a single pandas object\n",
        "    __slots__ = \"pandas_object\"\n",
        "\n",
        "    def __init__(self, pandas_object):\n",
        "        \"\"\"Initialize class\n",
        "\n",
        "        Args:\n",
        "            pandas_object (pandas.DataFrame|pandas.Series): pandas object\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(pandas_object, (pd.DataFrame, pd.Series)):\n",
        "            raise TypeError(\"Can only wrap pandas DataFrame or Series objects\")\n",
        "        self.pandas_object = pandas_object\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        \"\"\"Overwrite equality comparison\n",
        "\n",
        "        Args:\n",
        "            other (pandas.DataFrame|pandas.Series): pandas object to compare\n",
        "\n",
        "        Returns:\n",
        "            bool: whether other object is the same as this one\n",
        "        \"\"\"\n",
        "\n",
        "        return hash(self) == hash(other)\n",
        "\n",
        "    def __hash__(self):\n",
        "        \"\"\"Overwrite hash operator for use with pandas objects\n",
        "\n",
        "        Returns:\n",
        "            int: hashed value of object\n",
        "        \"\"\"\n",
        "\n",
        "        hashable = tuple(self.pandas_object.values.tobytes())\n",
        "        if isinstance(self.pandas_object, pd.DataFrame):\n",
        "            hashable += tuple(self.pandas_object.columns)\n",
        "        else:\n",
        "            hashable += tuple(self.pandas_object.name)\n",
        "        return hash(hashable)"
      ],
      "metadata": {
        "id": "iMCP0oIyudNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_items(\n",
        "    dataframe, col_user=DEFAULT_USER_COL, col_rating=DEFAULT_RATING_COL, k=DEFAULT_K\n",
        "):\n",
        "    \"\"\"Get the input customer-item-rating tuple in the format of Pandas\n",
        "    DataFrame, output a Pandas DataFrame in the dense format of top k items\n",
        "    for each user.\n",
        "\n",
        "    Note:\n",
        "        If it is implicit rating, just append a column of constants to be\n",
        "        ratings.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): DataFrame of rating data (in the format\n",
        "        customerID-itemID-rating)\n",
        "        col_user (str): column name for user\n",
        "        col_rating (str): column name for rating\n",
        "        k (int or None): number of items for each user; None means that the input has already been\n",
        "        filtered out top k items and sorted by ratings and there is no need to do that again.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame of top k items for each user, sorted by `col_user` and `rank`\n",
        "    \"\"\"\n",
        "    # Sort dataframe by col_user and (top k) col_rating\n",
        "    if k is None:\n",
        "        top_k_items = dataframe\n",
        "    else:\n",
        "        top_k_items = (\n",
        "            dataframe.sort_values([col_user, col_rating], ascending=[True, False])\n",
        "            .groupby(col_user, as_index=False)\n",
        "            .head(k)\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "    # Add ranks\n",
        "    top_k_items[\"rank\"] = top_k_items.groupby(col_user, sort=False).cumcount() + 1\n",
        "    return top_k_items"
      ],
      "metadata": {
        "id": "gksfojgpup_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@lru_cache_df(maxsize=1)\n",
        "def _get_reciprocal_rank(\n",
        "    rating_true,\n",
        "    rating_pred,\n",
        "    col_user=DEFAULT_USER_COL,\n",
        "    col_item=DEFAULT_ITEM_COL,\n",
        "    col_prediction=DEFAULT_PREDICTION_COL,\n",
        "    relevancy_method=\"top_k\",\n",
        "    k=DEFAULT_K,\n",
        "    threshold=DEFAULT_THRESHOLD,\n",
        "):\n",
        "    df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
        "        rating_true=rating_true,\n",
        "        rating_pred=rating_pred,\n",
        "        col_user=col_user,\n",
        "        col_item=col_item,\n",
        "        col_prediction=col_prediction,\n",
        "        relevancy_method=relevancy_method,\n",
        "        k=k,\n",
        "        threshold=threshold,\n",
        "    )\n",
        "\n",
        "    if df_hit.shape[0] == 0:\n",
        "        return None, n_users\n",
        "\n",
        "    # calculate reciprocal rank of items for each user and sum them up\n",
        "    df_hit_sorted = df_hit.copy()\n",
        "    df_hit_sorted[\"rr\"] = (\n",
        "        df_hit_sorted.groupby(col_user).cumcount() + 1\n",
        "    ) / df_hit_sorted[\"rank\"]\n",
        "    df_hit_sorted = df_hit_sorted.groupby(col_user).agg({\"rr\": \"sum\"}).reset_index()\n",
        "\n",
        "    return pd.merge(df_hit_sorted, df_hit_count, on=col_user), n_users"
      ],
      "metadata": {
        "id": "6EtRegtJvJMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit()\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ],
      "metadata": {
        "id": "0ZX-LXEodpUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe6c6c3-e8e8-405e-e3f0-4cb427c3f3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (train)4.7s: train loss = 0.41249 = (mf)0.41223 + (embed)0.00026\n",
            "Epoch 2 (train)4.7s: train loss = 0.23575 = (mf)0.23510 + (embed)0.00065\n",
            "Epoch 3 (train)4.5s: train loss = 0.22039 = (mf)0.21962 + (embed)0.00078\n",
            "Epoch 4 (train)4.1s: train loss = 0.20383 = (mf)0.20289 + (embed)0.00094\n",
            "Epoch 5 (train)5.0s + (eval)0.4s: train loss = 0.18939 = (mf)0.18826 + (embed)0.00113, recall = 0.23734, ndcg = 0.23797\n",
            "Epoch 6 (train)4.1s: train loss = 0.17521 = (mf)0.17387 + (embed)0.00134\n",
            "Epoch 7 (train)4.1s: train loss = 0.16457 = (mf)0.16303 + (embed)0.00154\n",
            "Epoch 8 (train)5.0s: train loss = 0.16066 = (mf)0.15893 + (embed)0.00173\n",
            "Epoch 9 (train)4.1s: train loss = 0.14934 = (mf)0.14742 + (embed)0.00192\n",
            "Epoch 10 (train)4.2s + (eval)0.5s: train loss = 0.14997 = (mf)0.14788 + (embed)0.00208, recall = 0.25885, ndcg = 0.25789\n",
            "Epoch 11 (train)5.8s: train loss = 0.14532 = (mf)0.14306 + (embed)0.00226\n",
            "Epoch 12 (train)5.0s: train loss = 0.14214 = (mf)0.13972 + (embed)0.00242\n",
            "Epoch 13 (train)4.9s: train loss = 0.13926 = (mf)0.13669 + (embed)0.00257\n",
            "Epoch 14 (train)4.1s: train loss = 0.13397 = (mf)0.13124 + (embed)0.00274\n",
            "Epoch 15 (train)4.0s + (eval)0.5s: train loss = 0.12815 = (mf)0.12523 + (embed)0.00292, recall = 0.26487, ndcg = 0.26429\n",
            "Epoch 16 (train)4.9s: train loss = 0.12947 = (mf)0.12638 + (embed)0.00308\n",
            "Epoch 17 (train)4.1s: train loss = 0.12516 = (mf)0.12189 + (embed)0.00327\n",
            "Epoch 18 (train)4.0s: train loss = 0.12083 = (mf)0.11737 + (embed)0.00346\n",
            "Epoch 19 (train)4.9s: train loss = 0.11314 = (mf)0.10946 + (embed)0.00368\n",
            "Epoch 20 (train)4.0s + (eval)0.3s: train loss = 0.11145 = (mf)0.10755 + (embed)0.00390, recall = 0.28080, ndcg = 0.28049\n",
            "Epoch 21 (train)5.1s: train loss = 0.10753 = (mf)0.10343 + (embed)0.00410\n",
            "Epoch 22 (train)4.6s: train loss = 0.10561 = (mf)0.10132 + (embed)0.00428\n",
            "Epoch 23 (train)4.1s: train loss = 0.09925 = (mf)0.09474 + (embed)0.00451\n",
            "Epoch 24 (train)4.7s: train loss = 0.09723 = (mf)0.09250 + (embed)0.00473\n",
            "Epoch 25 (train)4.2s + (eval)0.3s: train loss = 0.09519 = (mf)0.09028 + (embed)0.00491, recall = 0.29274, ndcg = 0.29313\n",
            "Epoch 26 (train)4.0s: train loss = 0.09338 = (mf)0.08826 + (embed)0.00512\n",
            "Epoch 27 (train)4.8s: train loss = 0.09147 = (mf)0.08617 + (embed)0.00531\n",
            "Epoch 28 (train)4.0s: train loss = 0.08852 = (mf)0.08299 + (embed)0.00553\n",
            "Epoch 29 (train)4.0s: train loss = 0.08776 = (mf)0.08204 + (embed)0.00572\n",
            "Epoch 30 (train)4.8s + (eval)0.5s: train loss = 0.08545 = (mf)0.07954 + (embed)0.00591, recall = 0.30297, ndcg = 0.30235\n",
            "Epoch 31 (train)4.0s: train loss = 0.08513 = (mf)0.07906 + (embed)0.00608\n",
            "Epoch 32 (train)4.0s: train loss = 0.08174 = (mf)0.07547 + (embed)0.00627\n",
            "Epoch 33 (train)4.8s: train loss = 0.07919 = (mf)0.07274 + (embed)0.00645\n",
            "Epoch 34 (train)4.0s: train loss = 0.07777 = (mf)0.07114 + (embed)0.00663\n",
            "Epoch 35 (train)4.0s + (eval)0.3s: train loss = 0.07555 = (mf)0.06871 + (embed)0.00684, recall = 0.31816, ndcg = 0.31722\n",
            "Epoch 36 (train)4.9s: train loss = 0.07577 = (mf)0.06880 + (embed)0.00696\n",
            "Epoch 37 (train)4.0s: train loss = 0.07147 = (mf)0.06433 + (embed)0.00714\n",
            "Epoch 38 (train)4.7s: train loss = 0.07029 = (mf)0.06300 + (embed)0.00729\n",
            "Epoch 39 (train)6.8s: train loss = 0.06900 = (mf)0.06155 + (embed)0.00746\n",
            "Epoch 40 (train)5.3s + (eval)0.7s: train loss = 0.06836 = (mf)0.06074 + (embed)0.00762, recall = 0.32562, ndcg = 0.32702\n",
            "Epoch 41 (train)4.7s: train loss = 0.06796 = (mf)0.06016 + (embed)0.00779\n",
            "Epoch 42 (train)4.1s: train loss = 0.06837 = (mf)0.06046 + (embed)0.00791\n",
            "Epoch 43 (train)4.5s: train loss = 0.06719 = (mf)0.05912 + (embed)0.00807\n",
            "Epoch 44 (train)4.5s: train loss = 0.06358 = (mf)0.05538 + (embed)0.00820\n",
            "Epoch 45 (train)4.1s + (eval)0.5s: train loss = 0.06362 = (mf)0.05527 + (embed)0.00834, recall = 0.33215, ndcg = 0.33738\n",
            "Epoch 46 (train)5.0s: train loss = 0.06131 = (mf)0.05283 + (embed)0.00848\n",
            "Epoch 47 (train)4.2s: train loss = 0.06077 = (mf)0.05217 + (embed)0.00859\n",
            "Epoch 48 (train)4.7s: train loss = 0.06060 = (mf)0.05187 + (embed)0.00873\n",
            "Epoch 49 (train)5.0s: train loss = 0.06029 = (mf)0.05145 + (embed)0.00884\n",
            "Epoch 50 (train)4.2s + (eval)0.3s: train loss = 0.05924 = (mf)0.05028 + (embed)0.00895, recall = 0.33682, ndcg = 0.34160\n",
            "Epoch 51 (train)4.3s: train loss = 0.05863 = (mf)0.04955 + (embed)0.00908\n",
            "Epoch 52 (train)4.9s: train loss = 0.05593 = (mf)0.04670 + (embed)0.00922\n",
            "Epoch 53 (train)4.1s: train loss = 0.05747 = (mf)0.04817 + (embed)0.00931\n",
            "Epoch 54 (train)4.5s: train loss = 0.05495 = (mf)0.04552 + (embed)0.00943\n",
            "Epoch 55 (train)4.7s + (eval)0.5s: train loss = 0.05517 = (mf)0.04565 + (embed)0.00952, recall = 0.34176, ndcg = 0.34467\n",
            "Epoch 56 (train)4.1s: train loss = 0.05353 = (mf)0.04391 + (embed)0.00962\n",
            "Epoch 57 (train)5.0s: train loss = 0.05400 = (mf)0.04429 + (embed)0.00971\n",
            "Epoch 58 (train)4.1s: train loss = 0.05328 = (mf)0.04350 + (embed)0.00979\n",
            "Epoch 59 (train)4.1s: train loss = 0.05229 = (mf)0.04241 + (embed)0.00988\n",
            "Epoch 60 (train)4.9s + (eval)0.3s: train loss = 0.05292 = (mf)0.04297 + (embed)0.00995, recall = 0.34571, ndcg = 0.34799\n",
            "Epoch 61 (train)4.0s: train loss = 0.05204 = (mf)0.04200 + (embed)0.01004\n",
            "Epoch 62 (train)4.1s: train loss = 0.05105 = (mf)0.04093 + (embed)0.01012\n",
            "Epoch 63 (train)4.9s: train loss = 0.04965 = (mf)0.03943 + (embed)0.01023\n",
            "Epoch 64 (train)4.2s: train loss = 0.04998 = (mf)0.03968 + (embed)0.01030\n",
            "Epoch 65 (train)4.2s + (eval)0.6s: train loss = 0.04994 = (mf)0.03957 + (embed)0.01037, recall = 0.34851, ndcg = 0.35163\n",
            "Epoch 66 (train)4.8s: train loss = 0.04981 = (mf)0.03934 + (embed)0.01047\n",
            "Epoch 67 (train)4.0s: train loss = 0.04741 = (mf)0.03687 + (embed)0.01054\n",
            "Epoch 68 (train)4.4s: train loss = 0.04818 = (mf)0.03756 + (embed)0.01062\n",
            "Epoch 69 (train)4.7s: train loss = 0.04768 = (mf)0.03699 + (embed)0.01069\n",
            "Epoch 70 (train)4.1s + (eval)0.3s: train loss = 0.04778 = (mf)0.03701 + (embed)0.01076, recall = 0.35289, ndcg = 0.35220\n",
            "Epoch 71 (train)4.9s: train loss = 0.04699 = (mf)0.03616 + (embed)0.01083\n",
            "Epoch 72 (train)4.6s: train loss = 0.04592 = (mf)0.03502 + (embed)0.01090\n",
            "Epoch 73 (train)4.0s: train loss = 0.04335 = (mf)0.03241 + (embed)0.01093\n",
            "Epoch 74 (train)4.9s: train loss = 0.04458 = (mf)0.03355 + (embed)0.01103\n",
            "Epoch 75 (train)4.0s + (eval)0.3s: train loss = 0.04575 = (mf)0.03469 + (embed)0.01107, recall = 0.35527, ndcg = 0.35435\n",
            "Epoch 76 (train)4.1s: train loss = 0.04499 = (mf)0.03386 + (embed)0.01113\n",
            "Epoch 77 (train)4.8s: train loss = 0.04471 = (mf)0.03353 + (embed)0.01118\n",
            "Epoch 78 (train)4.0s: train loss = 0.04272 = (mf)0.03148 + (embed)0.01123\n",
            "Epoch 79 (train)4.1s: train loss = 0.04387 = (mf)0.03258 + (embed)0.01128\n",
            "Epoch 80 (train)5.0s + (eval)0.5s: train loss = 0.04388 = (mf)0.03256 + (embed)0.01132, recall = 0.35542, ndcg = 0.35525\n",
            "Epoch 81 (train)4.0s: train loss = 0.04323 = (mf)0.03184 + (embed)0.01139\n",
            "Epoch 82 (train)4.3s: train loss = 0.04205 = (mf)0.03063 + (embed)0.01142\n",
            "Epoch 83 (train)4.7s: train loss = 0.04132 = (mf)0.02984 + (embed)0.01148\n",
            "Epoch 84 (train)4.0s: train loss = 0.04209 = (mf)0.03060 + (embed)0.01149\n",
            "Epoch 85 (train)4.5s + (eval)0.4s: train loss = 0.04081 = (mf)0.02924 + (embed)0.01157, recall = 0.35660, ndcg = 0.35922\n",
            "Epoch 86 (train)4.3s: train loss = 0.04233 = (mf)0.03071 + (embed)0.01162\n",
            "Epoch 87 (train)4.0s: train loss = 0.04118 = (mf)0.02953 + (embed)0.01166\n",
            "Epoch 88 (train)4.9s: train loss = 0.04094 = (mf)0.02924 + (embed)0.01170\n",
            "Epoch 89 (train)4.1s: train loss = 0.04151 = (mf)0.02976 + (embed)0.01175\n",
            "Epoch 90 (train)4.1s + (eval)0.5s: train loss = 0.04096 = (mf)0.02919 + (embed)0.01177, recall = 0.35834, ndcg = 0.36069\n",
            "Epoch 91 (train)4.9s: train loss = 0.03883 = (mf)0.02699 + (embed)0.01184\n",
            "Epoch 92 (train)4.1s: train loss = 0.04044 = (mf)0.02857 + (embed)0.01186\n",
            "Epoch 93 (train)4.1s: train loss = 0.04012 = (mf)0.02821 + (embed)0.01191\n",
            "Epoch 94 (train)4.9s: train loss = 0.03975 = (mf)0.02782 + (embed)0.01193\n",
            "Epoch 95 (train)4.0s + (eval)0.5s: train loss = 0.03892 = (mf)0.02699 + (embed)0.01194, recall = 0.35658, ndcg = 0.35701\n",
            "Epoch 96 (train)4.0s: train loss = 0.03878 = (mf)0.02679 + (embed)0.01199\n",
            "Epoch 97 (train)4.9s: train loss = 0.03919 = (mf)0.02717 + (embed)0.01202\n",
            "Epoch 98 (train)4.1s: train loss = 0.03817 = (mf)0.02611 + (embed)0.01206\n",
            "Epoch 99 (train)4.4s: train loss = 0.03809 = (mf)0.02601 + (embed)0.01208\n",
            "Epoch 100 (train)4.6s + (eval)0.3s: train loss = 0.03901 = (mf)0.02688 + (embed)0.01213, recall = 0.35765, ndcg = 0.35842\n",
            "Took 453.8012016260001 seconds for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rekomendasi"
      ],
      "metadata": {
        "id": "wCKxTv8dYjvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topk_scores = model.recommend_k_items(test, top_k=TOP_K, remove_seen=True)\n",
        "\n",
        "topk_scores.head(50)"
      ],
      "metadata": {
        "id": "Zb_V8MZiYml6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10edc32a-f494-4dc5-f4bf-7892e3b38a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    userId  movieId  prediction\n",
              "0        1     1200    9.587574\n",
              "1        1     2791    9.028669\n",
              "2        1     1210    8.843908\n",
              "3        1     1215    8.709219\n",
              "4        1     1198    8.698594\n",
              "5        1     1544    8.670197\n",
              "6        1     3793    8.620877\n",
              "7        1     1197    8.602399\n",
              "8        1     1387    8.546181\n",
              "9        1     3175    8.496447\n",
              "10       1     2302    8.390284\n",
              "11       1      589    8.322607\n",
              "12       1      592    8.277814\n",
              "13       1     2406    8.273941\n",
              "14       1     1562    8.256734\n",
              "15       1      858    8.202118\n",
              "16       1     1356    8.190173\n",
              "17       1      924    8.138670\n",
              "18       1     2918    8.093883\n",
              "19       1     1036    8.087258\n",
              "20       1     1371    8.046222\n",
              "21       1     2699    7.990677\n",
              "22       1     3072    7.968413\n",
              "23       1     1909    7.936406\n",
              "24       1      593    7.926958\n",
              "25       1      541    7.919140\n",
              "26       1     1527    7.833496\n",
              "27       1     2455    7.810703\n",
              "28       1     1373    7.789981\n",
              "29       1     3499    7.779974\n",
              "30       1     2011    7.719075\n",
              "31       1     3210    7.694046\n",
              "32       1     1676    7.682143\n",
              "33       1     1372    7.675606\n",
              "34       1       32    7.673785\n",
              "35       1     2081    7.667876\n",
              "36       1     1391    7.663414\n",
              "37       1     1079    7.657852\n",
              "38       1     2028    7.656351\n",
              "39       1     2746    7.655585\n",
              "40       1     1377    7.645561\n",
              "41       1     1089    7.643388\n",
              "42       1      318    7.623416\n",
              "43       1     3361    7.615606\n",
              "44       1     2002    7.608610\n",
              "45       1     1022    7.605686\n",
              "46       1     3039    7.587901\n",
              "47       1      588    7.580733\n",
              "48       1     1997    7.548630\n",
              "49       1      110    7.545680"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69fc0d33-00bd-4619-b568-361b41e6cff7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1200</td>\n",
              "      <td>9.587574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2791</td>\n",
              "      <td>9.028669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1210</td>\n",
              "      <td>8.843908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1215</td>\n",
              "      <td>8.709219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1198</td>\n",
              "      <td>8.698594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1544</td>\n",
              "      <td>8.670197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>3793</td>\n",
              "      <td>8.620877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1197</td>\n",
              "      <td>8.602399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1387</td>\n",
              "      <td>8.546181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>3175</td>\n",
              "      <td>8.496447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>2302</td>\n",
              "      <td>8.390284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>589</td>\n",
              "      <td>8.322607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>592</td>\n",
              "      <td>8.277814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>2406</td>\n",
              "      <td>8.273941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1</td>\n",
              "      <td>1562</td>\n",
              "      <td>8.256734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>858</td>\n",
              "      <td>8.202118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>1356</td>\n",
              "      <td>8.190173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>924</td>\n",
              "      <td>8.138670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>2918</td>\n",
              "      <td>8.093883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1036</td>\n",
              "      <td>8.087258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>1371</td>\n",
              "      <td>8.046222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>2699</td>\n",
              "      <td>7.990677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1</td>\n",
              "      <td>3072</td>\n",
              "      <td>7.968413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>1909</td>\n",
              "      <td>7.936406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>593</td>\n",
              "      <td>7.926958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1</td>\n",
              "      <td>541</td>\n",
              "      <td>7.919140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1</td>\n",
              "      <td>1527</td>\n",
              "      <td>7.833496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>2455</td>\n",
              "      <td>7.810703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1</td>\n",
              "      <td>1373</td>\n",
              "      <td>7.789981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td>3499</td>\n",
              "      <td>7.779974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1</td>\n",
              "      <td>2011</td>\n",
              "      <td>7.719075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1</td>\n",
              "      <td>3210</td>\n",
              "      <td>7.694046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1</td>\n",
              "      <td>1676</td>\n",
              "      <td>7.682143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1</td>\n",
              "      <td>1372</td>\n",
              "      <td>7.675606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>7.673785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1</td>\n",
              "      <td>2081</td>\n",
              "      <td>7.667876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1</td>\n",
              "      <td>1391</td>\n",
              "      <td>7.663414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>1079</td>\n",
              "      <td>7.657852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1</td>\n",
              "      <td>2028</td>\n",
              "      <td>7.656351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>2746</td>\n",
              "      <td>7.655585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1</td>\n",
              "      <td>1377</td>\n",
              "      <td>7.645561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1</td>\n",
              "      <td>1089</td>\n",
              "      <td>7.643388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1</td>\n",
              "      <td>318</td>\n",
              "      <td>7.623416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1</td>\n",
              "      <td>3361</td>\n",
              "      <td>7.615606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>2002</td>\n",
              "      <td>7.608610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1</td>\n",
              "      <td>1022</td>\n",
              "      <td>7.605686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1</td>\n",
              "      <td>3039</td>\n",
              "      <td>7.587901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1</td>\n",
              "      <td>588</td>\n",
              "      <td>7.580733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>1997</td>\n",
              "      <td>7.548630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1</td>\n",
              "      <td>110</td>\n",
              "      <td>7.545680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69fc0d33-00bd-4619-b568-361b41e6cff7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69fc0d33-00bd-4619-b568-361b41e6cff7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69fc0d33-00bd-4619-b568-361b41e6cff7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-34d5dc4e-b47a-48ff-8d56-1c22de16a08f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-34d5dc4e-b47a-48ff-8d56-1c22de16a08f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-34d5dc4e-b47a-48ff-8d56-1c22de16a08f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "topk_scores",
              "summary": "{\n  \"name\": \"topk_scores\",\n  \"rows\": 30500,\n  \"fields\": [\n    {\n      \"column\": \"userId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 176,\n        \"min\": 1,\n        \"max\": 610,\n        \"num_unique_values\": 610,\n        \"samples\": [\n          82,\n          219,\n          56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"movieId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28269,\n        \"min\": 1,\n        \"max\": 187593,\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          65682,\n          5989,\n          1254\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 30441,\n        \"samples\": [\n          6.010830879211426,\n          6.681656360626221,\n          9.695534706115723\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluasi"
      ],
      "metadata": {
        "id": "MbBLhhxBYnCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_ndcg = ndcg_at_k(test, topk_scores, k=TOP_K)\n",
        "eval_recall = recall_at_k(test, topk_scores, k=TOP_K)\n",
        "\n",
        "print(\"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
      ],
      "metadata": {
        "id": "YatNsY1PYotH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c321053-3d54-4db8-ce3e-3a34df64a40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG:\t0.358419\n",
            "Recall@K:\t0.357646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Recommenders contributors.\n",
        "# Licensed under the MIT License.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nbformat\n",
        "from nbconvert.preprocessors import ExecutePreprocessor\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "NOTEBOOK_OUTPUT_CONTENT_TYPE = \"application/notebook_utils.json+json\"\n",
        "def store_metadata(name, value):\n",
        "    \"\"\"Store data in the notebook's output source code.\n",
        "\n",
        "    Args:\n",
        "        name (str): Name of the data.\n",
        "        value (int,float,str): Value of the data.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"notebook_utils\": {\"name\": name, \"data\": True, \"display\": False}}\n",
        "    data_json = {\n",
        "        \"application/notebook_utils.json+json\": {\n",
        "            \"name\": name,\n",
        "            \"data\": value,\n",
        "            \"encoder\": \"json\",\n",
        "        }\n",
        "    }\n",
        "    display(data_json, metadata=metadata, raw=True)"
      ],
      "metadata": {
        "id": "Zuu_Rjqml3lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Record results for tests - ignore this cell\n",
        "store_metadata(\"ndcg\", eval_ndcg)\n",
        "store_metadata(\"recall\", eval_recall)"
      ],
      "metadata": {
        "id": "RKSpYWoul4j_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "78d9913d-9d53-49ed-bbb6-ca7dcf9c7722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "ndcg",
              "data": 0.3584193939316354,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "ndcg",
              "data": true,
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/notebook_utils.json+json": {
              "name": "recall",
              "data": 0.3576460884726755,
              "encoder": "json"
            }
          },
          "metadata": {
            "notebook_utils": {
              "name": "recall",
              "data": true,
              "display": false
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Infer Embedding"
      ],
      "metadata": {
        "id": "4cwf58nRY4IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.infer_embedding(user_file, item_file)"
      ],
      "metadata": {
        "id": "qr0aAfOoY59R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kombinasi Hyperparameter terbaik"
      ],
      "metadata": {
        "id": "-Hy1o4t6GjqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definisikan daftar nilai yang ingin Anda coba untuk setiap hyperparameter\n",
        "epochs_to_try = [25, 50, 100] # Contoh nilai, sesuaikan sesuai kebutuhan Anda\n",
        "topks_to_try = [10, 20, 50]   # Contoh nilai\n",
        "learning_rates_to_try = [0.005, 0.05, 0.5] # Contoh nilai\n",
        "n_layers_to_try = [3, 4, 5] # Contoh nilai untuk jumlah layer\n",
        "\n",
        "# Dictionary untuk menyimpan hasil evaluasi dari setiap kombinasi\n",
        "evaluation_results = {}\n",
        "\n",
        "# Mapping n_layers ke file YAML yang sesuai\n",
        "yaml_file_mapping = {\n",
        "    3: yaml3_file,\n",
        "    4: yaml4_file,\n",
        "    5: yaml5_file\n",
        "}\n",
        "\n",
        "# Mulai pencarian grid\n",
        "for epoch_val in epochs_to_try:\n",
        "    for topk_val in topks_to_try:\n",
        "        for lr_val in learning_rates_to_try:\n",
        "            for layers_val in n_layers_to_try:\n",
        "\n",
        "                print(f\"--- Trying combination: Epochs={epoch_val}, TopK={topk_val}, LearningRate={lr_val}, Layers={layers_val} ---\")\n",
        "\n",
        "                # Dapatkan yaml_file yang sesuai berdasarkan layers_val\n",
        "                current_yaml_file = yaml_file_mapping.get(layers_val)\n",
        "                if current_yaml_file is None:\n",
        "                    print(f\"Warning: No YAML file defined for {layers_val} layers. Skipping this combination.\")\n",
        "                    continue # Lewati kombinasi ini jika tidak ada file YAML yang sesuai\n",
        "\n",
        "                # Persiapkan hparams dengan kombinasi saat ini\n",
        "                current_hparams = prepare_hparams(current_yaml_file, # Gunakan yaml_file yang sesuai\n",
        "                                                  n_layers=layers_val,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  epochs=epoch_val,\n",
        "                                                  learning_rate=lr_val,\n",
        "                                                  eval_epoch=5,\n",
        "                                                  top_k=topk_val\n",
        "                                                 )\n",
        "\n",
        "                # Inisialisasi model LightGCN dengan hparams baru\n",
        "                model = LightGCN(current_hparams, data, seed=SEED)\n",
        "\n",
        "                # Latih model\n",
        "                with Timer() as train_time:\n",
        "                    model.fit()\n",
        "\n",
        "                print(f\"Training finished in {train_time.interval:.2f} seconds.\")\n",
        "\n",
        "                # Evaluasi model pada dataset test\n",
        "                topk_scores = model.recommend_k_items(test, top_k=topk_val, remove_seen=True)\n",
        "\n",
        "                eval_ndcg = ndcg_at_k(test, topk_scores, k=topk_val)\n",
        "                eval_recall = recall_at_k(test, topk_scores, k=topk_val)\n",
        "\n",
        "                print(f\"Evaluation results for this combination: NDCG@{topk_val}: {eval_ndcg:.5f}, Recall@{topk_val}: {eval_recall:.5f}\")\n",
        "\n",
        "                # Simpan hasil evaluasi\n",
        "                combination_key = f\"Epochs={epoch_val}_TopK={topk_val}_LR={lr_val}_Layers={layers_val}\"\n",
        "                evaluation_results[combination_key] = {\n",
        "                    'ndcg': eval_ndcg,\n",
        "                    'recall': eval_recall,\n",
        "                    'train_time': train_time.interval\n",
        "                }\n",
        "\n",
        "# Setelah semua kombinasi diuji, cetak ringkasan hasil\n",
        "print(\"\\n===== Hyperparameter Tuning Summary =====\")\n",
        "for combo, metrics in evaluation_results.items():\n",
        "    print(f\"{combo}: NDCG={metrics['ndcg']:.5f}, Recall={metrics['recall']:.5f}, TrainTime={metrics['train_time']:.2f}s\")\n",
        "\n",
        "# Anda dapat menganalisis 'evaluation_results' dictionary untuk menemukan kombinasi terbaik\n",
        "best_combination = max(evaluation_results, key=lambda k: evaluation_results[k]['ndcg'])\n",
        "print(f\"\\nBest combination based on NDCG: {best_combination}\")\n",
        "print(f\"NDCG: {evaluation_results[best_combination]['ndcg']:.5f}\")\n",
        "print(f\"Recall: {evaluation_results[best_combination]['recall']:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96mIUSE8-eMJ",
        "outputId": "c63e4990-dd1a-40f6-ac45-2d8e33f64204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming akan dipotong hingga 5000 baris terakhir.\u001b[0m\n",
            "Epoch 16 (train)2.9s: train loss = 0.09345 = (mf)0.09083 + (embed)0.00262\n",
            "Epoch 17 (train)2.9s: train loss = 0.08828 = (mf)0.08552 + (embed)0.00276\n",
            "Epoch 18 (train)3.4s: train loss = 0.08546 = (mf)0.08255 + (embed)0.00292\n",
            "Epoch 19 (train)3.2s: train loss = 0.08070 = (mf)0.07763 + (embed)0.00307\n",
            "Epoch 20 (train)2.9s + (eval)0.5s: train loss = 0.07972 = (mf)0.07650 + (embed)0.00323, recall = 0.17922, ndcg = 0.27794\n",
            "Epoch 21 (train)2.9s: train loss = 0.07571 = (mf)0.07234 + (embed)0.00338\n",
            "Epoch 22 (train)3.5s: train loss = 0.07324 = (mf)0.06972 + (embed)0.00353\n",
            "Epoch 23 (train)3.1s: train loss = 0.07042 = (mf)0.06676 + (embed)0.00366\n",
            "Epoch 24 (train)2.8s: train loss = 0.06851 = (mf)0.06470 + (embed)0.00381\n",
            "Epoch 25 (train)2.9s + (eval)0.2s: train loss = 0.06405 = (mf)0.06009 + (embed)0.00397, recall = 0.19078, ndcg = 0.29306\n",
            "Training finished in 78.73 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.29306, Recall@20: 0.19078\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.1s: train loss = 0.41398 = (mf)0.41374 + (embed)0.00025\n",
            "Epoch 2 (train)3.5s: train loss = 0.23003 = (mf)0.22939 + (embed)0.00064\n",
            "Epoch 3 (train)3.5s: train loss = 0.21890 = (mf)0.21816 + (embed)0.00074\n",
            "Epoch 4 (train)4.4s: train loss = 0.20812 = (mf)0.20726 + (embed)0.00086\n",
            "Epoch 5 (train)3.5s + (eval)0.6s: train loss = 0.19319 = (mf)0.19218 + (embed)0.00101, recall = 0.11770, ndcg = 0.19711\n",
            "Epoch 6 (train)3.5s: train loss = 0.18642 = (mf)0.18526 + (embed)0.00116\n",
            "Epoch 7 (train)4.3s: train loss = 0.16951 = (mf)0.16817 + (embed)0.00134\n",
            "Epoch 8 (train)3.4s: train loss = 0.15477 = (mf)0.15323 + (embed)0.00154\n",
            "Epoch 9 (train)3.4s: train loss = 0.14793 = (mf)0.14618 + (embed)0.00175\n",
            "Epoch 10 (train)4.1s + (eval)0.3s: train loss = 0.13777 = (mf)0.13583 + (embed)0.00194, recall = 0.15390, ndcg = 0.24442\n",
            "Epoch 11 (train)3.5s: train loss = 0.13703 = (mf)0.13492 + (embed)0.00211\n",
            "Epoch 12 (train)3.5s: train loss = 0.12978 = (mf)0.12751 + (embed)0.00227\n",
            "Epoch 13 (train)3.6s: train loss = 0.12673 = (mf)0.12430 + (embed)0.00243\n",
            "Epoch 14 (train)4.1s: train loss = 0.12118 = (mf)0.11859 + (embed)0.00259\n",
            "Epoch 15 (train)3.5s + (eval)0.5s: train loss = 0.11562 = (mf)0.11286 + (embed)0.00276, recall = 0.16192, ndcg = 0.25499\n",
            "Epoch 16 (train)3.5s: train loss = 0.11138 = (mf)0.10847 + (embed)0.00292\n",
            "Epoch 17 (train)4.5s: train loss = 0.10525 = (mf)0.10214 + (embed)0.00311\n",
            "Epoch 18 (train)3.7s: train loss = 0.10228 = (mf)0.09900 + (embed)0.00328\n",
            "Epoch 19 (train)3.6s: train loss = 0.09817 = (mf)0.09472 + (embed)0.00345\n",
            "Epoch 20 (train)4.4s + (eval)0.2s: train loss = 0.09900 = (mf)0.09539 + (embed)0.00361, recall = 0.17160, ndcg = 0.26978\n",
            "Epoch 21 (train)3.5s: train loss = 0.09220 = (mf)0.08842 + (embed)0.00378\n",
            "Epoch 22 (train)3.5s: train loss = 0.08988 = (mf)0.08592 + (embed)0.00396\n",
            "Epoch 23 (train)4.1s: train loss = 0.08632 = (mf)0.08219 + (embed)0.00413\n",
            "Epoch 24 (train)3.8s: train loss = 0.08413 = (mf)0.07984 + (embed)0.00429\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 0.07999 = (mf)0.07553 + (embed)0.00446, recall = 0.18090, ndcg = 0.28353\n",
            "Training finished in 95.80 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.28353, Recall@20: 0.18090\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.3s: train loss = 0.40946 = (mf)0.40921 + (embed)0.00026\n",
            "Epoch 2 (train)4.1s: train loss = 0.23778 = (mf)0.23715 + (embed)0.00064\n",
            "Epoch 3 (train)4.3s: train loss = 0.22560 = (mf)0.22486 + (embed)0.00074\n",
            "Epoch 4 (train)4.8s: train loss = 0.22064 = (mf)0.21977 + (embed)0.00087\n",
            "Epoch 5 (train)4.0s + (eval)0.4s: train loss = 0.21128 = (mf)0.21027 + (embed)0.00101, recall = 0.11157, ndcg = 0.19171\n",
            "Epoch 6 (train)4.8s: train loss = 0.20508 = (mf)0.20391 + (embed)0.00117\n",
            "Epoch 7 (train)4.3s: train loss = 0.19664 = (mf)0.19530 + (embed)0.00134\n",
            "Epoch 8 (train)4.2s: train loss = 0.18217 = (mf)0.18060 + (embed)0.00157\n",
            "Epoch 9 (train)5.1s: train loss = 0.16894 = (mf)0.16713 + (embed)0.00182\n",
            "Epoch 10 (train)4.2s + (eval)0.2s: train loss = 0.15609 = (mf)0.15401 + (embed)0.00208, recall = 0.15168, ndcg = 0.24061\n",
            "Epoch 11 (train)4.1s: train loss = 0.15048 = (mf)0.14815 + (embed)0.00233\n",
            "Epoch 12 (train)5.0s: train loss = 0.14658 = (mf)0.14406 + (embed)0.00252\n",
            "Epoch 13 (train)4.1s: train loss = 0.14304 = (mf)0.14033 + (embed)0.00271\n",
            "Epoch 14 (train)4.1s: train loss = 0.13685 = (mf)0.13397 + (embed)0.00288\n",
            "Epoch 15 (train)5.0s + (eval)0.5s: train loss = 0.13268 = (mf)0.12962 + (embed)0.00306, recall = 0.15376, ndcg = 0.24694\n",
            "Epoch 16 (train)4.1s: train loss = 0.12714 = (mf)0.12391 + (embed)0.00323\n",
            "Epoch 17 (train)4.4s: train loss = 0.12763 = (mf)0.12423 + (embed)0.00340\n",
            "Epoch 18 (train)4.7s: train loss = 0.12009 = (mf)0.11651 + (embed)0.00358\n",
            "Epoch 19 (train)4.1s: train loss = 0.12014 = (mf)0.11637 + (embed)0.00377\n",
            "Epoch 20 (train)4.7s + (eval)0.4s: train loss = 0.11325 = (mf)0.10929 + (embed)0.00396, recall = 0.16650, ndcg = 0.25891\n",
            "Epoch 21 (train)4.2s: train loss = 0.11303 = (mf)0.10888 + (embed)0.00415\n",
            "Epoch 22 (train)4.1s: train loss = 0.10750 = (mf)0.10316 + (embed)0.00434\n",
            "Epoch 23 (train)4.8s: train loss = 0.10635 = (mf)0.10183 + (embed)0.00453\n",
            "Epoch 24 (train)4.0s: train loss = 0.10252 = (mf)0.09779 + (embed)0.00473\n",
            "Epoch 25 (train)4.1s + (eval)0.5s: train loss = 0.09737 = (mf)0.09243 + (embed)0.00493, recall = 0.17164, ndcg = 0.27136\n",
            "Training finished in 112.62 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.27136, Recall@20: 0.17164\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.2s: train loss = 0.22585 = (mf)0.22426 + (embed)0.00159\n",
            "Epoch 2 (train)2.9s: train loss = 0.10928 = (mf)0.10574 + (embed)0.00355\n",
            "Epoch 3 (train)3.0s: train loss = 0.07935 = (mf)0.07404 + (embed)0.00532\n",
            "Epoch 4 (train)3.9s: train loss = 0.06684 = (mf)0.06000 + (embed)0.00684\n",
            "Epoch 5 (train)3.0s + (eval)0.4s: train loss = 0.05606 = (mf)0.04782 + (embed)0.00824, recall = 0.18406, ndcg = 0.26395\n",
            "Epoch 6 (train)2.9s: train loss = 0.05121 = (mf)0.04180 + (embed)0.00941\n",
            "Epoch 7 (train)2.9s: train loss = 0.05001 = (mf)0.03960 + (embed)0.01041\n",
            "Epoch 8 (train)3.8s: train loss = 0.04919 = (mf)0.03788 + (embed)0.01131\n",
            "Epoch 9 (train)3.0s: train loss = 0.04676 = (mf)0.03461 + (embed)0.01215\n",
            "Epoch 10 (train)2.9s + (eval)0.5s: train loss = 0.04611 = (mf)0.03318 + (embed)0.01293, recall = 0.17496, ndcg = 0.25141\n",
            "Epoch 11 (train)2.9s: train loss = 0.04384 = (mf)0.03023 + (embed)0.01361\n",
            "Epoch 12 (train)3.7s: train loss = 0.04472 = (mf)0.03049 + (embed)0.01423\n",
            "Epoch 13 (train)2.9s: train loss = 0.04653 = (mf)0.03170 + (embed)0.01482\n",
            "Epoch 14 (train)2.9s: train loss = 0.04502 = (mf)0.02963 + (embed)0.01539\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 0.04831 = (mf)0.03235 + (embed)0.01596, recall = 0.15857, ndcg = 0.23325\n",
            "Epoch 16 (train)3.7s: train loss = 0.04670 = (mf)0.03022 + (embed)0.01648\n",
            "Epoch 17 (train)2.9s: train loss = 0.04660 = (mf)0.02957 + (embed)0.01703\n",
            "Epoch 18 (train)2.9s: train loss = 0.04480 = (mf)0.02733 + (embed)0.01746\n",
            "Epoch 19 (train)2.9s: train loss = 0.04460 = (mf)0.02677 + (embed)0.01783\n",
            "Epoch 20 (train)3.8s + (eval)0.5s: train loss = 0.04608 = (mf)0.02783 + (embed)0.01826, recall = 0.15797, ndcg = 0.22983\n",
            "Epoch 21 (train)2.8s: train loss = 0.04695 = (mf)0.02828 + (embed)0.01866\n",
            "Epoch 22 (train)2.9s: train loss = 0.04534 = (mf)0.02637 + (embed)0.01898\n",
            "Epoch 23 (train)3.0s: train loss = 0.04782 = (mf)0.02842 + (embed)0.01940\n",
            "Epoch 24 (train)3.7s: train loss = 0.05025 = (mf)0.03043 + (embed)0.01983\n",
            "Epoch 25 (train)2.9s + (eval)0.2s: train loss = 0.04941 = (mf)0.02917 + (embed)0.02024, recall = 0.15599, ndcg = 0.22852\n",
            "Training finished in 80.04 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.22852, Recall@20: 0.15599\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.3s: train loss = 0.22170 = (mf)0.22006 + (embed)0.00164\n",
            "Epoch 2 (train)3.8s: train loss = 0.12730 = (mf)0.12367 + (embed)0.00363\n",
            "Epoch 3 (train)3.4s: train loss = 0.08971 = (mf)0.08423 + (embed)0.00548\n",
            "Epoch 4 (train)3.4s: train loss = 0.07208 = (mf)0.06475 + (embed)0.00732\n",
            "Epoch 5 (train)4.3s + (eval)0.6s: train loss = 0.06253 = (mf)0.05366 + (embed)0.00887, recall = 0.18474, ndcg = 0.27919\n",
            "Epoch 6 (train)3.4s: train loss = 0.05678 = (mf)0.04658 + (embed)0.01021\n",
            "Epoch 7 (train)3.4s: train loss = 0.05356 = (mf)0.04221 + (embed)0.01135\n",
            "Epoch 8 (train)4.3s: train loss = 0.04853 = (mf)0.03614 + (embed)0.01239\n",
            "Epoch 9 (train)3.5s: train loss = 0.04928 = (mf)0.03606 + (embed)0.01322\n",
            "Epoch 10 (train)3.5s + (eval)0.2s: train loss = 0.04449 = (mf)0.03057 + (embed)0.01392, recall = 0.18671, ndcg = 0.28015\n",
            "Epoch 11 (train)4.2s: train loss = 0.04598 = (mf)0.03141 + (embed)0.01457\n",
            "Epoch 12 (train)3.6s: train loss = 0.04212 = (mf)0.02693 + (embed)0.01519\n",
            "Epoch 13 (train)3.5s: train loss = 0.04390 = (mf)0.02817 + (embed)0.01573\n",
            "Epoch 14 (train)3.5s: train loss = 0.04251 = (mf)0.02625 + (embed)0.01627\n",
            "Epoch 15 (train)4.2s + (eval)0.2s: train loss = 0.04271 = (mf)0.02601 + (embed)0.01670, recall = 0.17963, ndcg = 0.25904\n",
            "Epoch 16 (train)3.5s: train loss = 0.04264 = (mf)0.02547 + (embed)0.01716\n",
            "Epoch 17 (train)3.5s: train loss = 0.04117 = (mf)0.02367 + (embed)0.01750\n",
            "Epoch 18 (train)4.4s: train loss = 0.04203 = (mf)0.02418 + (embed)0.01785\n",
            "Epoch 19 (train)3.5s: train loss = 0.04298 = (mf)0.02477 + (embed)0.01821\n",
            "Epoch 20 (train)3.5s + (eval)0.5s: train loss = 0.04050 = (mf)0.02192 + (embed)0.01858, recall = 0.17642, ndcg = 0.25555\n",
            "Epoch 21 (train)4.4s: train loss = 0.03980 = (mf)0.02097 + (embed)0.01883\n",
            "Epoch 22 (train)3.6s: train loss = 0.04132 = (mf)0.02227 + (embed)0.01905\n",
            "Epoch 23 (train)3.6s: train loss = 0.04020 = (mf)0.02083 + (embed)0.01937\n",
            "Epoch 24 (train)4.0s: train loss = 0.04063 = (mf)0.02108 + (embed)0.01955\n",
            "Epoch 25 (train)4.0s + (eval)0.2s: train loss = 0.04177 = (mf)0.02199 + (embed)0.01978, recall = 0.17744, ndcg = 0.25638\n",
            "Training finished in 96.13 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.25638, Recall@20: 0.17744\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.6s: train loss = 0.24581 = (mf)0.24423 + (embed)0.00159\n",
            "Epoch 2 (train)4.9s: train loss = 0.13730 = (mf)0.13319 + (embed)0.00410\n",
            "Epoch 3 (train)4.1s: train loss = 0.10293 = (mf)0.09684 + (embed)0.00608\n",
            "Epoch 4 (train)4.4s: train loss = 0.08680 = (mf)0.07879 + (embed)0.00802\n",
            "Epoch 5 (train)4.6s + (eval)0.6s: train loss = 0.07541 = (mf)0.06566 + (embed)0.00975, recall = 0.18488, ndcg = 0.27304\n",
            "Epoch 6 (train)4.1s: train loss = 0.06764 = (mf)0.05637 + (embed)0.01127\n",
            "Epoch 7 (train)4.8s: train loss = 0.06138 = (mf)0.04886 + (embed)0.01252\n",
            "Epoch 8 (train)4.0s: train loss = 0.05745 = (mf)0.04386 + (embed)0.01359\n",
            "Epoch 9 (train)4.1s: train loss = 0.05469 = (mf)0.04024 + (embed)0.01444\n",
            "Epoch 10 (train)4.9s + (eval)0.2s: train loss = 0.05255 = (mf)0.03729 + (embed)0.01526, recall = 0.19337, ndcg = 0.28652\n",
            "Epoch 11 (train)4.1s: train loss = 0.04974 = (mf)0.03381 + (embed)0.01593\n",
            "Epoch 12 (train)4.0s: train loss = 0.04767 = (mf)0.03121 + (embed)0.01646\n",
            "Epoch 13 (train)4.9s: train loss = 0.04904 = (mf)0.03224 + (embed)0.01680\n",
            "Epoch 14 (train)4.0s: train loss = 0.04710 = (mf)0.02984 + (embed)0.01726\n",
            "Epoch 15 (train)4.0s + (eval)0.5s: train loss = 0.04578 = (mf)0.02812 + (embed)0.01766, recall = 0.19307, ndcg = 0.28485\n",
            "Epoch 16 (train)4.9s: train loss = 0.04645 = (mf)0.02837 + (embed)0.01808\n",
            "Epoch 17 (train)4.0s: train loss = 0.04485 = (mf)0.02643 + (embed)0.01842\n",
            "Epoch 18 (train)4.0s: train loss = 0.04527 = (mf)0.02650 + (embed)0.01878\n",
            "Epoch 19 (train)4.9s: train loss = 0.04417 = (mf)0.02509 + (embed)0.01908\n",
            "Epoch 20 (train)4.1s + (eval)0.2s: train loss = 0.04340 = (mf)0.02400 + (embed)0.01940, recall = 0.19291, ndcg = 0.28102\n",
            "Epoch 21 (train)4.3s: train loss = 0.04309 = (mf)0.02348 + (embed)0.01961\n",
            "Epoch 22 (train)4.6s: train loss = 0.04304 = (mf)0.02320 + (embed)0.01984\n",
            "Epoch 23 (train)4.0s: train loss = 0.04317 = (mf)0.02319 + (embed)0.01998\n",
            "Epoch 24 (train)4.5s: train loss = 0.04199 = (mf)0.02177 + (embed)0.02022\n",
            "Epoch 25 (train)4.4s + (eval)0.5s: train loss = 0.04316 = (mf)0.02270 + (embed)0.02047, recall = 0.20159, ndcg = 0.28961\n",
            "Training finished in 111.16 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.28961, Recall@20: 0.20159\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.2s: train loss = 0.61028 = (mf)0.54674 + (embed)0.06355\n",
            "Epoch 2 (train)3.7s: train loss = 1.26392 = (mf)1.06308 + (embed)0.20085\n",
            "Epoch 3 (train)2.9s: train loss = 1.64503 = (mf)1.30664 + (embed)0.33839\n",
            "Epoch 4 (train)2.9s: train loss = 2.20110 = (mf)1.73806 + (embed)0.46303\n",
            "Epoch 5 (train)2.9s + (eval)0.5s: train loss = 2.44865 = (mf)1.86943 + (embed)0.57922, recall = 0.10468, ndcg = 0.16482\n",
            "Epoch 6 (train)3.6s: train loss = 2.69618 = (mf)2.01081 + (embed)0.68537\n",
            "Epoch 7 (train)2.9s: train loss = 2.81358 = (mf)2.03935 + (embed)0.77423\n",
            "Epoch 8 (train)2.9s: train loss = 2.97619 = (mf)2.11708 + (embed)0.85911\n",
            "Epoch 9 (train)3.1s: train loss = 3.38519 = (mf)2.43817 + (embed)0.94702\n",
            "Epoch 10 (train)3.7s + (eval)0.5s: train loss = 3.56706 = (mf)2.54143 + (embed)1.02562, recall = 0.11428, ndcg = 0.17756\n",
            "Epoch 11 (train)2.9s: train loss = 3.72057 = (mf)2.61757 + (embed)1.10300\n",
            "Epoch 12 (train)2.9s: train loss = 3.74268 = (mf)2.56327 + (embed)1.17941\n",
            "Epoch 13 (train)3.2s: train loss = 4.09688 = (mf)2.85480 + (embed)1.24208\n",
            "Epoch 14 (train)3.5s: train loss = 4.01266 = (mf)2.70692 + (embed)1.30574\n",
            "Epoch 15 (train)3.0s + (eval)0.2s: train loss = 4.05189 = (mf)2.68276 + (embed)1.36913, recall = 0.11222, ndcg = 0.17749\n",
            "Epoch 16 (train)2.9s: train loss = 4.38259 = (mf)2.95481 + (embed)1.42778\n",
            "Epoch 17 (train)3.4s: train loss = 4.48325 = (mf)3.00975 + (embed)1.47351\n",
            "Epoch 18 (train)3.3s: train loss = 4.40448 = (mf)2.88435 + (embed)1.52013\n",
            "Epoch 19 (train)2.9s: train loss = 4.50977 = (mf)2.94103 + (embed)1.56874\n",
            "Epoch 20 (train)2.9s + (eval)0.5s: train loss = 4.51241 = (mf)2.90095 + (embed)1.61146, recall = 0.10803, ndcg = 0.16955\n",
            "Epoch 21 (train)3.5s: train loss = 4.66565 = (mf)3.01349 + (embed)1.65216\n",
            "Epoch 22 (train)3.1s: train loss = 4.61987 = (mf)2.93197 + (embed)1.68789\n",
            "Epoch 23 (train)2.9s: train loss = 4.70400 = (mf)2.97348 + (embed)1.73052\n",
            "Epoch 24 (train)3.0s: train loss = 4.71185 = (mf)2.94333 + (embed)1.76852\n",
            "Epoch 25 (train)3.5s + (eval)0.3s: train loss = 4.82361 = (mf)3.01701 + (embed)1.80660, recall = 0.11597, ndcg = 0.17950\n",
            "Training finished in 80.86 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.17950, Recall@20: 0.11597\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.9s: train loss = 0.47995 = (mf)0.42878 + (embed)0.05118\n",
            "Epoch 2 (train)3.6s: train loss = 0.77437 = (mf)0.60923 + (embed)0.16513\n",
            "Epoch 3 (train)4.3s: train loss = 1.23736 = (mf)0.94926 + (embed)0.28810\n",
            "Epoch 4 (train)3.6s: train loss = 1.48918 = (mf)1.08396 + (embed)0.40522\n",
            "Epoch 5 (train)3.5s + (eval)0.4s: train loss = 1.78383 = (mf)1.27287 + (embed)0.51096, recall = 0.11021, ndcg = 0.17465\n",
            "Epoch 6 (train)4.4s: train loss = 1.89051 = (mf)1.28638 + (embed)0.60412\n",
            "Epoch 7 (train)3.5s: train loss = 2.04770 = (mf)1.36778 + (embed)0.67993\n",
            "Epoch 8 (train)3.5s: train loss = 2.17750 = (mf)1.42675 + (embed)0.75076\n",
            "Epoch 9 (train)4.4s: train loss = 2.24268 = (mf)1.42187 + (embed)0.82081\n",
            "Epoch 10 (train)3.5s + (eval)0.5s: train loss = 2.32450 = (mf)1.43776 + (embed)0.88675, recall = 0.10910, ndcg = 0.16974\n",
            "Epoch 11 (train)3.6s: train loss = 2.44196 = (mf)1.50012 + (embed)0.94183\n",
            "Epoch 12 (train)4.1s: train loss = 2.36284 = (mf)1.37683 + (embed)0.98601\n",
            "Epoch 13 (train)3.8s: train loss = 2.60935 = (mf)1.57215 + (embed)1.03720\n",
            "Epoch 14 (train)3.5s: train loss = 2.72032 = (mf)1.63534 + (embed)1.08498\n",
            "Epoch 15 (train)3.6s + (eval)0.3s: train loss = 2.62569 = (mf)1.50097 + (embed)1.12473, recall = 0.11153, ndcg = 0.17364\n",
            "Epoch 16 (train)4.2s: train loss = 2.72437 = (mf)1.56404 + (embed)1.16033\n",
            "Epoch 17 (train)3.5s: train loss = 2.72388 = (mf)1.52671 + (embed)1.19717\n",
            "Epoch 18 (train)3.6s: train loss = 2.76044 = (mf)1.52589 + (embed)1.23455\n",
            "Epoch 19 (train)4.4s: train loss = 2.87464 = (mf)1.61043 + (embed)1.26421\n",
            "Epoch 20 (train)3.6s + (eval)0.5s: train loss = 2.96651 = (mf)1.67221 + (embed)1.29430, recall = 0.10722, ndcg = 0.16415\n",
            "Epoch 21 (train)3.5s: train loss = 2.80201 = (mf)1.48430 + (embed)1.31771\n",
            "Epoch 22 (train)4.4s: train loss = 2.91583 = (mf)1.57336 + (embed)1.34247\n",
            "Epoch 23 (train)3.5s: train loss = 2.91822 = (mf)1.54755 + (embed)1.37067\n",
            "Epoch 24 (train)3.5s: train loss = 3.07137 = (mf)1.67091 + (embed)1.40046\n",
            "Epoch 25 (train)4.0s + (eval)0.3s: train loss = 3.02180 = (mf)1.59680 + (embed)1.42500, recall = 0.10255, ndcg = 0.16083\n",
            "Training finished in 96.94 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.16083, Recall@20: 0.10255\n",
            "--- Trying combination: Epochs=25, TopK=20, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.4s: train loss = 0.37154 = (mf)0.32676 + (embed)0.04478\n",
            "Epoch 2 (train)4.6s: train loss = 0.55807 = (mf)0.41728 + (embed)0.14079\n",
            "Epoch 3 (train)4.4s: train loss = 0.78714 = (mf)0.54568 + (embed)0.24146\n",
            "Epoch 4 (train)4.0s: train loss = 1.02516 = (mf)0.68118 + (embed)0.34398\n",
            "Epoch 5 (train)4.8s + (eval)0.8s: train loss = 1.13930 = (mf)0.70578 + (embed)0.43352, recall = 0.11691, ndcg = 0.17634\n",
            "Epoch 6 (train)4.1s: train loss = 1.30393 = (mf)0.78564 + (embed)0.51829\n",
            "Epoch 7 (train)4.0s: train loss = 1.40345 = (mf)0.81244 + (embed)0.59101\n",
            "Epoch 8 (train)4.9s: train loss = 1.56482 = (mf)0.91378 + (embed)0.65104\n",
            "Epoch 9 (train)4.1s: train loss = 1.60177 = (mf)0.89495 + (embed)0.70682\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 1.70500 = (mf)0.95157 + (embed)0.75344, recall = 0.10564, ndcg = 0.16377\n",
            "Epoch 11 (train)4.8s: train loss = 1.78476 = (mf)0.98141 + (embed)0.80335\n",
            "Epoch 12 (train)4.0s: train loss = 1.87646 = (mf)1.02981 + (embed)0.84665\n",
            "Epoch 13 (train)4.1s: train loss = 1.91287 = (mf)1.02147 + (embed)0.89140\n",
            "Epoch 14 (train)4.9s: train loss = 1.99532 = (mf)1.06484 + (embed)0.93048\n",
            "Epoch 15 (train)4.1s + (eval)0.5s: train loss = 1.98870 = (mf)1.02878 + (embed)0.95992, recall = 0.09710, ndcg = 0.15631\n",
            "Epoch 16 (train)4.5s: train loss = 1.98321 = (mf)0.99426 + (embed)0.98895\n",
            "Epoch 17 (train)4.6s: train loss = 2.05764 = (mf)1.04320 + (embed)1.01444\n",
            "Epoch 18 (train)4.1s: train loss = 2.09938 = (mf)1.05481 + (embed)1.04457\n",
            "Epoch 19 (train)4.8s: train loss = 2.14678 = (mf)1.07352 + (embed)1.07326\n",
            "Epoch 20 (train)4.3s + (eval)0.2s: train loss = 2.19160 = (mf)1.09950 + (embed)1.09209, recall = 0.11151, ndcg = 0.17100\n",
            "Epoch 21 (train)4.0s: train loss = 2.14205 = (mf)1.03173 + (embed)1.11031\n",
            "Epoch 22 (train)4.9s: train loss = 2.17219 = (mf)1.03903 + (embed)1.13316\n",
            "Epoch 23 (train)4.1s: train loss = 2.19344 = (mf)1.03924 + (embed)1.15420\n",
            "Epoch 24 (train)4.0s: train loss = 2.28314 = (mf)1.10979 + (embed)1.17336\n",
            "Epoch 25 (train)4.8s + (eval)0.2s: train loss = 2.26579 = (mf)1.07682 + (embed)1.18897, recall = 0.11463, ndcg = 0.17018\n",
            "Training finished in 111.34 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.17018, Recall@20: 0.11463\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.2s: train loss = 0.42189 = (mf)0.42166 + (embed)0.00024\n",
            "Epoch 2 (train)3.0s: train loss = 0.21701 = (mf)0.21635 + (embed)0.00065\n",
            "Epoch 3 (train)3.6s: train loss = 0.19499 = (mf)0.19422 + (embed)0.00077\n",
            "Epoch 4 (train)2.9s: train loss = 0.17437 = (mf)0.17344 + (embed)0.00092\n",
            "Epoch 5 (train)2.9s + (eval)0.7s: train loss = 0.16150 = (mf)0.16044 + (embed)0.00106, recall = 0.25827, ndcg = 0.26040\n",
            "Epoch 6 (train)3.4s: train loss = 0.15052 = (mf)0.14931 + (embed)0.00121\n",
            "Epoch 7 (train)3.4s: train loss = 0.14121 = (mf)0.13986 + (embed)0.00135\n",
            "Epoch 8 (train)2.9s: train loss = 0.13732 = (mf)0.13584 + (embed)0.00148\n",
            "Epoch 9 (train)2.9s: train loss = 0.12895 = (mf)0.12733 + (embed)0.00162\n",
            "Epoch 10 (train)3.2s + (eval)0.4s: train loss = 0.12084 = (mf)0.11909 + (embed)0.00175, recall = 0.27322, ndcg = 0.27066\n",
            "Epoch 11 (train)3.3s: train loss = 0.11748 = (mf)0.11558 + (embed)0.00189\n",
            "Epoch 12 (train)2.9s: train loss = 0.10959 = (mf)0.10755 + (embed)0.00205\n",
            "Epoch 13 (train)2.9s: train loss = 0.10830 = (mf)0.10611 + (embed)0.00219\n",
            "Epoch 14 (train)3.2s: train loss = 0.10200 = (mf)0.09967 + (embed)0.00234\n",
            "Epoch 15 (train)3.4s + (eval)0.5s: train loss = 0.09543 = (mf)0.09294 + (embed)0.00249, recall = 0.29020, ndcg = 0.28505\n",
            "Epoch 16 (train)2.9s: train loss = 0.09459 = (mf)0.09194 + (embed)0.00265\n",
            "Epoch 17 (train)2.9s: train loss = 0.09042 = (mf)0.08763 + (embed)0.00280\n",
            "Epoch 18 (train)3.4s: train loss = 0.08640 = (mf)0.08345 + (embed)0.00295\n",
            "Epoch 19 (train)3.3s: train loss = 0.08087 = (mf)0.07776 + (embed)0.00311\n",
            "Epoch 20 (train)2.9s + (eval)0.3s: train loss = 0.07748 = (mf)0.07420 + (embed)0.00328, recall = 0.30669, ndcg = 0.30588\n",
            "Epoch 21 (train)2.9s: train loss = 0.07510 = (mf)0.07167 + (embed)0.00342\n",
            "Epoch 22 (train)3.5s: train loss = 0.07288 = (mf)0.06930 + (embed)0.00358\n",
            "Epoch 23 (train)3.2s: train loss = 0.06891 = (mf)0.06518 + (embed)0.00373\n",
            "Epoch 24 (train)2.9s: train loss = 0.06651 = (mf)0.06262 + (embed)0.00389\n",
            "Epoch 25 (train)2.9s + (eval)0.5s: train loss = 0.06378 = (mf)0.05975 + (embed)0.00403, recall = 0.32402, ndcg = 0.32382\n",
            "Training finished in 80.00 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.32382, Recall@50: 0.32402\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.8s: train loss = 0.41340 = (mf)0.41315 + (embed)0.00025\n",
            "Epoch 2 (train)3.4s: train loss = 0.22639 = (mf)0.22574 + (embed)0.00065\n",
            "Epoch 3 (train)3.9s: train loss = 0.21515 = (mf)0.21440 + (embed)0.00074\n",
            "Epoch 4 (train)4.2s: train loss = 0.20395 = (mf)0.20308 + (embed)0.00087\n",
            "Epoch 5 (train)3.6s + (eval)0.5s: train loss = 0.19641 = (mf)0.19541 + (embed)0.00100, recall = 0.20944, ndcg = 0.20868\n",
            "Epoch 6 (train)3.7s: train loss = 0.18426 = (mf)0.18311 + (embed)0.00115\n",
            "Epoch 7 (train)4.1s: train loss = 0.17066 = (mf)0.16933 + (embed)0.00133\n",
            "Epoch 8 (train)3.5s: train loss = 0.15835 = (mf)0.15683 + (embed)0.00152\n",
            "Epoch 9 (train)3.4s: train loss = 0.14695 = (mf)0.14523 + (embed)0.00172\n",
            "Epoch 10 (train)4.4s + (eval)0.3s: train loss = 0.13922 = (mf)0.13731 + (embed)0.00191, recall = 0.26603, ndcg = 0.26065\n",
            "Epoch 11 (train)3.6s: train loss = 0.13447 = (mf)0.13238 + (embed)0.00208\n",
            "Epoch 12 (train)3.5s: train loss = 0.12907 = (mf)0.12682 + (embed)0.00224\n",
            "Epoch 13 (train)4.2s: train loss = 0.12482 = (mf)0.12241 + (embed)0.00241\n",
            "Epoch 14 (train)3.6s: train loss = 0.11880 = (mf)0.11621 + (embed)0.00259\n",
            "Epoch 15 (train)3.6s + (eval)0.5s: train loss = 0.11705 = (mf)0.11429 + (embed)0.00276, recall = 0.28359, ndcg = 0.27723\n",
            "Epoch 16 (train)4.1s: train loss = 0.10926 = (mf)0.10633 + (embed)0.00293\n",
            "Epoch 17 (train)3.8s: train loss = 0.10533 = (mf)0.10221 + (embed)0.00313\n",
            "Epoch 18 (train)3.5s: train loss = 0.10249 = (mf)0.09919 + (embed)0.00330\n",
            "Epoch 19 (train)3.5s: train loss = 0.10177 = (mf)0.09829 + (embed)0.00347\n",
            "Epoch 20 (train)4.3s + (eval)0.3s: train loss = 0.09547 = (mf)0.09183 + (embed)0.00364, recall = 0.29638, ndcg = 0.29026\n",
            "Epoch 21 (train)3.4s: train loss = 0.09217 = (mf)0.08836 + (embed)0.00382\n",
            "Epoch 22 (train)3.5s: train loss = 0.08905 = (mf)0.08506 + (embed)0.00399\n",
            "Epoch 23 (train)4.4s: train loss = 0.08703 = (mf)0.08287 + (embed)0.00416\n",
            "Epoch 24 (train)3.5s: train loss = 0.08148 = (mf)0.07711 + (embed)0.00437\n",
            "Epoch 25 (train)3.5s + (eval)0.5s: train loss = 0.08117 = (mf)0.07664 + (embed)0.00453, recall = 0.31367, ndcg = 0.31098\n",
            "Training finished in 96.11 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.31098, Recall@50: 0.31367\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.6s: train loss = 0.41404 = (mf)0.41379 + (embed)0.00025\n",
            "Epoch 2 (train)4.1s: train loss = 0.23205 = (mf)0.23140 + (embed)0.00065\n",
            "Epoch 3 (train)4.8s: train loss = 0.21886 = (mf)0.21808 + (embed)0.00078\n",
            "Epoch 4 (train)4.1s: train loss = 0.20313 = (mf)0.20219 + (embed)0.00094\n",
            "Epoch 5 (train)4.1s + (eval)0.5s: train loss = 0.18833 = (mf)0.18719 + (embed)0.00113, recall = 0.23589, ndcg = 0.23567\n",
            "Epoch 6 (train)4.9s: train loss = 0.17784 = (mf)0.17652 + (embed)0.00133\n",
            "Epoch 7 (train)4.0s: train loss = 0.16813 = (mf)0.16662 + (embed)0.00152\n",
            "Epoch 8 (train)4.0s: train loss = 0.16258 = (mf)0.16087 + (embed)0.00171\n",
            "Epoch 9 (train)4.9s: train loss = 0.15560 = (mf)0.15372 + (embed)0.00188\n",
            "Epoch 10 (train)4.1s + (eval)0.5s: train loss = 0.14917 = (mf)0.14710 + (embed)0.00207, recall = 0.25914, ndcg = 0.25818\n",
            "Epoch 11 (train)4.6s: train loss = 0.14446 = (mf)0.14223 + (embed)0.00223\n",
            "Epoch 12 (train)4.4s: train loss = 0.14205 = (mf)0.13964 + (embed)0.00241\n",
            "Epoch 13 (train)4.1s: train loss = 0.13847 = (mf)0.13589 + (embed)0.00258\n",
            "Epoch 14 (train)4.9s: train loss = 0.13293 = (mf)0.13020 + (embed)0.00273\n",
            "Epoch 15 (train)4.1s + (eval)0.3s: train loss = 0.13263 = (mf)0.12972 + (embed)0.00291, recall = 0.26430, ndcg = 0.26491\n",
            "Epoch 16 (train)4.0s: train loss = 0.12644 = (mf)0.12336 + (embed)0.00309\n",
            "Epoch 17 (train)4.9s: train loss = 0.12066 = (mf)0.11737 + (embed)0.00329\n",
            "Epoch 18 (train)4.0s: train loss = 0.12237 = (mf)0.11889 + (embed)0.00348\n",
            "Epoch 19 (train)4.0s: train loss = 0.11344 = (mf)0.10974 + (embed)0.00370\n",
            "Epoch 20 (train)4.9s + (eval)0.3s: train loss = 0.11154 = (mf)0.10759 + (embed)0.00395, recall = 0.28339, ndcg = 0.27917\n",
            "Epoch 21 (train)4.1s: train loss = 0.10750 = (mf)0.10335 + (embed)0.00415\n",
            "Epoch 22 (train)4.1s: train loss = 0.10375 = (mf)0.09935 + (embed)0.00440\n",
            "Epoch 23 (train)4.9s: train loss = 0.09988 = (mf)0.09527 + (embed)0.00460\n",
            "Epoch 24 (train)4.1s: train loss = 0.09759 = (mf)0.09276 + (embed)0.00483\n",
            "Epoch 25 (train)4.1s + (eval)0.7s: train loss = 0.09471 = (mf)0.08966 + (embed)0.00505, recall = 0.29744, ndcg = 0.29350\n",
            "Training finished in 111.05 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.29350, Recall@50: 0.29744\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.3s: train loss = 0.22348 = (mf)0.22182 + (embed)0.00166\n",
            "Epoch 2 (train)2.9s: train loss = 0.11036 = (mf)0.10676 + (embed)0.00360\n",
            "Epoch 3 (train)3.7s: train loss = 0.07826 = (mf)0.07284 + (embed)0.00542\n",
            "Epoch 4 (train)3.0s: train loss = 0.06205 = (mf)0.05515 + (embed)0.00690\n",
            "Epoch 5 (train)2.9s + (eval)0.5s: train loss = 0.05633 = (mf)0.04814 + (embed)0.00819, recall = 0.30104, ndcg = 0.28500\n",
            "Epoch 6 (train)2.9s: train loss = 0.05267 = (mf)0.04332 + (embed)0.00935\n",
            "Epoch 7 (train)3.8s: train loss = 0.04844 = (mf)0.03810 + (embed)0.01034\n",
            "Epoch 8 (train)2.9s: train loss = 0.04858 = (mf)0.03731 + (embed)0.01126\n",
            "Epoch 9 (train)3.0s: train loss = 0.04847 = (mf)0.03633 + (embed)0.01214\n",
            "Epoch 10 (train)3.0s + (eval)0.5s: train loss = 0.04414 = (mf)0.03123 + (embed)0.01292, recall = 0.28535, ndcg = 0.26518\n",
            "Epoch 11 (train)3.9s: train loss = 0.04526 = (mf)0.03166 + (embed)0.01360\n",
            "Epoch 12 (train)3.0s: train loss = 0.04615 = (mf)0.03191 + (embed)0.01424\n",
            "Epoch 13 (train)3.0s: train loss = 0.04552 = (mf)0.03074 + (embed)0.01479\n",
            "Epoch 14 (train)2.9s: train loss = 0.04630 = (mf)0.03089 + (embed)0.01541\n",
            "Epoch 15 (train)3.8s + (eval)0.3s: train loss = 0.04601 = (mf)0.03005 + (embed)0.01596, recall = 0.27902, ndcg = 0.26679\n",
            "Epoch 16 (train)2.9s: train loss = 0.04601 = (mf)0.02956 + (embed)0.01645\n",
            "Epoch 17 (train)2.9s: train loss = 0.04449 = (mf)0.02758 + (embed)0.01692\n",
            "Epoch 18 (train)2.9s: train loss = 0.04269 = (mf)0.02541 + (embed)0.01728\n",
            "Epoch 19 (train)3.8s: train loss = 0.04575 = (mf)0.02804 + (embed)0.01770\n",
            "Epoch 20 (train)2.9s + (eval)0.2s: train loss = 0.04484 = (mf)0.02681 + (embed)0.01802, recall = 0.27574, ndcg = 0.25876\n",
            "Epoch 21 (train)2.9s: train loss = 0.04494 = (mf)0.02653 + (embed)0.01842\n",
            "Epoch 22 (train)2.9s: train loss = 0.04804 = (mf)0.02920 + (embed)0.01884\n",
            "Epoch 23 (train)3.8s: train loss = 0.04879 = (mf)0.02962 + (embed)0.01916\n",
            "Epoch 24 (train)2.9s: train loss = 0.04738 = (mf)0.02784 + (embed)0.01953\n",
            "Epoch 25 (train)2.9s + (eval)0.6s: train loss = 0.04883 = (mf)0.02892 + (embed)0.01991, recall = 0.26899, ndcg = 0.25440\n",
            "Training finished in 80.75 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.25440, Recall@50: 0.26899\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.2s: train loss = 0.22442 = (mf)0.22280 + (embed)0.00162\n",
            "Epoch 2 (train)3.5s: train loss = 0.11792 = (mf)0.11417 + (embed)0.00375\n",
            "Epoch 3 (train)3.6s: train loss = 0.09256 = (mf)0.08692 + (embed)0.00563\n",
            "Epoch 4 (train)4.4s: train loss = 0.07247 = (mf)0.06505 + (embed)0.00742\n",
            "Epoch 5 (train)3.5s + (eval)0.5s: train loss = 0.06310 = (mf)0.05419 + (embed)0.00891, recall = 0.29757, ndcg = 0.29465\n",
            "Epoch 6 (train)3.5s: train loss = 0.05886 = (mf)0.04874 + (embed)0.01012\n",
            "Epoch 7 (train)4.3s: train loss = 0.05237 = (mf)0.04109 + (embed)0.01128\n",
            "Epoch 8 (train)3.4s: train loss = 0.05075 = (mf)0.03850 + (embed)0.01225\n",
            "Epoch 9 (train)3.4s: train loss = 0.04870 = (mf)0.03561 + (embed)0.01309\n",
            "Epoch 10 (train)4.1s + (eval)0.7s: train loss = 0.04634 = (mf)0.03255 + (embed)0.01379, recall = 0.29918, ndcg = 0.29096\n",
            "Epoch 11 (train)3.4s: train loss = 0.04688 = (mf)0.03238 + (embed)0.01449\n",
            "Epoch 12 (train)3.4s: train loss = 0.04404 = (mf)0.02889 + (embed)0.01515\n",
            "Epoch 13 (train)3.6s: train loss = 0.04421 = (mf)0.02850 + (embed)0.01571\n",
            "Epoch 14 (train)4.1s: train loss = 0.04386 = (mf)0.02765 + (embed)0.01621\n",
            "Epoch 15 (train)3.5s + (eval)0.3s: train loss = 0.04310 = (mf)0.02642 + (embed)0.01668, recall = 0.31095, ndcg = 0.29432\n",
            "Epoch 16 (train)3.5s: train loss = 0.04144 = (mf)0.02432 + (embed)0.01712\n",
            "Epoch 17 (train)4.3s: train loss = 0.04221 = (mf)0.02473 + (embed)0.01748\n",
            "Epoch 18 (train)3.5s: train loss = 0.04254 = (mf)0.02474 + (embed)0.01779\n",
            "Epoch 19 (train)3.5s: train loss = 0.04306 = (mf)0.02492 + (embed)0.01814\n",
            "Epoch 20 (train)4.2s + (eval)0.6s: train loss = 0.04320 = (mf)0.02460 + (embed)0.01860, recall = 0.30044, ndcg = 0.28682\n",
            "Epoch 21 (train)3.5s: train loss = 0.04107 = (mf)0.02218 + (embed)0.01889\n",
            "Epoch 22 (train)3.5s: train loss = 0.04341 = (mf)0.02428 + (embed)0.01913\n",
            "Epoch 23 (train)3.9s: train loss = 0.04341 = (mf)0.02405 + (embed)0.01937\n",
            "Epoch 24 (train)3.9s: train loss = 0.04113 = (mf)0.02150 + (embed)0.01962\n",
            "Epoch 25 (train)3.4s + (eval)0.2s: train loss = 0.04104 = (mf)0.02116 + (embed)0.01988, recall = 0.29653, ndcg = 0.27888\n",
            "Training finished in 95.32 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.27888, Recall@50: 0.29653\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.3s: train loss = 0.25302 = (mf)0.25151 + (embed)0.00151\n",
            "Epoch 2 (train)4.2s: train loss = 0.13767 = (mf)0.13370 + (embed)0.00396\n",
            "Epoch 3 (train)4.8s: train loss = 0.11139 = (mf)0.10544 + (embed)0.00595\n",
            "Epoch 4 (train)4.5s: train loss = 0.09003 = (mf)0.08221 + (embed)0.00781\n",
            "Epoch 5 (train)4.2s + (eval)0.5s: train loss = 0.07646 = (mf)0.06687 + (embed)0.00959, recall = 0.30426, ndcg = 0.29185\n",
            "Epoch 6 (train)5.2s: train loss = 0.06801 = (mf)0.05695 + (embed)0.01106\n",
            "Epoch 7 (train)4.2s: train loss = 0.06209 = (mf)0.04967 + (embed)0.01242\n",
            "Epoch 8 (train)4.2s: train loss = 0.05890 = (mf)0.04539 + (embed)0.01352\n",
            "Epoch 9 (train)5.1s: train loss = 0.05547 = (mf)0.04107 + (embed)0.01440\n",
            "Epoch 10 (train)4.3s + (eval)0.5s: train loss = 0.05266 = (mf)0.03741 + (embed)0.01525, recall = 0.31638, ndcg = 0.30367\n",
            "Epoch 11 (train)4.5s: train loss = 0.05236 = (mf)0.03641 + (embed)0.01595\n",
            "Epoch 12 (train)4.5s: train loss = 0.04937 = (mf)0.03278 + (embed)0.01659\n",
            "Epoch 13 (train)4.1s: train loss = 0.04752 = (mf)0.03044 + (embed)0.01708\n",
            "Epoch 14 (train)4.9s: train loss = 0.04762 = (mf)0.03018 + (embed)0.01744\n",
            "Epoch 15 (train)4.3s + (eval)0.3s: train loss = 0.04515 = (mf)0.02730 + (embed)0.01785, recall = 0.31899, ndcg = 0.30668\n",
            "Epoch 16 (train)4.2s: train loss = 0.04573 = (mf)0.02755 + (embed)0.01819\n",
            "Epoch 17 (train)5.1s: train loss = 0.04555 = (mf)0.02708 + (embed)0.01847\n",
            "Epoch 18 (train)4.1s: train loss = 0.04348 = (mf)0.02470 + (embed)0.01879\n",
            "Epoch 19 (train)4.1s: train loss = 0.04351 = (mf)0.02448 + (embed)0.01903\n",
            "Epoch 20 (train)5.0s + (eval)0.3s: train loss = 0.04314 = (mf)0.02386 + (embed)0.01928, recall = 0.31975, ndcg = 0.30610\n",
            "Epoch 21 (train)4.2s: train loss = 0.04188 = (mf)0.02236 + (embed)0.01952\n",
            "Epoch 22 (train)4.2s: train loss = 0.04244 = (mf)0.02267 + (embed)0.01977\n",
            "Epoch 23 (train)5.0s: train loss = 0.04179 = (mf)0.02181 + (embed)0.01998\n",
            "Epoch 24 (train)4.1s: train loss = 0.04197 = (mf)0.02176 + (embed)0.02021\n",
            "Epoch 25 (train)4.6s + (eval)0.7s: train loss = 0.04046 = (mf)0.02017 + (embed)0.02029, recall = 0.32293, ndcg = 0.30670\n",
            "Training finished in 114.94 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.30670, Recall@50: 0.32293\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.4s: train loss = 0.63100 = (mf)0.56701 + (embed)0.06399\n",
            "Epoch 2 (train)2.9s: train loss = 1.28390 = (mf)1.08210 + (embed)0.20180\n",
            "Epoch 3 (train)3.8s: train loss = 1.82347 = (mf)1.48318 + (embed)0.34029\n",
            "Epoch 4 (train)2.9s: train loss = 2.18547 = (mf)1.72177 + (embed)0.46371\n",
            "Epoch 5 (train)2.9s + (eval)0.5s: train loss = 2.50631 = (mf)1.92720 + (embed)0.57911, recall = 0.19171, ndcg = 0.19233\n",
            "Epoch 6 (train)3.0s: train loss = 2.89584 = (mf)2.20354 + (embed)0.69230\n",
            "Epoch 7 (train)3.7s: train loss = 2.97858 = (mf)2.18117 + (embed)0.79741\n",
            "Epoch 8 (train)2.9s: train loss = 3.17289 = (mf)2.29412 + (embed)0.87877\n",
            "Epoch 9 (train)2.9s: train loss = 3.20881 = (mf)2.25631 + (embed)0.95250\n",
            "Epoch 10 (train)3.0s + (eval)0.7s: train loss = 3.32305 = (mf)2.30502 + (embed)1.01803, recall = 0.20763, ndcg = 0.19843\n",
            "Epoch 11 (train)3.5s: train loss = 3.54014 = (mf)2.45502 + (embed)1.08512\n",
            "Epoch 12 (train)2.9s: train loss = 3.55918 = (mf)2.41053 + (embed)1.14864\n",
            "Epoch 13 (train)2.9s: train loss = 3.74928 = (mf)2.53230 + (embed)1.21698\n",
            "Epoch 14 (train)3.2s: train loss = 4.29352 = (mf)3.01166 + (embed)1.28186\n",
            "Epoch 15 (train)3.6s + (eval)0.3s: train loss = 4.22274 = (mf)2.88162 + (embed)1.34111, recall = 0.20554, ndcg = 0.19624\n",
            "Epoch 16 (train)3.0s: train loss = 4.30617 = (mf)2.91239 + (embed)1.39378\n",
            "Epoch 17 (train)3.1s: train loss = 4.37085 = (mf)2.92763 + (embed)1.44322\n",
            "Epoch 18 (train)3.7s: train loss = 4.32134 = (mf)2.83006 + (embed)1.49128\n",
            "Epoch 19 (train)3.2s: train loss = 4.43312 = (mf)2.89672 + (embed)1.53640\n",
            "Epoch 20 (train)3.0s + (eval)0.3s: train loss = 4.22072 = (mf)2.64296 + (embed)1.57776, recall = 0.21402, ndcg = 0.19768\n",
            "Epoch 21 (train)3.0s: train loss = 4.52410 = (mf)2.90107 + (embed)1.62304\n",
            "Epoch 22 (train)3.7s: train loss = 4.45584 = (mf)2.79278 + (embed)1.66306\n",
            "Epoch 23 (train)3.1s: train loss = 4.69483 = (mf)2.99111 + (embed)1.70371\n",
            "Epoch 24 (train)3.0s: train loss = 4.72637 = (mf)2.98663 + (embed)1.73973\n",
            "Epoch 25 (train)2.9s + (eval)0.5s: train loss = 4.81034 = (mf)3.03242 + (embed)1.77792, recall = 0.21425, ndcg = 0.20119\n",
            "Training finished in 81.27 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.20119, Recall@50: 0.21425\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.1s: train loss = 0.42575 = (mf)0.37369 + (embed)0.05206\n",
            "Epoch 2 (train)3.6s: train loss = 0.79800 = (mf)0.62896 + (embed)0.16904\n",
            "Epoch 3 (train)4.5s: train loss = 1.15998 = (mf)0.86588 + (embed)0.29410\n",
            "Epoch 4 (train)3.6s: train loss = 1.47119 = (mf)1.05826 + (embed)0.41293\n",
            "Epoch 5 (train)3.6s + (eval)0.5s: train loss = 1.59486 = (mf)1.08257 + (embed)0.51230, recall = 0.19999, ndcg = 0.19450\n",
            "Epoch 6 (train)4.6s: train loss = 1.84969 = (mf)1.24788 + (embed)0.60181\n",
            "Epoch 7 (train)3.6s: train loss = 1.95729 = (mf)1.27485 + (embed)0.68244\n",
            "Epoch 8 (train)3.6s: train loss = 2.12883 = (mf)1.37561 + (embed)0.75323\n",
            "Epoch 9 (train)4.3s: train loss = 2.22031 = (mf)1.40178 + (embed)0.81853\n",
            "Epoch 10 (train)3.7s + (eval)0.5s: train loss = 2.43908 = (mf)1.55892 + (embed)0.88016, recall = 0.18794, ndcg = 0.17745\n",
            "Epoch 11 (train)3.5s: train loss = 2.46487 = (mf)1.52438 + (embed)0.94049\n",
            "Epoch 12 (train)3.8s: train loss = 2.45184 = (mf)1.45713 + (embed)0.99471\n",
            "Epoch 13 (train)4.0s: train loss = 2.49862 = (mf)1.45145 + (embed)1.04717\n",
            "Epoch 14 (train)3.5s: train loss = 2.53337 = (mf)1.43904 + (embed)1.09433\n",
            "Epoch 15 (train)3.5s + (eval)0.3s: train loss = 2.79515 = (mf)1.65430 + (embed)1.14086, recall = 0.20773, ndcg = 0.19077\n",
            "Epoch 16 (train)4.3s: train loss = 2.82153 = (mf)1.63736 + (embed)1.18417\n",
            "Epoch 17 (train)3.5s: train loss = 2.80379 = (mf)1.57743 + (embed)1.22636\n",
            "Epoch 18 (train)3.5s: train loss = 2.79332 = (mf)1.53202 + (embed)1.26130\n",
            "Epoch 19 (train)4.3s: train loss = 3.02329 = (mf)1.73282 + (embed)1.29046\n",
            "Epoch 20 (train)3.6s + (eval)0.3s: train loss = 2.89670 = (mf)1.57453 + (embed)1.32217, recall = 0.20711, ndcg = 0.20157\n",
            "Epoch 21 (train)3.5s: train loss = 2.78023 = (mf)1.43285 + (embed)1.34738\n",
            "Epoch 22 (train)4.0s: train loss = 2.88420 = (mf)1.51910 + (embed)1.36510\n",
            "Epoch 23 (train)3.9s: train loss = 3.01063 = (mf)1.62140 + (embed)1.38922\n",
            "Epoch 24 (train)3.5s: train loss = 2.97167 = (mf)1.55005 + (embed)1.42163\n",
            "Epoch 25 (train)3.5s + (eval)0.7s: train loss = 3.11306 = (mf)1.66688 + (embed)1.44617, recall = 0.19633, ndcg = 0.19238\n",
            "Training finished in 97.34 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19238, Recall@50: 0.19633\n",
            "--- Trying combination: Epochs=25, TopK=50, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.5s: train loss = 0.35262 = (mf)0.30794 + (embed)0.04469\n",
            "Epoch 2 (train)4.8s: train loss = 0.59320 = (mf)0.44407 + (embed)0.14913\n",
            "Epoch 3 (train)4.2s: train loss = 0.81219 = (mf)0.55580 + (embed)0.25639\n",
            "Epoch 4 (train)4.0s: train loss = 1.01325 = (mf)0.66370 + (embed)0.34954\n",
            "Epoch 5 (train)4.9s + (eval)0.5s: train loss = 1.16110 = (mf)0.72330 + (embed)0.43780, recall = 0.22017, ndcg = 0.20790\n",
            "Epoch 6 (train)4.1s: train loss = 1.32021 = (mf)0.80536 + (embed)0.51485\n",
            "Epoch 7 (train)4.1s: train loss = 1.48187 = (mf)0.89488 + (embed)0.58699\n",
            "Epoch 8 (train)4.9s: train loss = 1.57599 = (mf)0.91450 + (embed)0.66149\n",
            "Epoch 9 (train)4.1s: train loss = 1.63457 = (mf)0.91257 + (embed)0.72200\n",
            "Epoch 10 (train)4.0s + (eval)0.6s: train loss = 1.75418 = (mf)0.97781 + (embed)0.77637, recall = 0.17925, ndcg = 0.16811\n",
            "Epoch 11 (train)4.8s: train loss = 1.88584 = (mf)1.06109 + (embed)0.82475\n",
            "Epoch 12 (train)4.1s: train loss = 1.89919 = (mf)1.03416 + (embed)0.86503\n",
            "Epoch 13 (train)4.1s: train loss = 1.98929 = (mf)1.09182 + (embed)0.89747\n",
            "Epoch 14 (train)4.8s: train loss = 2.00321 = (mf)1.06832 + (embed)0.93490\n",
            "Epoch 15 (train)4.0s + (eval)0.3s: train loss = 2.06653 = (mf)1.10123 + (embed)0.96530, recall = 0.18944, ndcg = 0.17087\n",
            "Epoch 16 (train)4.5s: train loss = 2.09232 = (mf)1.09125 + (embed)1.00107\n",
            "Epoch 17 (train)4.5s: train loss = 2.16186 = (mf)1.12827 + (embed)1.03359\n",
            "Epoch 18 (train)4.0s: train loss = 2.12646 = (mf)1.06387 + (embed)1.06259\n",
            "Epoch 19 (train)4.7s: train loss = 2.07586 = (mf)0.99421 + (embed)1.08165\n",
            "Epoch 20 (train)4.3s + (eval)0.3s: train loss = 2.12468 = (mf)1.02032 + (embed)1.10436, recall = 0.20821, ndcg = 0.18883\n",
            "Epoch 21 (train)4.1s: train loss = 2.12478 = (mf)0.99994 + (embed)1.12484\n",
            "Epoch 22 (train)5.0s: train loss = 2.22625 = (mf)1.08374 + (embed)1.14251\n",
            "Epoch 23 (train)4.1s: train loss = 2.26173 = (mf)1.10486 + (embed)1.15688\n",
            "Epoch 24 (train)4.0s: train loss = 2.12916 = (mf)0.95560 + (embed)1.17357\n",
            "Epoch 25 (train)4.8s + (eval)0.6s: train loss = 2.22249 = (mf)1.03292 + (embed)1.18957, recall = 0.20011, ndcg = 0.18467\n",
            "Training finished in 111.75 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.18467, Recall@50: 0.20011\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.4s: train loss = 0.42070 = (mf)0.42046 + (embed)0.00024\n",
            "Epoch 2 (train)3.7s: train loss = 0.22059 = (mf)0.21994 + (embed)0.00064\n",
            "Epoch 3 (train)2.9s: train loss = 0.21091 = (mf)0.21017 + (embed)0.00073\n",
            "Epoch 4 (train)3.0s: train loss = 0.19491 = (mf)0.19406 + (embed)0.00085\n",
            "Epoch 5 (train)2.9s + (eval)0.5s: train loss = 0.17986 = (mf)0.17888 + (embed)0.00098, recall = 0.08296, ndcg = 0.22935\n",
            "Epoch 6 (train)3.8s: train loss = 0.16248 = (mf)0.16135 + (embed)0.00114\n",
            "Epoch 7 (train)2.9s: train loss = 0.14846 = (mf)0.14716 + (embed)0.00131\n",
            "Epoch 8 (train)2.9s: train loss = 0.14132 = (mf)0.13986 + (embed)0.00146\n",
            "Epoch 9 (train)2.9s: train loss = 0.13269 = (mf)0.13107 + (embed)0.00162\n",
            "Epoch 10 (train)3.8s + (eval)0.2s: train loss = 0.12491 = (mf)0.12313 + (embed)0.00177, recall = 0.10083, ndcg = 0.26369\n",
            "Epoch 11 (train)2.9s: train loss = 0.11868 = (mf)0.11675 + (embed)0.00192\n",
            "Epoch 12 (train)2.9s: train loss = 0.11085 = (mf)0.10877 + (embed)0.00207\n",
            "Epoch 13 (train)2.8s: train loss = 0.10856 = (mf)0.10634 + (embed)0.00222\n",
            "Epoch 14 (train)3.8s: train loss = 0.10206 = (mf)0.09969 + (embed)0.00237\n",
            "Epoch 15 (train)2.9s + (eval)0.5s: train loss = 0.09689 = (mf)0.09438 + (embed)0.00251, recall = 0.10922, ndcg = 0.28044\n",
            "Epoch 16 (train)2.8s: train loss = 0.09549 = (mf)0.09282 + (embed)0.00267\n",
            "Epoch 17 (train)2.8s: train loss = 0.08806 = (mf)0.08524 + (embed)0.00282\n",
            "Epoch 18 (train)3.8s: train loss = 0.08446 = (mf)0.08147 + (embed)0.00299\n",
            "Epoch 19 (train)2.8s: train loss = 0.08080 = (mf)0.07765 + (embed)0.00315\n",
            "Epoch 20 (train)2.8s + (eval)0.2s: train loss = 0.08077 = (mf)0.07747 + (embed)0.00331, recall = 0.11960, ndcg = 0.29265\n",
            "Epoch 21 (train)2.9s: train loss = 0.07394 = (mf)0.07048 + (embed)0.00346\n",
            "Epoch 22 (train)3.8s: train loss = 0.07208 = (mf)0.06847 + (embed)0.00361\n",
            "Epoch 23 (train)3.0s: train loss = 0.06936 = (mf)0.06559 + (embed)0.00376\n",
            "Epoch 24 (train)3.0s: train loss = 0.06678 = (mf)0.06287 + (embed)0.00391\n",
            "Epoch 25 (train)2.9s + (eval)0.2s: train loss = 0.06539 = (mf)0.06133 + (embed)0.00406, recall = 0.12608, ndcg = 0.30898\n",
            "Epoch 26 (train)3.8s: train loss = 0.06141 = (mf)0.05721 + (embed)0.00421\n",
            "Epoch 27 (train)2.9s: train loss = 0.05975 = (mf)0.05542 + (embed)0.00434\n",
            "Epoch 28 (train)2.9s: train loss = 0.05860 = (mf)0.05412 + (embed)0.00448\n",
            "Epoch 29 (train)2.9s: train loss = 0.05726 = (mf)0.05264 + (embed)0.00462\n",
            "Epoch 30 (train)3.7s + (eval)0.5s: train loss = 0.05617 = (mf)0.05144 + (embed)0.00473, recall = 0.13097, ndcg = 0.31726\n",
            "Epoch 31 (train)2.9s: train loss = 0.05320 = (mf)0.04834 + (embed)0.00486\n",
            "Epoch 32 (train)2.9s: train loss = 0.05311 = (mf)0.04811 + (embed)0.00500\n",
            "Epoch 33 (train)3.2s: train loss = 0.05284 = (mf)0.04774 + (embed)0.00510\n",
            "Epoch 34 (train)3.5s: train loss = 0.05120 = (mf)0.04599 + (embed)0.00521\n",
            "Epoch 35 (train)2.9s + (eval)0.2s: train loss = 0.04838 = (mf)0.04304 + (embed)0.00534, recall = 0.13642, ndcg = 0.33302\n",
            "Epoch 36 (train)2.9s: train loss = 0.04709 = (mf)0.04165 + (embed)0.00544\n",
            "Epoch 37 (train)3.2s: train loss = 0.04634 = (mf)0.04078 + (embed)0.00556\n",
            "Epoch 38 (train)3.5s: train loss = 0.04738 = (mf)0.04175 + (embed)0.00563\n",
            "Epoch 39 (train)2.9s: train loss = 0.04514 = (mf)0.03940 + (embed)0.00574\n",
            "Epoch 40 (train)2.9s + (eval)0.5s: train loss = 0.04549 = (mf)0.03965 + (embed)0.00584, recall = 0.13487, ndcg = 0.33018\n",
            "Epoch 41 (train)3.3s: train loss = 0.04392 = (mf)0.03799 + (embed)0.00593\n",
            "Epoch 42 (train)3.3s: train loss = 0.04359 = (mf)0.03757 + (embed)0.00602\n",
            "Epoch 43 (train)2.9s: train loss = 0.04225 = (mf)0.03612 + (embed)0.00613\n",
            "Epoch 44 (train)2.9s: train loss = 0.04209 = (mf)0.03589 + (embed)0.00620\n",
            "Epoch 45 (train)3.3s + (eval)0.3s: train loss = 0.04031 = (mf)0.03402 + (embed)0.00630, recall = 0.13920, ndcg = 0.33829\n",
            "Epoch 46 (train)3.3s: train loss = 0.04028 = (mf)0.03391 + (embed)0.00637\n",
            "Epoch 47 (train)2.9s: train loss = 0.03998 = (mf)0.03354 + (embed)0.00644\n",
            "Epoch 48 (train)2.9s: train loss = 0.03946 = (mf)0.03292 + (embed)0.00653\n",
            "Epoch 49 (train)3.2s: train loss = 0.03860 = (mf)0.03199 + (embed)0.00661\n",
            "Epoch 50 (train)3.4s + (eval)0.5s: train loss = 0.03741 = (mf)0.03072 + (embed)0.00669, recall = 0.13652, ndcg = 0.33644\n",
            "Training finished in 160.00 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.33644, Recall@10: 0.13652\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.2s: train loss = 0.41626 = (mf)0.41601 + (embed)0.00025\n",
            "Epoch 2 (train)4.1s: train loss = 0.23266 = (mf)0.23202 + (embed)0.00064\n",
            "Epoch 3 (train)3.5s: train loss = 0.21686 = (mf)0.21612 + (embed)0.00074\n",
            "Epoch 4 (train)3.5s: train loss = 0.20211 = (mf)0.20123 + (embed)0.00088\n",
            "Epoch 5 (train)4.3s + (eval)0.8s: train loss = 0.19164 = (mf)0.19063 + (embed)0.00102, recall = 0.07613, ndcg = 0.21997\n",
            "Epoch 6 (train)3.5s: train loss = 0.17844 = (mf)0.17724 + (embed)0.00119\n",
            "Epoch 7 (train)3.5s: train loss = 0.16788 = (mf)0.16650 + (embed)0.00138\n",
            "Epoch 8 (train)4.3s: train loss = 0.15288 = (mf)0.15132 + (embed)0.00156\n",
            "Epoch 9 (train)3.5s: train loss = 0.14421 = (mf)0.14244 + (embed)0.00176\n",
            "Epoch 10 (train)3.5s + (eval)0.2s: train loss = 0.13762 = (mf)0.13569 + (embed)0.00194, recall = 0.09847, ndcg = 0.25936\n",
            "Epoch 11 (train)3.9s: train loss = 0.13214 = (mf)0.13002 + (embed)0.00212\n",
            "Epoch 12 (train)4.0s: train loss = 0.13116 = (mf)0.12889 + (embed)0.00227\n",
            "Epoch 13 (train)3.5s: train loss = 0.12496 = (mf)0.12255 + (embed)0.00241\n",
            "Epoch 14 (train)3.4s: train loss = 0.12025 = (mf)0.11766 + (embed)0.00259\n",
            "Epoch 15 (train)4.3s + (eval)0.5s: train loss = 0.11463 = (mf)0.11188 + (embed)0.00275, recall = 0.10167, ndcg = 0.26954\n",
            "Epoch 16 (train)3.5s: train loss = 0.11192 = (mf)0.10903 + (embed)0.00290\n",
            "Epoch 17 (train)3.5s: train loss = 0.10637 = (mf)0.10328 + (embed)0.00309\n",
            "Epoch 18 (train)4.3s: train loss = 0.10023 = (mf)0.09697 + (embed)0.00326\n",
            "Epoch 19 (train)3.5s: train loss = 0.09829 = (mf)0.09486 + (embed)0.00342\n",
            "Epoch 20 (train)3.4s + (eval)0.2s: train loss = 0.09597 = (mf)0.09237 + (embed)0.00360, recall = 0.11130, ndcg = 0.28613\n",
            "Epoch 21 (train)4.0s: train loss = 0.09295 = (mf)0.08918 + (embed)0.00377\n",
            "Epoch 22 (train)3.8s: train loss = 0.08646 = (mf)0.08250 + (embed)0.00396\n",
            "Epoch 23 (train)3.4s: train loss = 0.08582 = (mf)0.08170 + (embed)0.00412\n",
            "Epoch 24 (train)3.4s: train loss = 0.08344 = (mf)0.07912 + (embed)0.00431\n",
            "Epoch 25 (train)4.3s + (eval)0.2s: train loss = 0.08131 = (mf)0.07683 + (embed)0.00448, recall = 0.11889, ndcg = 0.29786\n",
            "Epoch 26 (train)3.4s: train loss = 0.07792 = (mf)0.07327 + (embed)0.00465\n",
            "Epoch 27 (train)3.5s: train loss = 0.07631 = (mf)0.07150 + (embed)0.00481\n",
            "Epoch 28 (train)4.3s: train loss = 0.07221 = (mf)0.06723 + (embed)0.00498\n",
            "Epoch 29 (train)3.4s: train loss = 0.07167 = (mf)0.06652 + (embed)0.00515\n",
            "Epoch 30 (train)3.4s + (eval)0.5s: train loss = 0.07070 = (mf)0.06539 + (embed)0.00531, recall = 0.12464, ndcg = 0.31389\n",
            "Epoch 31 (train)3.9s: train loss = 0.06793 = (mf)0.06247 + (embed)0.00546\n",
            "Epoch 32 (train)3.8s: train loss = 0.06643 = (mf)0.06083 + (embed)0.00561\n",
            "Epoch 33 (train)3.5s: train loss = 0.06632 = (mf)0.06057 + (embed)0.00575\n",
            "Epoch 34 (train)3.4s: train loss = 0.06047 = (mf)0.05456 + (embed)0.00590\n",
            "Epoch 35 (train)4.3s + (eval)0.2s: train loss = 0.06118 = (mf)0.05513 + (embed)0.00605, recall = 0.13269, ndcg = 0.32703\n",
            "Epoch 36 (train)3.4s: train loss = 0.06061 = (mf)0.05441 + (embed)0.00619\n",
            "Epoch 37 (train)3.4s: train loss = 0.05896 = (mf)0.05264 + (embed)0.00632\n",
            "Epoch 38 (train)4.3s: train loss = 0.05811 = (mf)0.05166 + (embed)0.00646\n",
            "Epoch 39 (train)3.4s: train loss = 0.05767 = (mf)0.05110 + (embed)0.00657\n",
            "Epoch 40 (train)3.4s + (eval)0.5s: train loss = 0.05694 = (mf)0.05025 + (embed)0.00670, recall = 0.13684, ndcg = 0.33590\n",
            "Epoch 41 (train)3.9s: train loss = 0.05576 = (mf)0.04896 + (embed)0.00681\n",
            "Epoch 42 (train)3.8s: train loss = 0.05539 = (mf)0.04849 + (embed)0.00690\n",
            "Epoch 43 (train)3.4s: train loss = 0.05320 = (mf)0.04617 + (embed)0.00703\n",
            "Epoch 44 (train)3.5s: train loss = 0.05349 = (mf)0.04635 + (embed)0.00714\n",
            "Epoch 45 (train)4.3s + (eval)0.2s: train loss = 0.05114 = (mf)0.04390 + (embed)0.00724, recall = 0.13666, ndcg = 0.33676\n",
            "Epoch 46 (train)3.6s: train loss = 0.04993 = (mf)0.04258 + (embed)0.00735\n",
            "Epoch 47 (train)3.4s: train loss = 0.04905 = (mf)0.04160 + (embed)0.00745\n",
            "Epoch 48 (train)4.3s: train loss = 0.04891 = (mf)0.04135 + (embed)0.00756\n",
            "Epoch 49 (train)3.5s: train loss = 0.04946 = (mf)0.04181 + (embed)0.00765\n",
            "Epoch 50 (train)3.5s + (eval)0.2s: train loss = 0.04840 = (mf)0.04066 + (embed)0.00774, recall = 0.14272, ndcg = 0.34517\n",
            "Training finished in 189.78 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.34517, Recall@10: 0.14272\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.7s: train loss = 0.41277 = (mf)0.41252 + (embed)0.00025\n",
            "Epoch 2 (train)4.1s: train loss = 0.23605 = (mf)0.23541 + (embed)0.00064\n",
            "Epoch 3 (train)4.9s: train loss = 0.23005 = (mf)0.22931 + (embed)0.00074\n",
            "Epoch 4 (train)4.1s: train loss = 0.21864 = (mf)0.21777 + (embed)0.00086\n",
            "Epoch 5 (train)4.1s + (eval)0.8s: train loss = 0.21087 = (mf)0.20985 + (embed)0.00102, recall = 0.06981, ndcg = 0.20483\n",
            "Epoch 6 (train)4.9s: train loss = 0.20103 = (mf)0.19984 + (embed)0.00119\n",
            "Epoch 7 (train)4.2s: train loss = 0.19100 = (mf)0.18961 + (embed)0.00139\n",
            "Epoch 8 (train)4.1s: train loss = 0.17748 = (mf)0.17586 + (embed)0.00162\n",
            "Epoch 9 (train)4.8s: train loss = 0.15953 = (mf)0.15767 + (embed)0.00186\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 0.15377 = (mf)0.15167 + (embed)0.00210, recall = 0.09693, ndcg = 0.25540\n",
            "Epoch 11 (train)4.4s: train loss = 0.14800 = (mf)0.14571 + (embed)0.00230\n",
            "Epoch 12 (train)4.5s: train loss = 0.14249 = (mf)0.13998 + (embed)0.00251\n",
            "Epoch 13 (train)4.0s: train loss = 0.13999 = (mf)0.13730 + (embed)0.00269\n",
            "Epoch 14 (train)4.4s: train loss = 0.13770 = (mf)0.13484 + (embed)0.00286\n",
            "Epoch 15 (train)4.4s + (eval)0.2s: train loss = 0.13084 = (mf)0.12779 + (embed)0.00305, recall = 0.10104, ndcg = 0.26146\n",
            "Epoch 16 (train)4.0s: train loss = 0.12740 = (mf)0.12418 + (embed)0.00321\n",
            "Epoch 17 (train)4.6s: train loss = 0.12767 = (mf)0.12429 + (embed)0.00338\n",
            "Epoch 18 (train)4.3s: train loss = 0.12120 = (mf)0.11764 + (embed)0.00356\n",
            "Epoch 19 (train)4.1s: train loss = 0.11640 = (mf)0.11267 + (embed)0.00374\n",
            "Epoch 20 (train)4.9s + (eval)0.5s: train loss = 0.11408 = (mf)0.11014 + (embed)0.00393, recall = 0.10791, ndcg = 0.27425\n",
            "Epoch 21 (train)4.0s: train loss = 0.10975 = (mf)0.10562 + (embed)0.00414\n",
            "Epoch 22 (train)4.0s: train loss = 0.10765 = (mf)0.10331 + (embed)0.00434\n",
            "Epoch 23 (train)4.8s: train loss = 0.10492 = (mf)0.10039 + (embed)0.00453\n",
            "Epoch 24 (train)4.0s: train loss = 0.10120 = (mf)0.09647 + (embed)0.00473\n",
            "Epoch 25 (train)4.0s + (eval)0.2s: train loss = 0.09855 = (mf)0.09362 + (embed)0.00493, recall = 0.11454, ndcg = 0.28591\n",
            "Epoch 26 (train)4.8s: train loss = 0.09617 = (mf)0.09104 + (embed)0.00513\n",
            "Epoch 27 (train)4.0s: train loss = 0.09148 = (mf)0.08616 + (embed)0.00532\n",
            "Epoch 28 (train)3.9s: train loss = 0.08683 = (mf)0.08127 + (embed)0.00556\n",
            "Epoch 29 (train)4.8s: train loss = 0.08786 = (mf)0.08212 + (embed)0.00574\n",
            "Epoch 30 (train)4.0s + (eval)0.5s: train loss = 0.08679 = (mf)0.08088 + (embed)0.00591, recall = 0.12125, ndcg = 0.29927\n",
            "Epoch 31 (train)4.0s: train loss = 0.08404 = (mf)0.07795 + (embed)0.00610\n",
            "Epoch 32 (train)4.9s: train loss = 0.08185 = (mf)0.07557 + (embed)0.00627\n",
            "Epoch 33 (train)4.0s: train loss = 0.07979 = (mf)0.07334 + (embed)0.00646\n",
            "Epoch 34 (train)4.2s: train loss = 0.07772 = (mf)0.07108 + (embed)0.00664\n",
            "Epoch 35 (train)4.7s + (eval)0.2s: train loss = 0.07744 = (mf)0.07066 + (embed)0.00678, recall = 0.12741, ndcg = 0.31781\n",
            "Epoch 36 (train)4.0s: train loss = 0.07264 = (mf)0.06566 + (embed)0.00698\n",
            "Epoch 37 (train)4.3s: train loss = 0.07590 = (mf)0.06881 + (embed)0.00709\n",
            "Epoch 38 (train)4.5s: train loss = 0.07293 = (mf)0.06567 + (embed)0.00726\n",
            "Epoch 39 (train)4.0s: train loss = 0.07267 = (mf)0.06527 + (embed)0.00740\n",
            "Epoch 40 (train)4.4s + (eval)0.7s: train loss = 0.06926 = (mf)0.06174 + (embed)0.00752, recall = 0.13041, ndcg = 0.32458\n",
            "Epoch 41 (train)4.1s: train loss = 0.06745 = (mf)0.05977 + (embed)0.00768\n",
            "Epoch 42 (train)4.0s: train loss = 0.06822 = (mf)0.06039 + (embed)0.00784\n",
            "Epoch 43 (train)4.7s: train loss = 0.06699 = (mf)0.05901 + (embed)0.00798\n",
            "Epoch 44 (train)4.2s: train loss = 0.06598 = (mf)0.05786 + (embed)0.00812\n",
            "Epoch 45 (train)4.1s + (eval)0.2s: train loss = 0.06443 = (mf)0.05617 + (embed)0.00826, recall = 0.13471, ndcg = 0.33167\n",
            "Epoch 46 (train)4.9s: train loss = 0.06410 = (mf)0.05574 + (embed)0.00836\n",
            "Epoch 47 (train)4.1s: train loss = 0.06286 = (mf)0.05437 + (embed)0.00849\n",
            "Epoch 48 (train)4.1s: train loss = 0.06313 = (mf)0.05454 + (embed)0.00859\n",
            "Epoch 49 (train)4.8s: train loss = 0.06096 = (mf)0.05224 + (embed)0.00872\n",
            "Epoch 50 (train)4.0s + (eval)0.2s: train loss = 0.06005 = (mf)0.05122 + (embed)0.00883, recall = 0.13744, ndcg = 0.33520\n",
            "Training finished in 219.63 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.33520, Recall@10: 0.13744\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.4s: train loss = 0.22473 = (mf)0.22314 + (embed)0.00159\n",
            "Epoch 2 (train)2.9s: train loss = 0.11203 = (mf)0.10855 + (embed)0.00348\n",
            "Epoch 3 (train)2.9s: train loss = 0.08233 = (mf)0.07703 + (embed)0.00530\n",
            "Epoch 4 (train)3.0s: train loss = 0.06681 = (mf)0.05993 + (embed)0.00688\n",
            "Epoch 5 (train)3.7s + (eval)0.8s: train loss = 0.05570 = (mf)0.04746 + (embed)0.00824, recall = 0.11066, ndcg = 0.27137\n",
            "Epoch 6 (train)3.0s: train loss = 0.05475 = (mf)0.04538 + (embed)0.00937\n",
            "Epoch 7 (train)3.0s: train loss = 0.05018 = (mf)0.03976 + (embed)0.01042\n",
            "Epoch 8 (train)3.6s: train loss = 0.04824 = (mf)0.03689 + (embed)0.01135\n",
            "Epoch 9 (train)3.3s: train loss = 0.04702 = (mf)0.03486 + (embed)0.01216\n",
            "Epoch 10 (train)2.9s + (eval)0.2s: train loss = 0.04544 = (mf)0.03257 + (embed)0.01287, recall = 0.10147, ndcg = 0.24247\n",
            "Epoch 11 (train)3.0s: train loss = 0.04587 = (mf)0.03236 + (embed)0.01351\n",
            "Epoch 12 (train)3.7s: train loss = 0.04369 = (mf)0.02952 + (embed)0.01417\n",
            "Epoch 13 (train)3.1s: train loss = 0.04712 = (mf)0.03242 + (embed)0.01470\n",
            "Epoch 14 (train)3.0s: train loss = 0.04430 = (mf)0.02905 + (embed)0.01525\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 0.04439 = (mf)0.02863 + (embed)0.01576, recall = 0.10009, ndcg = 0.24250\n",
            "Epoch 16 (train)3.7s: train loss = 0.04562 = (mf)0.02932 + (embed)0.01630\n",
            "Epoch 17 (train)2.9s: train loss = 0.04496 = (mf)0.02810 + (embed)0.01686\n",
            "Epoch 18 (train)2.9s: train loss = 0.04883 = (mf)0.03148 + (embed)0.01735\n",
            "Epoch 19 (train)2.9s: train loss = 0.04689 = (mf)0.02916 + (embed)0.01772\n",
            "Epoch 20 (train)3.7s + (eval)0.6s: train loss = 0.04565 = (mf)0.02755 + (embed)0.01811, recall = 0.09845, ndcg = 0.23016\n",
            "Epoch 21 (train)2.9s: train loss = 0.04776 = (mf)0.02922 + (embed)0.01854\n",
            "Epoch 22 (train)2.9s: train loss = 0.04655 = (mf)0.02756 + (embed)0.01898\n",
            "Epoch 23 (train)2.9s: train loss = 0.04753 = (mf)0.02825 + (embed)0.01929\n",
            "Epoch 24 (train)3.7s: train loss = 0.04641 = (mf)0.02677 + (embed)0.01963\n",
            "Epoch 25 (train)2.9s + (eval)0.2s: train loss = 0.04750 = (mf)0.02759 + (embed)0.01991, recall = 0.09707, ndcg = 0.23451\n",
            "Epoch 26 (train)2.9s: train loss = 0.04839 = (mf)0.02818 + (embed)0.02021\n",
            "Epoch 27 (train)2.8s: train loss = 0.04835 = (mf)0.02784 + (embed)0.02051\n",
            "Epoch 28 (train)3.7s: train loss = 0.04757 = (mf)0.02672 + (embed)0.02085\n",
            "Epoch 29 (train)2.9s: train loss = 0.04984 = (mf)0.02871 + (embed)0.02114\n",
            "Epoch 30 (train)2.9s + (eval)0.2s: train loss = 0.04575 = (mf)0.02426 + (embed)0.02150, recall = 0.09401, ndcg = 0.21324\n",
            "Epoch 31 (train)2.9s: train loss = 0.04685 = (mf)0.02505 + (embed)0.02181\n",
            "Epoch 32 (train)3.8s: train loss = 0.04796 = (mf)0.02586 + (embed)0.02210\n",
            "Epoch 33 (train)2.9s: train loss = 0.05043 = (mf)0.02806 + (embed)0.02237\n",
            "Epoch 34 (train)2.9s: train loss = 0.04736 = (mf)0.02461 + (embed)0.02275\n",
            "Epoch 35 (train)2.9s + (eval)0.5s: train loss = 0.05146 = (mf)0.02856 + (embed)0.02290, recall = 0.09303, ndcg = 0.21897\n",
            "Epoch 36 (train)3.8s: train loss = 0.05095 = (mf)0.02773 + (embed)0.02322\n",
            "Epoch 37 (train)2.9s: train loss = 0.05147 = (mf)0.02797 + (embed)0.02350\n",
            "Epoch 38 (train)2.9s: train loss = 0.04913 = (mf)0.02545 + (embed)0.02367\n",
            "Epoch 39 (train)2.9s: train loss = 0.05060 = (mf)0.02670 + (embed)0.02389\n",
            "Epoch 40 (train)3.8s + (eval)0.2s: train loss = 0.04826 = (mf)0.02413 + (embed)0.02412, recall = 0.09716, ndcg = 0.21894\n",
            "Epoch 41 (train)2.9s: train loss = 0.05305 = (mf)0.02876 + (embed)0.02430\n",
            "Epoch 42 (train)2.9s: train loss = 0.05085 = (mf)0.02640 + (embed)0.02444\n",
            "Epoch 43 (train)2.9s: train loss = 0.05200 = (mf)0.02735 + (embed)0.02465\n",
            "Epoch 44 (train)3.8s: train loss = 0.05025 = (mf)0.02530 + (embed)0.02495\n",
            "Epoch 45 (train)2.9s + (eval)0.5s: train loss = 0.05251 = (mf)0.02742 + (embed)0.02509, recall = 0.10067, ndcg = 0.23233\n",
            "Epoch 46 (train)2.9s: train loss = 0.05297 = (mf)0.02769 + (embed)0.02527\n",
            "Epoch 47 (train)2.9s: train loss = 0.05304 = (mf)0.02768 + (embed)0.02536\n",
            "Epoch 48 (train)3.7s: train loss = 0.05183 = (mf)0.02627 + (embed)0.02556\n",
            "Epoch 49 (train)2.9s: train loss = 0.05198 = (mf)0.02628 + (embed)0.02570\n",
            "Epoch 50 (train)2.9s + (eval)0.2s: train loss = 0.05123 = (mf)0.02536 + (embed)0.02587, recall = 0.09439, ndcg = 0.21913\n",
            "Training finished in 160.90 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.21913, Recall@10: 0.09439\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.9s: train loss = 0.23274 = (mf)0.23117 + (embed)0.00157\n",
            "Epoch 2 (train)3.4s: train loss = 0.12128 = (mf)0.11761 + (embed)0.00367\n",
            "Epoch 3 (train)3.8s: train loss = 0.09101 = (mf)0.08541 + (embed)0.00560\n",
            "Epoch 4 (train)4.0s: train loss = 0.07199 = (mf)0.06465 + (embed)0.00734\n",
            "Epoch 5 (train)3.5s + (eval)0.5s: train loss = 0.06322 = (mf)0.05441 + (embed)0.00881, recall = 0.11407, ndcg = 0.27425\n",
            "Epoch 6 (train)3.4s: train loss = 0.05738 = (mf)0.04726 + (embed)0.01012\n",
            "Epoch 7 (train)4.3s: train loss = 0.05479 = (mf)0.04356 + (embed)0.01123\n",
            "Epoch 8 (train)3.5s: train loss = 0.04960 = (mf)0.03747 + (embed)0.01214\n",
            "Epoch 9 (train)3.4s: train loss = 0.04811 = (mf)0.03516 + (embed)0.01296\n",
            "Epoch 10 (train)4.3s + (eval)0.5s: train loss = 0.04811 = (mf)0.03448 + (embed)0.01362, recall = 0.12147, ndcg = 0.29262\n",
            "Epoch 11 (train)3.5s: train loss = 0.04696 = (mf)0.03263 + (embed)0.01432\n",
            "Epoch 12 (train)3.4s: train loss = 0.04528 = (mf)0.03033 + (embed)0.01495\n",
            "Epoch 13 (train)4.0s: train loss = 0.04345 = (mf)0.02795 + (embed)0.01550\n",
            "Epoch 14 (train)3.7s: train loss = 0.04404 = (mf)0.02806 + (embed)0.01598\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.04139 = (mf)0.02503 + (embed)0.01636, recall = 0.11283, ndcg = 0.26535\n",
            "Epoch 16 (train)3.5s: train loss = 0.04403 = (mf)0.02724 + (embed)0.01679\n",
            "Epoch 17 (train)4.3s: train loss = 0.04217 = (mf)0.02505 + (embed)0.01712\n",
            "Epoch 18 (train)3.4s: train loss = 0.04290 = (mf)0.02545 + (embed)0.01746\n",
            "Epoch 19 (train)3.4s: train loss = 0.04074 = (mf)0.02295 + (embed)0.01779\n",
            "Epoch 20 (train)4.3s + (eval)0.2s: train loss = 0.04146 = (mf)0.02337 + (embed)0.01810, recall = 0.11610, ndcg = 0.26068\n",
            "Epoch 21 (train)3.5s: train loss = 0.04030 = (mf)0.02189 + (embed)0.01841\n",
            "Epoch 22 (train)3.5s: train loss = 0.04089 = (mf)0.02220 + (embed)0.01869\n",
            "Epoch 23 (train)3.9s: train loss = 0.04095 = (mf)0.02205 + (embed)0.01890\n",
            "Epoch 24 (train)3.9s: train loss = 0.04024 = (mf)0.02109 + (embed)0.01915\n",
            "Epoch 25 (train)3.5s + (eval)0.5s: train loss = 0.04095 = (mf)0.02156 + (embed)0.01938, recall = 0.11245, ndcg = 0.25521\n",
            "Epoch 26 (train)3.5s: train loss = 0.03988 = (mf)0.02030 + (embed)0.01958\n",
            "Epoch 27 (train)4.3s: train loss = 0.04065 = (mf)0.02092 + (embed)0.01974\n",
            "Epoch 28 (train)3.4s: train loss = 0.03994 = (mf)0.02003 + (embed)0.01991\n",
            "Epoch 29 (train)3.4s: train loss = 0.04120 = (mf)0.02113 + (embed)0.02008\n",
            "Epoch 30 (train)4.3s + (eval)0.2s: train loss = 0.03995 = (mf)0.01966 + (embed)0.02030, recall = 0.11287, ndcg = 0.25913\n",
            "Epoch 31 (train)3.5s: train loss = 0.04126 = (mf)0.02069 + (embed)0.02056\n",
            "Epoch 32 (train)3.5s: train loss = 0.04134 = (mf)0.02052 + (embed)0.02082\n",
            "Epoch 33 (train)4.0s: train loss = 0.03971 = (mf)0.01871 + (embed)0.02100\n",
            "Epoch 34 (train)3.8s: train loss = 0.04140 = (mf)0.02026 + (embed)0.02114\n",
            "Epoch 35 (train)3.5s + (eval)0.5s: train loss = 0.04116 = (mf)0.01983 + (embed)0.02133, recall = 0.10788, ndcg = 0.25366\n",
            "Epoch 36 (train)3.5s: train loss = 0.03990 = (mf)0.01841 + (embed)0.02149\n",
            "Epoch 37 (train)4.2s: train loss = 0.03888 = (mf)0.01726 + (embed)0.02161\n",
            "Epoch 38 (train)3.5s: train loss = 0.04137 = (mf)0.01977 + (embed)0.02160\n",
            "Epoch 39 (train)3.5s: train loss = 0.04081 = (mf)0.01903 + (embed)0.02178\n",
            "Epoch 40 (train)4.3s + (eval)0.2s: train loss = 0.04128 = (mf)0.01941 + (embed)0.02187, recall = 0.10815, ndcg = 0.25110\n",
            "Epoch 41 (train)3.5s: train loss = 0.04010 = (mf)0.01807 + (embed)0.02203\n",
            "Epoch 42 (train)3.4s: train loss = 0.04173 = (mf)0.01953 + (embed)0.02219\n",
            "Epoch 43 (train)4.0s: train loss = 0.03936 = (mf)0.01716 + (embed)0.02220\n",
            "Epoch 44 (train)3.8s: train loss = 0.04155 = (mf)0.01922 + (embed)0.02233\n",
            "Epoch 45 (train)3.5s + (eval)0.2s: train loss = 0.04030 = (mf)0.01790 + (embed)0.02240, recall = 0.11098, ndcg = 0.25146\n",
            "Epoch 46 (train)3.5s: train loss = 0.04052 = (mf)0.01800 + (embed)0.02252\n",
            "Epoch 47 (train)4.4s: train loss = 0.04004 = (mf)0.01731 + (embed)0.02273\n",
            "Epoch 48 (train)3.5s: train loss = 0.04068 = (mf)0.01789 + (embed)0.02279\n",
            "Epoch 49 (train)3.5s: train loss = 0.04144 = (mf)0.01856 + (embed)0.02287\n",
            "Epoch 50 (train)4.3s + (eval)0.5s: train loss = 0.04103 = (mf)0.01802 + (embed)0.02301, recall = 0.11299, ndcg = 0.25399\n",
            "Training finished in 189.79 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.25399, Recall@10: 0.11299\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.8s: train loss = 0.24195 = (mf)0.24035 + (embed)0.00159\n",
            "Epoch 2 (train)4.9s: train loss = 0.14019 = (mf)0.13623 + (embed)0.00396\n",
            "Epoch 3 (train)4.1s: train loss = 0.11062 = (mf)0.10475 + (embed)0.00587\n",
            "Epoch 4 (train)4.5s: train loss = 0.09018 = (mf)0.08241 + (embed)0.00776\n",
            "Epoch 5 (train)4.6s + (eval)0.5s: train loss = 0.07765 = (mf)0.06809 + (embed)0.00956, recall = 0.11808, ndcg = 0.28465\n",
            "Epoch 6 (train)4.1s: train loss = 0.06780 = (mf)0.05667 + (embed)0.01112\n",
            "Epoch 7 (train)4.9s: train loss = 0.06255 = (mf)0.05017 + (embed)0.01238\n",
            "Epoch 8 (train)4.2s: train loss = 0.05784 = (mf)0.04435 + (embed)0.01349\n",
            "Epoch 9 (train)4.1s: train loss = 0.05634 = (mf)0.04189 + (embed)0.01445\n",
            "Epoch 10 (train)5.0s + (eval)0.2s: train loss = 0.05170 = (mf)0.03642 + (embed)0.01529, recall = 0.12236, ndcg = 0.29395\n",
            "Epoch 11 (train)4.1s: train loss = 0.05059 = (mf)0.03463 + (embed)0.01596\n",
            "Epoch 12 (train)4.0s: train loss = 0.04883 = (mf)0.03222 + (embed)0.01661\n",
            "Epoch 13 (train)4.8s: train loss = 0.04827 = (mf)0.03113 + (embed)0.01713\n",
            "Epoch 14 (train)4.0s: train loss = 0.04626 = (mf)0.02871 + (embed)0.01755\n",
            "Epoch 15 (train)4.1s + (eval)0.6s: train loss = 0.04570 = (mf)0.02788 + (embed)0.01782, recall = 0.12500, ndcg = 0.30458\n",
            "Epoch 16 (train)4.7s: train loss = 0.04622 = (mf)0.02802 + (embed)0.01821\n",
            "Epoch 17 (train)4.0s: train loss = 0.04519 = (mf)0.02671 + (embed)0.01848\n",
            "Epoch 18 (train)4.4s: train loss = 0.04508 = (mf)0.02625 + (embed)0.01883\n",
            "Epoch 19 (train)4.5s: train loss = 0.04488 = (mf)0.02583 + (embed)0.01905\n",
            "Epoch 20 (train)4.1s + (eval)0.2s: train loss = 0.04370 = (mf)0.02428 + (embed)0.01942, recall = 0.12100, ndcg = 0.28885\n",
            "Epoch 21 (train)4.7s: train loss = 0.04482 = (mf)0.02512 + (embed)0.01970\n",
            "Epoch 22 (train)4.2s: train loss = 0.04387 = (mf)0.02395 + (embed)0.01993\n",
            "Epoch 23 (train)4.0s: train loss = 0.04321 = (mf)0.02305 + (embed)0.02016\n",
            "Epoch 24 (train)4.8s: train loss = 0.04254 = (mf)0.02210 + (embed)0.02043\n",
            "Epoch 25 (train)4.1s + (eval)0.5s: train loss = 0.04180 = (mf)0.02119 + (embed)0.02061, recall = 0.12271, ndcg = 0.28675\n",
            "Epoch 26 (train)4.0s: train loss = 0.04234 = (mf)0.02156 + (embed)0.02078\n",
            "Epoch 27 (train)4.9s: train loss = 0.04293 = (mf)0.02206 + (embed)0.02087\n",
            "Epoch 28 (train)4.0s: train loss = 0.04164 = (mf)0.02056 + (embed)0.02107\n",
            "Epoch 29 (train)4.1s: train loss = 0.04092 = (mf)0.01965 + (embed)0.02127\n",
            "Epoch 30 (train)4.9s + (eval)0.2s: train loss = 0.04186 = (mf)0.02043 + (embed)0.02143, recall = 0.12194, ndcg = 0.27965\n",
            "Epoch 31 (train)4.1s: train loss = 0.04159 = (mf)0.02013 + (embed)0.02146\n",
            "Epoch 32 (train)4.0s: train loss = 0.04096 = (mf)0.01934 + (embed)0.02162\n",
            "Epoch 33 (train)4.9s: train loss = 0.04126 = (mf)0.01950 + (embed)0.02176\n",
            "Epoch 34 (train)4.1s: train loss = 0.04118 = (mf)0.01927 + (embed)0.02191\n",
            "Epoch 35 (train)4.1s + (eval)0.3s: train loss = 0.04038 = (mf)0.01838 + (embed)0.02199, recall = 0.12128, ndcg = 0.26502\n",
            "Epoch 36 (train)4.6s: train loss = 0.04206 = (mf)0.01991 + (embed)0.02215\n",
            "Epoch 37 (train)4.0s: train loss = 0.04171 = (mf)0.01947 + (embed)0.02225\n",
            "Epoch 38 (train)4.3s: train loss = 0.04276 = (mf)0.02038 + (embed)0.02239\n",
            "Epoch 39 (train)4.6s: train loss = 0.04145 = (mf)0.01885 + (embed)0.02260\n",
            "Epoch 40 (train)4.0s + (eval)0.5s: train loss = 0.04163 = (mf)0.01901 + (embed)0.02262, recall = 0.12405, ndcg = 0.27710\n",
            "Epoch 41 (train)4.6s: train loss = 0.04080 = (mf)0.01809 + (embed)0.02271\n",
            "Epoch 42 (train)4.3s: train loss = 0.04050 = (mf)0.01774 + (embed)0.02276\n",
            "Epoch 43 (train)4.0s: train loss = 0.04013 = (mf)0.01723 + (embed)0.02289\n",
            "Epoch 44 (train)4.9s: train loss = 0.04105 = (mf)0.01809 + (embed)0.02296\n",
            "Epoch 45 (train)4.1s + (eval)0.2s: train loss = 0.04181 = (mf)0.01874 + (embed)0.02307, recall = 0.12005, ndcg = 0.26549\n",
            "Epoch 46 (train)4.1s: train loss = 0.04116 = (mf)0.01803 + (embed)0.02313\n",
            "Epoch 47 (train)4.9s: train loss = 0.04075 = (mf)0.01754 + (embed)0.02321\n",
            "Epoch 48 (train)4.1s: train loss = 0.04059 = (mf)0.01731 + (embed)0.02328\n",
            "Epoch 49 (train)4.1s: train loss = 0.04073 = (mf)0.01735 + (embed)0.02338\n",
            "Epoch 50 (train)4.9s + (eval)0.5s: train loss = 0.04021 = (mf)0.01683 + (embed)0.02338, recall = 0.11745, ndcg = 0.26352\n",
            "Training finished in 222.36 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.26352, Recall@10: 0.11745\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.4s: train loss = 0.60269 = (mf)0.53639 + (embed)0.06630\n",
            "Epoch 2 (train)3.7s: train loss = 1.24121 = (mf)1.03444 + (embed)0.20677\n",
            "Epoch 3 (train)2.9s: train loss = 1.74712 = (mf)1.40046 + (embed)0.34666\n",
            "Epoch 4 (train)2.9s: train loss = 2.20939 = (mf)1.73901 + (embed)0.47038\n",
            "Epoch 5 (train)2.9s + (eval)0.8s: train loss = 2.42218 = (mf)1.83690 + (embed)0.58527, recall = 0.05746, ndcg = 0.16033\n",
            "Epoch 6 (train)3.8s: train loss = 2.63121 = (mf)1.94286 + (embed)0.68835\n",
            "Epoch 7 (train)2.9s: train loss = 2.92626 = (mf)2.14405 + (embed)0.78221\n",
            "Epoch 8 (train)2.9s: train loss = 3.10794 = (mf)2.23353 + (embed)0.87441\n",
            "Epoch 9 (train)2.9s: train loss = 3.41926 = (mf)2.46563 + (embed)0.95364\n",
            "Epoch 10 (train)3.8s + (eval)0.2s: train loss = 3.59677 = (mf)2.56948 + (embed)1.02728, recall = 0.06050, ndcg = 0.16865\n",
            "Epoch 11 (train)2.9s: train loss = 3.52815 = (mf)2.42718 + (embed)1.10098\n",
            "Epoch 12 (train)2.9s: train loss = 3.68338 = (mf)2.51138 + (embed)1.17200\n",
            "Epoch 13 (train)2.9s: train loss = 3.88292 = (mf)2.64904 + (embed)1.23388\n",
            "Epoch 14 (train)3.8s: train loss = 4.00195 = (mf)2.70730 + (embed)1.29465\n",
            "Epoch 15 (train)2.9s + (eval)0.5s: train loss = 4.07581 = (mf)2.72401 + (embed)1.35180, recall = 0.05943, ndcg = 0.16527\n",
            "Epoch 16 (train)2.9s: train loss = 4.44561 = (mf)3.04046 + (embed)1.40515\n",
            "Epoch 17 (train)2.9s: train loss = 4.22170 = (mf)2.76480 + (embed)1.45690\n",
            "Epoch 18 (train)3.7s: train loss = 4.54934 = (mf)3.04585 + (embed)1.50348\n",
            "Epoch 19 (train)2.9s: train loss = 4.49953 = (mf)2.94626 + (embed)1.55328\n",
            "Epoch 20 (train)2.9s + (eval)0.2s: train loss = 4.45307 = (mf)2.85479 + (embed)1.59828, recall = 0.07048, ndcg = 0.18543\n",
            "Epoch 21 (train)3.0s: train loss = 4.66488 = (mf)3.02000 + (embed)1.64487\n",
            "Epoch 22 (train)3.7s: train loss = 4.59688 = (mf)2.90721 + (embed)1.68967\n",
            "Epoch 23 (train)2.9s: train loss = 4.46964 = (mf)2.74091 + (embed)1.72874\n",
            "Epoch 24 (train)2.9s: train loss = 4.71694 = (mf)2.94840 + (embed)1.76855\n",
            "Epoch 25 (train)3.0s + (eval)0.3s: train loss = 4.85121 = (mf)3.04764 + (embed)1.80357, recall = 0.06377, ndcg = 0.17095\n",
            "Epoch 26 (train)3.6s: train loss = 4.80527 = (mf)2.97360 + (embed)1.83167\n",
            "Epoch 27 (train)2.9s: train loss = 4.69911 = (mf)2.83984 + (embed)1.85927\n",
            "Epoch 28 (train)2.9s: train loss = 4.84780 = (mf)2.95091 + (embed)1.89689\n",
            "Epoch 29 (train)2.9s: train loss = 4.81691 = (mf)2.89908 + (embed)1.91782\n",
            "Epoch 30 (train)3.7s + (eval)0.5s: train loss = 4.84735 = (mf)2.90124 + (embed)1.94611, recall = 0.06128, ndcg = 0.17100\n",
            "Epoch 31 (train)2.8s: train loss = 4.78311 = (mf)2.80830 + (embed)1.97480\n",
            "Epoch 32 (train)2.9s: train loss = 5.04210 = (mf)3.04320 + (embed)1.99890\n",
            "Epoch 33 (train)3.1s: train loss = 5.07323 = (mf)3.04662 + (embed)2.02661\n",
            "Epoch 34 (train)3.6s: train loss = 4.71699 = (mf)2.66124 + (embed)2.05575\n",
            "Epoch 35 (train)2.9s + (eval)0.2s: train loss = 4.96396 = (mf)2.88700 + (embed)2.07696, recall = 0.07023, ndcg = 0.17897\n",
            "Epoch 36 (train)2.9s: train loss = 4.80345 = (mf)2.70835 + (embed)2.09510\n",
            "Epoch 37 (train)3.3s: train loss = 4.93815 = (mf)2.81579 + (embed)2.12236\n",
            "Epoch 38 (train)3.5s: train loss = 5.46680 = (mf)3.32064 + (embed)2.14616\n",
            "Epoch 39 (train)3.0s: train loss = 5.25587 = (mf)3.09166 + (embed)2.16420\n",
            "Epoch 40 (train)3.0s + (eval)0.2s: train loss = 5.48365 = (mf)3.29295 + (embed)2.19070, recall = 0.06849, ndcg = 0.17810\n",
            "Epoch 41 (train)3.5s: train loss = 5.08108 = (mf)2.87215 + (embed)2.20893\n",
            "Epoch 42 (train)3.3s: train loss = 5.26569 = (mf)3.03712 + (embed)2.22857\n",
            "Epoch 43 (train)2.9s: train loss = 5.06137 = (mf)2.80642 + (embed)2.25496\n",
            "Epoch 44 (train)2.9s: train loss = 5.05132 = (mf)2.78526 + (embed)2.26606\n",
            "Epoch 45 (train)3.4s + (eval)0.7s: train loss = 5.12191 = (mf)2.83053 + (embed)2.29139, recall = 0.06282, ndcg = 0.15806\n",
            "Epoch 46 (train)3.0s: train loss = 4.85581 = (mf)2.54699 + (embed)2.30882\n",
            "Epoch 47 (train)3.0s: train loss = 5.49740 = (mf)3.17941 + (embed)2.31799\n",
            "Epoch 48 (train)2.9s: train loss = 5.47655 = (mf)3.13884 + (embed)2.33771\n",
            "Epoch 49 (train)3.7s: train loss = 5.23680 = (mf)2.87665 + (embed)2.36016\n",
            "Epoch 50 (train)3.0s + (eval)0.2s: train loss = 5.19635 = (mf)2.82105 + (embed)2.37530, recall = 0.06714, ndcg = 0.16721\n",
            "Training finished in 160.55 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.16721, Recall@10: 0.06714\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.8s: train loss = 0.39820 = (mf)0.34624 + (embed)0.05196\n",
            "Epoch 2 (train)3.6s: train loss = 0.76657 = (mf)0.59697 + (embed)0.16960\n",
            "Epoch 3 (train)3.5s: train loss = 1.18968 = (mf)0.88918 + (embed)0.30050\n",
            "Epoch 4 (train)3.7s: train loss = 1.35558 = (mf)0.94393 + (embed)0.41165\n",
            "Epoch 5 (train)4.2s + (eval)0.6s: train loss = 1.66371 = (mf)1.14719 + (embed)0.51653, recall = 0.06168, ndcg = 0.17983\n",
            "Epoch 6 (train)3.5s: train loss = 1.78164 = (mf)1.17187 + (embed)0.60977\n",
            "Epoch 7 (train)3.5s: train loss = 1.93989 = (mf)1.25329 + (embed)0.68660\n",
            "Epoch 8 (train)4.3s: train loss = 2.07179 = (mf)1.30563 + (embed)0.76616\n",
            "Epoch 9 (train)3.5s: train loss = 2.19011 = (mf)1.35359 + (embed)0.83652\n",
            "Epoch 10 (train)3.4s + (eval)0.5s: train loss = 2.40521 = (mf)1.50100 + (embed)0.90421, recall = 0.06981, ndcg = 0.18695\n",
            "Epoch 11 (train)4.3s: train loss = 2.59393 = (mf)1.62557 + (embed)0.96836\n",
            "Epoch 12 (train)3.4s: train loss = 2.64641 = (mf)1.62820 + (embed)1.01821\n",
            "Epoch 13 (train)3.4s: train loss = 2.50924 = (mf)1.44587 + (embed)1.06337\n",
            "Epoch 14 (train)4.0s: train loss = 2.62861 = (mf)1.52172 + (embed)1.10690\n",
            "Epoch 15 (train)3.8s + (eval)0.2s: train loss = 2.74612 = (mf)1.60145 + (embed)1.14468, recall = 0.05866, ndcg = 0.16368\n",
            "Epoch 16 (train)3.5s: train loss = 2.95077 = (mf)1.76269 + (embed)1.18807\n",
            "Epoch 17 (train)3.6s: train loss = 2.98317 = (mf)1.75690 + (embed)1.22628\n",
            "Epoch 18 (train)4.3s: train loss = 2.96025 = (mf)1.69711 + (embed)1.26314\n",
            "Epoch 19 (train)3.5s: train loss = 3.01171 = (mf)1.71817 + (embed)1.29353\n",
            "Epoch 20 (train)3.5s + (eval)0.2s: train loss = 2.88935 = (mf)1.56402 + (embed)1.32533, recall = 0.06426, ndcg = 0.17839\n",
            "Epoch 21 (train)4.3s: train loss = 3.09563 = (mf)1.74201 + (embed)1.35362\n",
            "Epoch 22 (train)3.5s: train loss = 3.07080 = (mf)1.68932 + (embed)1.38147\n",
            "Epoch 23 (train)3.4s: train loss = 3.19667 = (mf)1.78952 + (embed)1.40715\n",
            "Epoch 24 (train)4.0s: train loss = 3.18136 = (mf)1.74372 + (embed)1.43764\n",
            "Epoch 25 (train)3.7s + (eval)0.5s: train loss = 2.96945 = (mf)1.50832 + (embed)1.46113, recall = 0.06480, ndcg = 0.17425\n",
            "Epoch 26 (train)3.4s: train loss = 3.22124 = (mf)1.73789 + (embed)1.48336\n",
            "Epoch 27 (train)3.6s: train loss = 3.11357 = (mf)1.60778 + (embed)1.50579\n",
            "Epoch 28 (train)4.1s: train loss = 3.18257 = (mf)1.65877 + (embed)1.52380\n",
            "Epoch 29 (train)3.5s: train loss = 3.08583 = (mf)1.54728 + (embed)1.53855\n",
            "Epoch 30 (train)3.4s + (eval)0.2s: train loss = 3.15394 = (mf)1.59941 + (embed)1.55453, recall = 0.05653, ndcg = 0.15575\n",
            "Epoch 31 (train)4.3s: train loss = 3.23275 = (mf)1.66542 + (embed)1.56733\n",
            "Epoch 32 (train)3.5s: train loss = 3.20120 = (mf)1.61741 + (embed)1.58378\n",
            "Epoch 33 (train)3.5s: train loss = 3.16640 = (mf)1.57692 + (embed)1.58948\n",
            "Epoch 34 (train)3.9s: train loss = 3.15006 = (mf)1.54585 + (embed)1.60421\n",
            "Epoch 35 (train)3.8s + (eval)0.5s: train loss = 3.29078 = (mf)1.67028 + (embed)1.62050, recall = 0.06423, ndcg = 0.16179\n",
            "Epoch 36 (train)3.5s: train loss = 3.12502 = (mf)1.49289 + (embed)1.63213\n",
            "Epoch 37 (train)3.7s: train loss = 3.29990 = (mf)1.64785 + (embed)1.65205\n",
            "Epoch 38 (train)4.1s: train loss = 3.19073 = (mf)1.53521 + (embed)1.65552\n",
            "Epoch 39 (train)3.5s: train loss = 3.20617 = (mf)1.54052 + (embed)1.66565\n",
            "Epoch 40 (train)3.5s + (eval)0.2s: train loss = 3.22544 = (mf)1.54873 + (embed)1.67671, recall = 0.05710, ndcg = 0.15244\n",
            "Epoch 41 (train)4.3s: train loss = 3.18650 = (mf)1.49949 + (embed)1.68701\n",
            "Epoch 42 (train)3.4s: train loss = 3.21888 = (mf)1.52636 + (embed)1.69252\n",
            "Epoch 43 (train)3.4s: train loss = 3.29417 = (mf)1.59726 + (embed)1.69691\n",
            "Epoch 44 (train)4.1s: train loss = 3.31931 = (mf)1.60431 + (embed)1.71500\n",
            "Epoch 45 (train)3.7s + (eval)0.2s: train loss = 3.30294 = (mf)1.57377 + (embed)1.72917, recall = 0.05999, ndcg = 0.15290\n",
            "Epoch 46 (train)3.6s: train loss = 3.22750 = (mf)1.48836 + (embed)1.73914\n",
            "Epoch 47 (train)3.9s: train loss = 3.33657 = (mf)1.59410 + (embed)1.74247\n",
            "Epoch 48 (train)4.1s: train loss = 3.33218 = (mf)1.58521 + (embed)1.74697\n",
            "Epoch 49 (train)3.6s: train loss = 3.34591 = (mf)1.58498 + (embed)1.76093\n",
            "Epoch 50 (train)3.5s + (eval)0.5s: train loss = 3.19574 = (mf)1.42266 + (embed)1.77308, recall = 0.06911, ndcg = 0.17884\n",
            "Training finished in 190.97 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.17884, Recall@10: 0.06911\n",
            "--- Trying combination: Epochs=50, TopK=10, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.9s: train loss = 0.34882 = (mf)0.30237 + (embed)0.04646\n",
            "Epoch 2 (train)4.9s: train loss = 0.58571 = (mf)0.43447 + (embed)0.15124\n",
            "Epoch 3 (train)4.1s: train loss = 0.83600 = (mf)0.56886 + (embed)0.26715\n",
            "Epoch 4 (train)4.1s: train loss = 1.00563 = (mf)0.64349 + (embed)0.36214\n",
            "Epoch 5 (train)4.9s + (eval)0.9s: train loss = 1.18446 = (mf)0.73964 + (embed)0.44482, recall = 0.06580, ndcg = 0.18224\n",
            "Epoch 6 (train)4.1s: train loss = 1.30930 = (mf)0.78882 + (embed)0.52048\n",
            "Epoch 7 (train)4.0s: train loss = 1.47733 = (mf)0.87859 + (embed)0.59874\n",
            "Epoch 8 (train)4.9s: train loss = 1.62772 = (mf)0.96305 + (embed)0.66468\n",
            "Epoch 9 (train)4.1s: train loss = 1.64448 = (mf)0.92497 + (embed)0.71951\n",
            "Epoch 10 (train)4.2s + (eval)0.3s: train loss = 1.78830 = (mf)1.02247 + (embed)0.76583, recall = 0.05607, ndcg = 0.15248\n",
            "Epoch 11 (train)4.7s: train loss = 1.76920 = (mf)0.95915 + (embed)0.81005\n",
            "Epoch 12 (train)4.0s: train loss = 1.87722 = (mf)1.02142 + (embed)0.85581\n",
            "Epoch 13 (train)4.6s: train loss = 1.86450 = (mf)0.97279 + (embed)0.89170\n",
            "Epoch 14 (train)4.4s: train loss = 1.98714 = (mf)1.06034 + (embed)0.92680\n",
            "Epoch 15 (train)4.0s + (eval)0.5s: train loss = 2.07645 = (mf)1.11506 + (embed)0.96139, recall = 0.06227, ndcg = 0.16670\n",
            "Epoch 16 (train)4.8s: train loss = 2.04046 = (mf)1.04592 + (embed)0.99454\n",
            "Epoch 17 (train)4.1s: train loss = 2.10976 = (mf)1.08133 + (embed)1.02843\n",
            "Epoch 18 (train)4.1s: train loss = 2.15482 = (mf)1.09968 + (embed)1.05515\n",
            "Epoch 19 (train)4.9s: train loss = 2.11180 = (mf)1.03377 + (embed)1.07803\n",
            "Epoch 20 (train)4.1s + (eval)0.2s: train loss = 2.19029 = (mf)1.09527 + (embed)1.09502, recall = 0.05021, ndcg = 0.13468\n",
            "Epoch 21 (train)4.1s: train loss = 2.26906 = (mf)1.15485 + (embed)1.11420\n",
            "Epoch 22 (train)4.8s: train loss = 2.16811 = (mf)1.04101 + (embed)1.12710\n",
            "Epoch 23 (train)4.0s: train loss = 2.32659 = (mf)1.17881 + (embed)1.14778\n",
            "Epoch 24 (train)4.0s: train loss = 2.31463 = (mf)1.14785 + (embed)1.16678\n",
            "Epoch 25 (train)4.9s + (eval)0.2s: train loss = 2.18328 = (mf)0.99741 + (embed)1.18587, recall = 0.07149, ndcg = 0.17838\n",
            "Epoch 26 (train)4.0s: train loss = 2.37481 = (mf)1.17004 + (embed)1.20477\n",
            "Epoch 27 (train)4.0s: train loss = 2.28845 = (mf)1.07321 + (embed)1.21524\n",
            "Epoch 28 (train)4.8s: train loss = 2.42824 = (mf)1.19477 + (embed)1.23347\n",
            "Epoch 29 (train)4.0s: train loss = 2.38200 = (mf)1.13423 + (embed)1.24777\n",
            "Epoch 30 (train)5.2s + (eval)0.7s: train loss = 2.46815 = (mf)1.20432 + (embed)1.26383, recall = 0.07230, ndcg = 0.17555\n",
            "Epoch 31 (train)4.2s: train loss = 2.33079 = (mf)1.05180 + (embed)1.27899\n",
            "Epoch 32 (train)4.1s: train loss = 2.42721 = (mf)1.13717 + (embed)1.29004\n",
            "Epoch 33 (train)5.0s: train loss = 2.30473 = (mf)1.00600 + (embed)1.29873\n",
            "Epoch 34 (train)4.1s: train loss = 2.42435 = (mf)1.12098 + (embed)1.30337\n",
            "Epoch 35 (train)4.0s + (eval)0.2s: train loss = 2.36195 = (mf)1.04814 + (embed)1.31381, recall = 0.06528, ndcg = 0.16006\n",
            "Epoch 36 (train)4.9s: train loss = 2.37409 = (mf)1.05829 + (embed)1.31580\n",
            "Epoch 37 (train)4.1s: train loss = 2.42450 = (mf)1.10422 + (embed)1.32028\n",
            "Epoch 38 (train)4.1s: train loss = 2.42787 = (mf)1.09551 + (embed)1.33237\n",
            "Epoch 39 (train)4.9s: train loss = 2.39344 = (mf)1.04920 + (embed)1.34424\n",
            "Epoch 40 (train)4.1s + (eval)0.2s: train loss = 2.41724 = (mf)1.06871 + (embed)1.34853, recall = 0.06671, ndcg = 0.16237\n",
            "Epoch 41 (train)4.1s: train loss = 2.33093 = (mf)0.96768 + (embed)1.36325\n",
            "Epoch 42 (train)4.9s: train loss = 2.38407 = (mf)1.01758 + (embed)1.36649\n",
            "Epoch 43 (train)4.1s: train loss = 2.35298 = (mf)0.98338 + (embed)1.36960\n",
            "Epoch 44 (train)4.3s: train loss = 2.44360 = (mf)1.06831 + (embed)1.37529\n",
            "Epoch 45 (train)4.8s + (eval)0.5s: train loss = 2.36598 = (mf)0.99041 + (embed)1.37557, recall = 0.05492, ndcg = 0.13651\n",
            "Epoch 46 (train)4.1s: train loss = 2.36357 = (mf)0.97752 + (embed)1.38605\n",
            "Epoch 47 (train)4.8s: train loss = 2.34607 = (mf)0.95216 + (embed)1.39390\n",
            "Epoch 48 (train)4.2s: train loss = 2.45047 = (mf)1.05798 + (embed)1.39249\n",
            "Epoch 49 (train)4.1s: train loss = 2.38743 = (mf)0.99182 + (embed)1.39561\n",
            "Epoch 50 (train)4.8s + (eval)0.2s: train loss = 2.34508 = (mf)0.93693 + (embed)1.40815, recall = 0.05933, ndcg = 0.15371\n",
            "Training finished in 223.54 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.15371, Recall@10: 0.05933\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.6s: train loss = 0.41971 = (mf)0.41947 + (embed)0.00024\n",
            "Epoch 2 (train)3.8s: train loss = 0.21550 = (mf)0.21485 + (embed)0.00065\n",
            "Epoch 3 (train)3.0s: train loss = 0.19878 = (mf)0.19802 + (embed)0.00077\n",
            "Epoch 4 (train)3.0s: train loss = 0.18196 = (mf)0.18108 + (embed)0.00087\n",
            "Epoch 5 (train)3.0s + (eval)0.6s: train loss = 0.16798 = (mf)0.16697 + (embed)0.00101, recall = 0.14798, ndcg = 0.23727\n",
            "Epoch 6 (train)3.8s: train loss = 0.15463 = (mf)0.15349 + (embed)0.00114\n",
            "Epoch 7 (train)3.0s: train loss = 0.14424 = (mf)0.14295 + (embed)0.00129\n",
            "Epoch 8 (train)3.0s: train loss = 0.13967 = (mf)0.13825 + (embed)0.00142\n",
            "Epoch 9 (train)3.0s: train loss = 0.13036 = (mf)0.12878 + (embed)0.00158\n",
            "Epoch 10 (train)3.8s + (eval)0.2s: train loss = 0.12792 = (mf)0.12621 + (embed)0.00172, recall = 0.16694, ndcg = 0.25656\n",
            "Epoch 11 (train)2.9s: train loss = 0.11749 = (mf)0.11562 + (embed)0.00187\n",
            "Epoch 12 (train)2.9s: train loss = 0.11147 = (mf)0.10944 + (embed)0.00203\n",
            "Epoch 13 (train)2.9s: train loss = 0.10575 = (mf)0.10356 + (embed)0.00219\n",
            "Epoch 14 (train)3.8s: train loss = 0.10134 = (mf)0.09898 + (embed)0.00236\n",
            "Epoch 15 (train)2.9s + (eval)0.5s: train loss = 0.09749 = (mf)0.09497 + (embed)0.00252, recall = 0.17835, ndcg = 0.27235\n",
            "Epoch 16 (train)2.9s: train loss = 0.09213 = (mf)0.08946 + (embed)0.00267\n",
            "Epoch 17 (train)3.2s: train loss = 0.08764 = (mf)0.08479 + (embed)0.00284\n",
            "Epoch 18 (train)3.5s: train loss = 0.08381 = (mf)0.08081 + (embed)0.00300\n",
            "Epoch 19 (train)2.9s: train loss = 0.07952 = (mf)0.07635 + (embed)0.00318\n",
            "Epoch 20 (train)3.0s + (eval)0.2s: train loss = 0.07528 = (mf)0.07195 + (embed)0.00333, recall = 0.19226, ndcg = 0.29525\n",
            "Epoch 21 (train)3.4s: train loss = 0.07080 = (mf)0.06729 + (embed)0.00351\n",
            "Epoch 22 (train)3.5s: train loss = 0.07098 = (mf)0.06732 + (embed)0.00366\n",
            "Epoch 23 (train)3.0s: train loss = 0.06757 = (mf)0.06378 + (embed)0.00379\n",
            "Epoch 24 (train)3.0s: train loss = 0.06853 = (mf)0.06459 + (embed)0.00394\n",
            "Epoch 25 (train)3.7s + (eval)0.8s: train loss = 0.06444 = (mf)0.06034 + (embed)0.00410, recall = 0.20374, ndcg = 0.30926\n",
            "Epoch 26 (train)3.0s: train loss = 0.06065 = (mf)0.05640 + (embed)0.00425\n",
            "Epoch 27 (train)3.0s: train loss = 0.05781 = (mf)0.05343 + (embed)0.00438\n",
            "Epoch 28 (train)3.0s: train loss = 0.05690 = (mf)0.05237 + (embed)0.00453\n",
            "Epoch 29 (train)3.8s: train loss = 0.05587 = (mf)0.05121 + (embed)0.00465\n",
            "Epoch 30 (train)2.9s + (eval)0.2s: train loss = 0.05242 = (mf)0.04763 + (embed)0.00480, recall = 0.21150, ndcg = 0.32131\n",
            "Epoch 31 (train)3.0s: train loss = 0.05328 = (mf)0.04836 + (embed)0.00492\n",
            "Epoch 32 (train)3.0s: train loss = 0.05312 = (mf)0.04808 + (embed)0.00504\n",
            "Epoch 33 (train)3.9s: train loss = 0.05043 = (mf)0.04526 + (embed)0.00516\n",
            "Epoch 34 (train)3.0s: train loss = 0.04917 = (mf)0.04390 + (embed)0.00527\n",
            "Epoch 35 (train)3.0s + (eval)0.2s: train loss = 0.04897 = (mf)0.04357 + (embed)0.00540, recall = 0.21614, ndcg = 0.32622\n",
            "Epoch 36 (train)3.0s: train loss = 0.04477 = (mf)0.03925 + (embed)0.00552\n",
            "Epoch 37 (train)3.9s: train loss = 0.04779 = (mf)0.04218 + (embed)0.00562\n",
            "Epoch 38 (train)3.0s: train loss = 0.04478 = (mf)0.03904 + (embed)0.00574\n",
            "Epoch 39 (train)3.0s: train loss = 0.04520 = (mf)0.03938 + (embed)0.00582\n",
            "Epoch 40 (train)3.0s + (eval)0.7s: train loss = 0.04269 = (mf)0.03677 + (embed)0.00592, recall = 0.21731, ndcg = 0.33297\n",
            "Epoch 41 (train)3.8s: train loss = 0.04153 = (mf)0.03550 + (embed)0.00603\n",
            "Epoch 42 (train)3.1s: train loss = 0.04216 = (mf)0.03604 + (embed)0.00613\n",
            "Epoch 43 (train)3.0s: train loss = 0.04117 = (mf)0.03494 + (embed)0.00622\n",
            "Epoch 44 (train)3.5s: train loss = 0.04021 = (mf)0.03391 + (embed)0.00630\n",
            "Epoch 45 (train)3.3s + (eval)0.2s: train loss = 0.03937 = (mf)0.03295 + (embed)0.00641, recall = 0.22132, ndcg = 0.33588\n",
            "Epoch 46 (train)2.9s: train loss = 0.03935 = (mf)0.03287 + (embed)0.00648\n",
            "Epoch 47 (train)3.0s: train loss = 0.03836 = (mf)0.03182 + (embed)0.00655\n",
            "Epoch 48 (train)3.6s: train loss = 0.03711 = (mf)0.03047 + (embed)0.00664\n",
            "Epoch 49 (train)3.1s: train loss = 0.03688 = (mf)0.03015 + (embed)0.00672\n",
            "Epoch 50 (train)2.9s + (eval)0.2s: train loss = 0.03631 = (mf)0.02951 + (embed)0.00680, recall = 0.22460, ndcg = 0.33967\n",
            "Training finished in 163.97 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.33967, Recall@20: 0.22460\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.6s: train loss = 0.41410 = (mf)0.41385 + (embed)0.00025\n",
            "Epoch 2 (train)3.6s: train loss = 0.23227 = (mf)0.23163 + (embed)0.00064\n",
            "Epoch 3 (train)3.7s: train loss = 0.22052 = (mf)0.21979 + (embed)0.00073\n",
            "Epoch 4 (train)4.3s: train loss = 0.20896 = (mf)0.20810 + (embed)0.00086\n",
            "Epoch 5 (train)3.6s + (eval)0.9s: train loss = 0.19993 = (mf)0.19893 + (embed)0.00100, recall = 0.11903, ndcg = 0.20148\n",
            "Epoch 6 (train)3.6s: train loss = 0.18317 = (mf)0.18201 + (embed)0.00116\n",
            "Epoch 7 (train)4.3s: train loss = 0.17046 = (mf)0.16912 + (embed)0.00134\n",
            "Epoch 8 (train)3.4s: train loss = 0.15433 = (mf)0.15277 + (embed)0.00155\n",
            "Epoch 9 (train)3.5s: train loss = 0.14760 = (mf)0.14585 + (embed)0.00175\n",
            "Epoch 10 (train)4.4s + (eval)0.2s: train loss = 0.14118 = (mf)0.13925 + (embed)0.00193, recall = 0.15516, ndcg = 0.24727\n",
            "Epoch 11 (train)3.5s: train loss = 0.13382 = (mf)0.13171 + (embed)0.00211\n",
            "Epoch 12 (train)3.5s: train loss = 0.12534 = (mf)0.12306 + (embed)0.00228\n",
            "Epoch 13 (train)4.1s: train loss = 0.12419 = (mf)0.12174 + (embed)0.00245\n",
            "Epoch 14 (train)3.7s: train loss = 0.12094 = (mf)0.11834 + (embed)0.00260\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.11543 = (mf)0.11266 + (embed)0.00277, recall = 0.16305, ndcg = 0.25683\n",
            "Epoch 16 (train)3.7s: train loss = 0.11247 = (mf)0.10954 + (embed)0.00293\n",
            "Epoch 17 (train)4.2s: train loss = 0.10636 = (mf)0.10326 + (embed)0.00310\n",
            "Epoch 18 (train)3.6s: train loss = 0.10119 = (mf)0.09790 + (embed)0.00329\n",
            "Epoch 19 (train)3.6s: train loss = 0.09845 = (mf)0.09501 + (embed)0.00344\n",
            "Epoch 20 (train)4.3s + (eval)0.6s: train loss = 0.09466 = (mf)0.09104 + (embed)0.00362, recall = 0.17561, ndcg = 0.27199\n",
            "Epoch 21 (train)3.9s: train loss = 0.09057 = (mf)0.08677 + (embed)0.00379\n",
            "Epoch 22 (train)4.0s: train loss = 0.08980 = (mf)0.08584 + (embed)0.00396\n",
            "Epoch 23 (train)4.4s: train loss = 0.08417 = (mf)0.08003 + (embed)0.00414\n",
            "Epoch 24 (train)3.5s: train loss = 0.08242 = (mf)0.07811 + (embed)0.00431\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 0.07952 = (mf)0.07503 + (embed)0.00450, recall = 0.18343, ndcg = 0.28783\n",
            "Epoch 26 (train)4.3s: train loss = 0.07799 = (mf)0.07333 + (embed)0.00466\n",
            "Epoch 27 (train)3.6s: train loss = 0.07442 = (mf)0.06959 + (embed)0.00483\n",
            "Epoch 28 (train)3.5s: train loss = 0.07399 = (mf)0.06900 + (embed)0.00499\n",
            "Epoch 29 (train)4.0s: train loss = 0.07240 = (mf)0.06727 + (embed)0.00513\n",
            "Epoch 30 (train)4.0s + (eval)0.5s: train loss = 0.06879 = (mf)0.06349 + (embed)0.00530, recall = 0.19524, ndcg = 0.30237\n",
            "Epoch 31 (train)3.5s: train loss = 0.06783 = (mf)0.06241 + (embed)0.00542\n",
            "Epoch 32 (train)3.6s: train loss = 0.06802 = (mf)0.06244 + (embed)0.00558\n",
            "Epoch 33 (train)4.2s: train loss = 0.06515 = (mf)0.05944 + (embed)0.00571\n",
            "Epoch 34 (train)3.6s: train loss = 0.06486 = (mf)0.05901 + (embed)0.00585\n",
            "Epoch 35 (train)3.5s + (eval)0.2s: train loss = 0.06191 = (mf)0.05593 + (embed)0.00598, recall = 0.20427, ndcg = 0.31962\n",
            "Epoch 36 (train)4.4s: train loss = 0.06040 = (mf)0.05429 + (embed)0.00611\n",
            "Epoch 37 (train)3.5s: train loss = 0.06043 = (mf)0.05419 + (embed)0.00624\n",
            "Epoch 38 (train)3.5s: train loss = 0.05774 = (mf)0.05139 + (embed)0.00635\n",
            "Epoch 39 (train)4.3s: train loss = 0.05766 = (mf)0.05117 + (embed)0.00649\n",
            "Epoch 40 (train)3.5s + (eval)0.2s: train loss = 0.05606 = (mf)0.04944 + (embed)0.00662, recall = 0.20885, ndcg = 0.32398\n",
            "Epoch 41 (train)3.5s: train loss = 0.05611 = (mf)0.04938 + (embed)0.00673\n",
            "Epoch 42 (train)3.9s: train loss = 0.05452 = (mf)0.04765 + (embed)0.00687\n",
            "Epoch 43 (train)4.0s: train loss = 0.05373 = (mf)0.04677 + (embed)0.00696\n",
            "Epoch 44 (train)3.6s: train loss = 0.05458 = (mf)0.04750 + (embed)0.00707\n",
            "Epoch 45 (train)3.5s + (eval)0.6s: train loss = 0.05213 = (mf)0.04496 + (embed)0.00717, recall = 0.21446, ndcg = 0.32821\n",
            "Epoch 46 (train)4.3s: train loss = 0.05146 = (mf)0.04417 + (embed)0.00729\n",
            "Epoch 47 (train)3.5s: train loss = 0.05056 = (mf)0.04318 + (embed)0.00739\n",
            "Epoch 48 (train)3.4s: train loss = 0.04992 = (mf)0.04243 + (embed)0.00749\n",
            "Epoch 49 (train)4.4s: train loss = 0.04977 = (mf)0.04218 + (embed)0.00759\n",
            "Epoch 50 (train)3.5s + (eval)0.2s: train loss = 0.04842 = (mf)0.04075 + (embed)0.00767, recall = 0.21602, ndcg = 0.32928\n",
            "Training finished in 194.71 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.32928, Recall@20: 0.21602\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.41463 = (mf)0.41438 + (embed)0.00025\n",
            "Epoch 2 (train)4.1s: train loss = 0.23895 = (mf)0.23830 + (embed)0.00064\n",
            "Epoch 3 (train)4.3s: train loss = 0.21784 = (mf)0.21707 + (embed)0.00077\n",
            "Epoch 4 (train)4.8s: train loss = 0.20690 = (mf)0.20596 + (embed)0.00094\n",
            "Epoch 5 (train)4.2s + (eval)0.6s: train loss = 0.19345 = (mf)0.19233 + (embed)0.00113, recall = 0.14524, ndcg = 0.23385\n",
            "Epoch 6 (train)5.1s: train loss = 0.17732 = (mf)0.17597 + (embed)0.00135\n",
            "Epoch 7 (train)4.1s: train loss = 0.16645 = (mf)0.16488 + (embed)0.00157\n",
            "Epoch 8 (train)4.1s: train loss = 0.15767 = (mf)0.15588 + (embed)0.00179\n",
            "Epoch 9 (train)4.9s: train loss = 0.15259 = (mf)0.15063 + (embed)0.00197\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 0.14853 = (mf)0.14638 + (embed)0.00215, recall = 0.15704, ndcg = 0.24985\n",
            "Epoch 11 (train)4.1s: train loss = 0.14351 = (mf)0.14119 + (embed)0.00231\n",
            "Epoch 12 (train)5.0s: train loss = 0.14038 = (mf)0.13792 + (embed)0.00246\n",
            "Epoch 13 (train)4.1s: train loss = 0.13898 = (mf)0.13638 + (embed)0.00260\n",
            "Epoch 14 (train)4.1s: train loss = 0.13211 = (mf)0.12935 + (embed)0.00276\n",
            "Epoch 15 (train)4.9s + (eval)0.5s: train loss = 0.13168 = (mf)0.12878 + (embed)0.00291, recall = 0.15876, ndcg = 0.25283\n",
            "Epoch 16 (train)4.1s: train loss = 0.12729 = (mf)0.12418 + (embed)0.00310\n",
            "Epoch 17 (train)4.5s: train loss = 0.12147 = (mf)0.11818 + (embed)0.00328\n",
            "Epoch 18 (train)4.5s: train loss = 0.11990 = (mf)0.11642 + (embed)0.00348\n",
            "Epoch 19 (train)4.0s: train loss = 0.11320 = (mf)0.10951 + (embed)0.00370\n",
            "Epoch 20 (train)4.6s + (eval)0.3s: train loss = 0.11109 = (mf)0.10720 + (embed)0.00389, recall = 0.16926, ndcg = 0.26707\n",
            "Epoch 21 (train)4.2s: train loss = 0.10910 = (mf)0.10501 + (embed)0.00409\n",
            "Epoch 22 (train)4.1s: train loss = 0.10388 = (mf)0.09958 + (embed)0.00431\n",
            "Epoch 23 (train)4.9s: train loss = 0.10263 = (mf)0.09814 + (embed)0.00450\n",
            "Epoch 24 (train)4.2s: train loss = 0.09976 = (mf)0.09506 + (embed)0.00470\n",
            "Epoch 25 (train)4.2s + (eval)0.5s: train loss = 0.09716 = (mf)0.09225 + (embed)0.00491, recall = 0.17589, ndcg = 0.27671\n",
            "Epoch 26 (train)4.9s: train loss = 0.09618 = (mf)0.09107 + (embed)0.00511\n",
            "Epoch 27 (train)4.2s: train loss = 0.09363 = (mf)0.08834 + (embed)0.00529\n",
            "Epoch 28 (train)4.2s: train loss = 0.08899 = (mf)0.08353 + (embed)0.00546\n",
            "Epoch 29 (train)5.1s: train loss = 0.08877 = (mf)0.08311 + (embed)0.00566\n",
            "Epoch 30 (train)4.2s + (eval)0.2s: train loss = 0.08419 = (mf)0.07832 + (embed)0.00587, recall = 0.18556, ndcg = 0.28737\n",
            "Epoch 31 (train)4.7s: train loss = 0.08302 = (mf)0.07696 + (embed)0.00606\n",
            "Epoch 32 (train)4.7s: train loss = 0.08070 = (mf)0.07446 + (embed)0.00624\n",
            "Epoch 33 (train)4.2s: train loss = 0.07845 = (mf)0.07201 + (embed)0.00644\n",
            "Epoch 34 (train)5.0s: train loss = 0.07752 = (mf)0.07090 + (embed)0.00662\n",
            "Epoch 35 (train)4.3s + (eval)0.2s: train loss = 0.07544 = (mf)0.06867 + (embed)0.00676, recall = 0.19572, ndcg = 0.30357\n",
            "Epoch 36 (train)4.2s: train loss = 0.07410 = (mf)0.06718 + (embed)0.00693\n",
            "Epoch 37 (train)5.1s: train loss = 0.07219 = (mf)0.06508 + (embed)0.00711\n",
            "Epoch 38 (train)4.2s: train loss = 0.07122 = (mf)0.06394 + (embed)0.00727\n",
            "Epoch 39 (train)4.2s: train loss = 0.06924 = (mf)0.06179 + (embed)0.00745\n",
            "Epoch 40 (train)5.1s + (eval)0.5s: train loss = 0.06685 = (mf)0.05924 + (embed)0.00761, recall = 0.20232, ndcg = 0.31681\n",
            "Epoch 41 (train)4.2s: train loss = 0.06857 = (mf)0.06084 + (embed)0.00772\n",
            "Epoch 42 (train)4.6s: train loss = 0.06647 = (mf)0.05860 + (embed)0.00786\n",
            "Epoch 43 (train)4.6s: train loss = 0.06415 = (mf)0.05614 + (embed)0.00802\n",
            "Epoch 44 (train)4.2s: train loss = 0.06466 = (mf)0.05652 + (embed)0.00814\n",
            "Epoch 45 (train)5.0s + (eval)0.3s: train loss = 0.06383 = (mf)0.05556 + (embed)0.00828, recall = 0.20500, ndcg = 0.32039\n",
            "Epoch 46 (train)4.2s: train loss = 0.06130 = (mf)0.05289 + (embed)0.00841\n",
            "Epoch 47 (train)4.2s: train loss = 0.06189 = (mf)0.05334 + (embed)0.00855\n",
            "Epoch 48 (train)5.0s: train loss = 0.06308 = (mf)0.05445 + (embed)0.00863\n",
            "Epoch 49 (train)4.0s: train loss = 0.06002 = (mf)0.05124 + (embed)0.00879\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 0.06031 = (mf)0.05144 + (embed)0.00888, recall = 0.21072, ndcg = 0.32889\n",
            "Training finished in 227.03 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.32889, Recall@20: 0.21072\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.6s: train loss = 0.22582 = (mf)0.22423 + (embed)0.00158\n",
            "Epoch 2 (train)3.0s: train loss = 0.10912 = (mf)0.10562 + (embed)0.00350\n",
            "Epoch 3 (train)3.8s: train loss = 0.08318 = (mf)0.07793 + (embed)0.00525\n",
            "Epoch 4 (train)2.9s: train loss = 0.06533 = (mf)0.05847 + (embed)0.00686\n",
            "Epoch 5 (train)2.9s + (eval)0.6s: train loss = 0.05716 = (mf)0.04898 + (embed)0.00818, recall = 0.17233, ndcg = 0.25849\n",
            "Epoch 6 (train)2.9s: train loss = 0.05337 = (mf)0.04397 + (embed)0.00940\n",
            "Epoch 7 (train)3.8s: train loss = 0.04919 = (mf)0.03885 + (embed)0.01035\n",
            "Epoch 8 (train)2.9s: train loss = 0.04712 = (mf)0.03591 + (embed)0.01121\n",
            "Epoch 9 (train)2.9s: train loss = 0.04567 = (mf)0.03364 + (embed)0.01203\n",
            "Epoch 10 (train)2.9s + (eval)0.6s: train loss = 0.04444 = (mf)0.03160 + (embed)0.01284, recall = 0.17451, ndcg = 0.26018\n",
            "Epoch 11 (train)3.7s: train loss = 0.04509 = (mf)0.03156 + (embed)0.01353\n",
            "Epoch 12 (train)2.9s: train loss = 0.04487 = (mf)0.03072 + (embed)0.01415\n",
            "Epoch 13 (train)2.9s: train loss = 0.04371 = (mf)0.02902 + (embed)0.01470\n",
            "Epoch 14 (train)2.9s: train loss = 0.04505 = (mf)0.02972 + (embed)0.01533\n",
            "Epoch 15 (train)3.7s + (eval)0.2s: train loss = 0.04665 = (mf)0.03076 + (embed)0.01589, recall = 0.16861, ndcg = 0.24225\n",
            "Epoch 16 (train)2.9s: train loss = 0.04639 = (mf)0.02992 + (embed)0.01646\n",
            "Epoch 17 (train)2.9s: train loss = 0.04653 = (mf)0.02963 + (embed)0.01689\n",
            "Epoch 18 (train)3.1s: train loss = 0.04562 = (mf)0.02824 + (embed)0.01739\n",
            "Epoch 19 (train)3.7s: train loss = 0.04677 = (mf)0.02892 + (embed)0.01785\n",
            "Epoch 20 (train)2.9s + (eval)0.2s: train loss = 0.04602 = (mf)0.02777 + (embed)0.01825, recall = 0.16930, ndcg = 0.24333\n",
            "Epoch 21 (train)2.9s: train loss = 0.04721 = (mf)0.02859 + (embed)0.01862\n",
            "Epoch 22 (train)3.2s: train loss = 0.04763 = (mf)0.02867 + (embed)0.01896\n",
            "Epoch 23 (train)3.5s: train loss = 0.04628 = (mf)0.02692 + (embed)0.01936\n",
            "Epoch 24 (train)2.9s: train loss = 0.04691 = (mf)0.02718 + (embed)0.01974\n",
            "Epoch 25 (train)2.9s + (eval)0.5s: train loss = 0.04698 = (mf)0.02691 + (embed)0.02007, recall = 0.15766, ndcg = 0.22499\n",
            "Epoch 26 (train)3.5s: train loss = 0.04962 = (mf)0.02927 + (embed)0.02035\n",
            "Epoch 27 (train)3.2s: train loss = 0.04833 = (mf)0.02773 + (embed)0.02060\n",
            "Epoch 28 (train)2.9s: train loss = 0.04805 = (mf)0.02712 + (embed)0.02093\n",
            "Epoch 29 (train)2.9s: train loss = 0.04933 = (mf)0.02818 + (embed)0.02114\n",
            "Epoch 30 (train)3.4s + (eval)0.3s: train loss = 0.04775 = (mf)0.02628 + (embed)0.02147, recall = 0.15898, ndcg = 0.22798\n",
            "Epoch 31 (train)3.2s: train loss = 0.04888 = (mf)0.02711 + (embed)0.02178\n",
            "Epoch 32 (train)2.9s: train loss = 0.04770 = (mf)0.02563 + (embed)0.02207\n",
            "Epoch 33 (train)2.9s: train loss = 0.04745 = (mf)0.02516 + (embed)0.02229\n",
            "Epoch 34 (train)3.5s: train loss = 0.04935 = (mf)0.02680 + (embed)0.02255\n",
            "Epoch 35 (train)3.1s + (eval)0.2s: train loss = 0.04814 = (mf)0.02541 + (embed)0.02273, recall = 0.16265, ndcg = 0.23087\n",
            "Epoch 36 (train)2.9s: train loss = 0.04858 = (mf)0.02564 + (embed)0.02294\n",
            "Epoch 37 (train)2.9s: train loss = 0.04726 = (mf)0.02409 + (embed)0.02317\n",
            "Epoch 38 (train)3.6s: train loss = 0.04995 = (mf)0.02661 + (embed)0.02334\n",
            "Epoch 39 (train)3.2s: train loss = 0.05059 = (mf)0.02702 + (embed)0.02356\n",
            "Epoch 40 (train)2.9s + (eval)0.5s: train loss = 0.04803 = (mf)0.02424 + (embed)0.02379, recall = 0.15029, ndcg = 0.21502\n",
            "Epoch 41 (train)2.9s: train loss = 0.05216 = (mf)0.02808 + (embed)0.02408\n",
            "Epoch 42 (train)3.7s: train loss = 0.05117 = (mf)0.02683 + (embed)0.02434\n",
            "Epoch 43 (train)2.9s: train loss = 0.04983 = (mf)0.02530 + (embed)0.02453\n",
            "Epoch 44 (train)2.9s: train loss = 0.05049 = (mf)0.02575 + (embed)0.02474\n",
            "Epoch 45 (train)2.9s + (eval)0.2s: train loss = 0.05111 = (mf)0.02634 + (embed)0.02477, recall = 0.15081, ndcg = 0.21983\n",
            "Epoch 46 (train)3.7s: train loss = 0.05092 = (mf)0.02606 + (embed)0.02486\n",
            "Epoch 47 (train)3.0s: train loss = 0.05124 = (mf)0.02621 + (embed)0.02504\n",
            "Epoch 48 (train)2.9s: train loss = 0.05013 = (mf)0.02496 + (embed)0.02517\n",
            "Epoch 49 (train)2.9s: train loss = 0.05108 = (mf)0.02573 + (embed)0.02535\n",
            "Epoch 50 (train)3.6s + (eval)0.3s: train loss = 0.05128 = (mf)0.02580 + (embed)0.02548, recall = 0.14761, ndcg = 0.20895\n",
            "Training finished in 160.91 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.20895, Recall@20: 0.14761\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.0s: train loss = 0.22467 = (mf)0.22305 + (embed)0.00163\n",
            "Epoch 2 (train)4.3s: train loss = 0.12142 = (mf)0.11776 + (embed)0.00367\n",
            "Epoch 3 (train)3.4s: train loss = 0.09121 = (mf)0.08567 + (embed)0.00554\n",
            "Epoch 4 (train)3.4s: train loss = 0.07391 = (mf)0.06666 + (embed)0.00725\n",
            "Epoch 5 (train)4.3s + (eval)1.0s: train loss = 0.06511 = (mf)0.05639 + (embed)0.00872, recall = 0.18634, ndcg = 0.28189\n",
            "Epoch 6 (train)3.4s: train loss = 0.05811 = (mf)0.04810 + (embed)0.01001\n",
            "Epoch 7 (train)3.5s: train loss = 0.05369 = (mf)0.04254 + (embed)0.01116\n",
            "Epoch 8 (train)4.0s: train loss = 0.05166 = (mf)0.03950 + (embed)0.01216\n",
            "Epoch 9 (train)3.8s: train loss = 0.04776 = (mf)0.03477 + (embed)0.01299\n",
            "Epoch 10 (train)3.4s + (eval)0.2s: train loss = 0.04852 = (mf)0.03477 + (embed)0.01375, recall = 0.19076, ndcg = 0.27489\n",
            "Epoch 11 (train)3.5s: train loss = 0.04571 = (mf)0.03129 + (embed)0.01442\n",
            "Epoch 12 (train)4.3s: train loss = 0.04519 = (mf)0.03013 + (embed)0.01506\n",
            "Epoch 13 (train)3.4s: train loss = 0.04529 = (mf)0.02970 + (embed)0.01559\n",
            "Epoch 14 (train)3.4s: train loss = 0.04315 = (mf)0.02709 + (embed)0.01606\n",
            "Epoch 15 (train)4.2s + (eval)0.2s: train loss = 0.04558 = (mf)0.02906 + (embed)0.01652, recall = 0.18367, ndcg = 0.25786\n",
            "Epoch 16 (train)3.4s: train loss = 0.04282 = (mf)0.02584 + (embed)0.01698\n",
            "Epoch 17 (train)3.4s: train loss = 0.04097 = (mf)0.02358 + (embed)0.01739\n",
            "Epoch 18 (train)3.8s: train loss = 0.04262 = (mf)0.02495 + (embed)0.01768\n",
            "Epoch 19 (train)3.9s: train loss = 0.04193 = (mf)0.02394 + (embed)0.01800\n",
            "Epoch 20 (train)3.5s + (eval)0.5s: train loss = 0.04324 = (mf)0.02494 + (embed)0.01830, recall = 0.17787, ndcg = 0.25475\n",
            "Epoch 21 (train)3.5s: train loss = 0.04425 = (mf)0.02565 + (embed)0.01859\n",
            "Epoch 22 (train)4.3s: train loss = 0.04140 = (mf)0.02247 + (embed)0.01893\n",
            "Epoch 23 (train)3.4s: train loss = 0.04246 = (mf)0.02326 + (embed)0.01920\n",
            "Epoch 24 (train)3.5s: train loss = 0.04234 = (mf)0.02291 + (embed)0.01943\n",
            "Epoch 25 (train)4.3s + (eval)0.2s: train loss = 0.04147 = (mf)0.02183 + (embed)0.01964, recall = 0.18425, ndcg = 0.26314\n",
            "Epoch 26 (train)3.5s: train loss = 0.04110 = (mf)0.02125 + (embed)0.01985\n",
            "Epoch 27 (train)3.5s: train loss = 0.04172 = (mf)0.02165 + (embed)0.02008\n",
            "Epoch 28 (train)4.0s: train loss = 0.04069 = (mf)0.02039 + (embed)0.02030\n",
            "Epoch 29 (train)3.8s: train loss = 0.04202 = (mf)0.02156 + (embed)0.02046\n",
            "Epoch 30 (train)3.4s + (eval)0.2s: train loss = 0.04171 = (mf)0.02104 + (embed)0.02067, recall = 0.18417, ndcg = 0.26204\n",
            "Epoch 31 (train)3.4s: train loss = 0.04059 = (mf)0.01976 + (embed)0.02083\n",
            "Epoch 32 (train)4.3s: train loss = 0.04137 = (mf)0.02038 + (embed)0.02099\n",
            "Epoch 33 (train)3.5s: train loss = 0.04114 = (mf)0.02003 + (embed)0.02111\n",
            "Epoch 34 (train)3.4s: train loss = 0.04128 = (mf)0.02000 + (embed)0.02128\n",
            "Epoch 35 (train)4.3s + (eval)0.5s: train loss = 0.04162 = (mf)0.02015 + (embed)0.02146, recall = 0.17816, ndcg = 0.24786\n",
            "Epoch 36 (train)3.5s: train loss = 0.04141 = (mf)0.01979 + (embed)0.02162\n",
            "Epoch 37 (train)3.5s: train loss = 0.04072 = (mf)0.01902 + (embed)0.02170\n",
            "Epoch 38 (train)4.1s: train loss = 0.04228 = (mf)0.02037 + (embed)0.02191\n",
            "Epoch 39 (train)3.8s: train loss = 0.04144 = (mf)0.01940 + (embed)0.02204\n",
            "Epoch 40 (train)3.6s + (eval)0.2s: train loss = 0.04065 = (mf)0.01851 + (embed)0.02215, recall = 0.18277, ndcg = 0.25040\n",
            "Epoch 41 (train)3.7s: train loss = 0.04146 = (mf)0.01918 + (embed)0.02228\n",
            "Epoch 42 (train)4.2s: train loss = 0.04041 = (mf)0.01792 + (embed)0.02249\n",
            "Epoch 43 (train)3.5s: train loss = 0.04220 = (mf)0.01960 + (embed)0.02260\n",
            "Epoch 44 (train)3.5s: train loss = 0.03990 = (mf)0.01722 + (embed)0.02267\n",
            "Epoch 45 (train)4.2s + (eval)0.2s: train loss = 0.04214 = (mf)0.01934 + (embed)0.02279, recall = 0.17940, ndcg = 0.25099\n",
            "Epoch 46 (train)3.4s: train loss = 0.04069 = (mf)0.01789 + (embed)0.02281\n",
            "Epoch 47 (train)3.4s: train loss = 0.04157 = (mf)0.01865 + (embed)0.02291\n",
            "Epoch 48 (train)4.0s: train loss = 0.04104 = (mf)0.01799 + (embed)0.02305\n",
            "Epoch 49 (train)3.7s: train loss = 0.04127 = (mf)0.01810 + (embed)0.02317\n",
            "Epoch 50 (train)3.4s + (eval)0.5s: train loss = 0.04216 = (mf)0.01892 + (embed)0.02324, recall = 0.18006, ndcg = 0.24919\n",
            "Training finished in 190.04 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.24919, Recall@20: 0.18006\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.0s: train loss = 0.23844 = (mf)0.23680 + (embed)0.00164\n",
            "Epoch 2 (train)4.1s: train loss = 0.13292 = (mf)0.12891 + (embed)0.00401\n",
            "Epoch 3 (train)4.8s: train loss = 0.10432 = (mf)0.09831 + (embed)0.00601\n",
            "Epoch 4 (train)4.0s: train loss = 0.08306 = (mf)0.07511 + (embed)0.00795\n",
            "Epoch 5 (train)4.0s + (eval)1.1s: train loss = 0.07559 = (mf)0.06594 + (embed)0.00965, recall = 0.18727, ndcg = 0.28406\n",
            "Epoch 6 (train)4.7s: train loss = 0.06538 = (mf)0.05422 + (embed)0.01116\n",
            "Epoch 7 (train)4.0s: train loss = 0.05966 = (mf)0.04723 + (embed)0.01244\n",
            "Epoch 8 (train)4.2s: train loss = 0.05745 = (mf)0.04397 + (embed)0.01348\n",
            "Epoch 9 (train)4.6s: train loss = 0.05349 = (mf)0.03902 + (embed)0.01447\n",
            "Epoch 10 (train)4.0s + (eval)0.2s: train loss = 0.05090 = (mf)0.03562 + (embed)0.01528, recall = 0.20181, ndcg = 0.30095\n",
            "Epoch 11 (train)4.6s: train loss = 0.05118 = (mf)0.03532 + (embed)0.01586\n",
            "Epoch 12 (train)4.3s: train loss = 0.04975 = (mf)0.03330 + (embed)0.01646\n",
            "Epoch 13 (train)4.1s: train loss = 0.04684 = (mf)0.02980 + (embed)0.01703\n",
            "Epoch 14 (train)4.8s: train loss = 0.04653 = (mf)0.02908 + (embed)0.01745\n",
            "Epoch 15 (train)4.2s + (eval)0.2s: train loss = 0.04574 = (mf)0.02789 + (embed)0.01785, recall = 0.19825, ndcg = 0.29475\n",
            "Epoch 16 (train)4.0s: train loss = 0.04412 = (mf)0.02595 + (embed)0.01817\n",
            "Epoch 17 (train)4.8s: train loss = 0.04486 = (mf)0.02629 + (embed)0.01858\n",
            "Epoch 18 (train)4.0s: train loss = 0.04331 = (mf)0.02441 + (embed)0.01891\n",
            "Epoch 19 (train)4.0s: train loss = 0.04351 = (mf)0.02434 + (embed)0.01917\n",
            "Epoch 20 (train)4.8s + (eval)0.5s: train loss = 0.04376 = (mf)0.02434 + (embed)0.01942, recall = 0.18836, ndcg = 0.27907\n",
            "Epoch 21 (train)4.0s: train loss = 0.04388 = (mf)0.02414 + (embed)0.01973\n",
            "Epoch 22 (train)4.0s: train loss = 0.04217 = (mf)0.02224 + (embed)0.01993\n",
            "Epoch 23 (train)4.8s: train loss = 0.04345 = (mf)0.02334 + (embed)0.02011\n",
            "Epoch 24 (train)4.0s: train loss = 0.04128 = (mf)0.02093 + (embed)0.02035\n",
            "Epoch 25 (train)4.1s + (eval)0.2s: train loss = 0.04055 = (mf)0.02009 + (embed)0.02046, recall = 0.20449, ndcg = 0.29161\n",
            "Epoch 26 (train)4.9s: train loss = 0.04125 = (mf)0.02061 + (embed)0.02065\n",
            "Epoch 27 (train)4.1s: train loss = 0.04145 = (mf)0.02064 + (embed)0.02081\n",
            "Epoch 28 (train)4.2s: train loss = 0.03989 = (mf)0.01895 + (embed)0.02094\n",
            "Epoch 29 (train)4.8s: train loss = 0.04084 = (mf)0.01976 + (embed)0.02107\n",
            "Epoch 30 (train)4.0s + (eval)0.5s: train loss = 0.04077 = (mf)0.01949 + (embed)0.02128, recall = 0.19502, ndcg = 0.28099\n",
            "Epoch 31 (train)4.6s: train loss = 0.04118 = (mf)0.01979 + (embed)0.02139\n",
            "Epoch 32 (train)4.3s: train loss = 0.04066 = (mf)0.01911 + (embed)0.02154\n",
            "Epoch 33 (train)4.0s: train loss = 0.04040 = (mf)0.01865 + (embed)0.02175\n",
            "Epoch 34 (train)4.8s: train loss = 0.04121 = (mf)0.01932 + (embed)0.02190\n",
            "Epoch 35 (train)4.2s + (eval)0.2s: train loss = 0.04034 = (mf)0.01830 + (embed)0.02204, recall = 0.20262, ndcg = 0.29286\n",
            "Epoch 36 (train)4.0s: train loss = 0.04059 = (mf)0.01857 + (embed)0.02202\n",
            "Epoch 37 (train)4.9s: train loss = 0.04120 = (mf)0.01896 + (embed)0.02224\n",
            "Epoch 38 (train)4.1s: train loss = 0.03983 = (mf)0.01746 + (embed)0.02237\n",
            "Epoch 39 (train)4.1s: train loss = 0.03891 = (mf)0.01642 + (embed)0.02249\n",
            "Epoch 40 (train)5.0s + (eval)0.2s: train loss = 0.03950 = (mf)0.01693 + (embed)0.02256, recall = 0.19243, ndcg = 0.27837\n",
            "Epoch 41 (train)4.1s: train loss = 0.04019 = (mf)0.01765 + (embed)0.02255\n",
            "Epoch 42 (train)4.1s: train loss = 0.04028 = (mf)0.01750 + (embed)0.02278\n",
            "Epoch 43 (train)4.9s: train loss = 0.03983 = (mf)0.01703 + (embed)0.02280\n",
            "Epoch 44 (train)4.1s: train loss = 0.04039 = (mf)0.01749 + (embed)0.02290\n",
            "Epoch 45 (train)4.2s + (eval)0.7s: train loss = 0.03891 = (mf)0.01592 + (embed)0.02299, recall = 0.19531, ndcg = 0.28182\n",
            "Epoch 46 (train)4.6s: train loss = 0.04023 = (mf)0.01730 + (embed)0.02293\n",
            "Epoch 47 (train)4.0s: train loss = 0.03925 = (mf)0.01619 + (embed)0.02306\n",
            "Epoch 48 (train)4.6s: train loss = 0.04052 = (mf)0.01738 + (embed)0.02314\n",
            "Epoch 49 (train)4.5s: train loss = 0.04079 = (mf)0.01745 + (embed)0.02334\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 0.04026 = (mf)0.01689 + (embed)0.02337, recall = 0.19939, ndcg = 0.28521\n",
            "Training finished in 221.35 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.28521, Recall@20: 0.19939\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.7s: train loss = 0.63172 = (mf)0.56792 + (embed)0.06379\n",
            "Epoch 2 (train)3.0s: train loss = 1.23466 = (mf)1.03370 + (embed)0.20096\n",
            "Epoch 3 (train)3.6s: train loss = 1.73725 = (mf)1.39923 + (embed)0.33802\n",
            "Epoch 4 (train)3.2s: train loss = 2.04311 = (mf)1.58165 + (embed)0.46146\n",
            "Epoch 5 (train)3.1s + (eval)0.6s: train loss = 2.45214 = (mf)1.87469 + (embed)0.57745, recall = 0.10418, ndcg = 0.16006\n",
            "Epoch 6 (train)3.0s: train loss = 2.77553 = (mf)2.08849 + (embed)0.68704\n",
            "Epoch 7 (train)3.8s: train loss = 2.88783 = (mf)2.10841 + (embed)0.77943\n",
            "Epoch 8 (train)3.0s: train loss = 3.17185 = (mf)2.30724 + (embed)0.86461\n",
            "Epoch 9 (train)3.0s: train loss = 3.37317 = (mf)2.42781 + (embed)0.94535\n",
            "Epoch 10 (train)3.0s + (eval)0.2s: train loss = 3.38400 = (mf)2.36314 + (embed)1.02085, recall = 0.09419, ndcg = 0.14951\n",
            "Epoch 11 (train)3.9s: train loss = 3.61334 = (mf)2.52192 + (embed)1.09142\n",
            "Epoch 12 (train)3.0s: train loss = 3.81501 = (mf)2.65083 + (embed)1.16418\n",
            "Epoch 13 (train)3.0s: train loss = 4.16456 = (mf)2.93552 + (embed)1.22904\n",
            "Epoch 14 (train)3.2s: train loss = 3.75827 = (mf)2.46822 + (embed)1.29005\n",
            "Epoch 15 (train)3.7s + (eval)0.5s: train loss = 4.19646 = (mf)2.85501 + (embed)1.34145, recall = 0.10145, ndcg = 0.16439\n",
            "Epoch 16 (train)3.1s: train loss = 4.11552 = (mf)2.71946 + (embed)1.39605\n",
            "Epoch 17 (train)3.0s: train loss = 4.29715 = (mf)2.84365 + (embed)1.45350\n",
            "Epoch 18 (train)3.6s: train loss = 4.44633 = (mf)2.93588 + (embed)1.51046\n",
            "Epoch 19 (train)3.2s: train loss = 4.69382 = (mf)3.13150 + (embed)1.56232\n",
            "Epoch 20 (train)3.0s + (eval)0.2s: train loss = 4.93544 = (mf)3.32605 + (embed)1.60939, recall = 0.10885, ndcg = 0.16586\n",
            "Epoch 21 (train)3.0s: train loss = 4.78340 = (mf)3.13141 + (embed)1.65199\n",
            "Epoch 22 (train)3.8s: train loss = 4.68074 = (mf)2.98026 + (embed)1.70048\n",
            "Epoch 23 (train)3.0s: train loss = 4.87221 = (mf)3.13896 + (embed)1.73325\n",
            "Epoch 24 (train)3.0s: train loss = 4.47597 = (mf)2.69957 + (embed)1.77639\n",
            "Epoch 25 (train)3.0s + (eval)0.2s: train loss = 4.82650 = (mf)3.02313 + (embed)1.80337, recall = 0.11215, ndcg = 0.17327\n",
            "Epoch 26 (train)3.9s: train loss = 4.74957 = (mf)2.91976 + (embed)1.82981\n",
            "Epoch 27 (train)3.0s: train loss = 4.66739 = (mf)2.79855 + (embed)1.86884\n",
            "Epoch 28 (train)3.1s: train loss = 4.98607 = (mf)3.09160 + (embed)1.89447\n",
            "Epoch 29 (train)3.2s: train loss = 5.22681 = (mf)3.29615 + (embed)1.93066\n",
            "Epoch 30 (train)4.1s + (eval)0.3s: train loss = 4.68073 = (mf)2.71832 + (embed)1.96242, recall = 0.11059, ndcg = 0.16971\n",
            "Epoch 31 (train)3.3s: train loss = 4.94041 = (mf)2.94900 + (embed)1.99141\n",
            "Epoch 32 (train)3.1s: train loss = 4.75293 = (mf)2.73218 + (embed)2.02075\n",
            "Epoch 33 (train)3.5s: train loss = 4.72731 = (mf)2.68119 + (embed)2.04612\n",
            "Epoch 34 (train)3.5s: train loss = 5.18925 = (mf)3.12268 + (embed)2.06657\n",
            "Epoch 35 (train)3.0s + (eval)0.5s: train loss = 5.23377 = (mf)3.14154 + (embed)2.09224, recall = 0.10357, ndcg = 0.16086\n",
            "Epoch 36 (train)3.0s: train loss = 5.09823 = (mf)2.97669 + (embed)2.12154\n",
            "Epoch 37 (train)3.8s: train loss = 5.48523 = (mf)3.33377 + (embed)2.15147\n",
            "Epoch 38 (train)3.0s: train loss = 5.28767 = (mf)3.10519 + (embed)2.18248\n",
            "Epoch 39 (train)3.0s: train loss = 5.20558 = (mf)2.99998 + (embed)2.20560\n",
            "Epoch 40 (train)3.0s + (eval)0.2s: train loss = 5.63682 = (mf)3.40884 + (embed)2.22798, recall = 0.10645, ndcg = 0.16687\n",
            "Epoch 41 (train)4.0s: train loss = 5.14763 = (mf)2.90361 + (embed)2.24403\n",
            "Epoch 42 (train)3.1s: train loss = 5.25426 = (mf)2.99164 + (embed)2.26262\n",
            "Epoch 43 (train)3.0s: train loss = 5.17700 = (mf)2.89765 + (embed)2.27935\n",
            "Epoch 44 (train)3.0s: train loss = 5.44867 = (mf)3.15055 + (embed)2.29812\n",
            "Epoch 45 (train)3.9s + (eval)0.6s: train loss = 5.30088 = (mf)2.98553 + (embed)2.31535, recall = 0.10768, ndcg = 0.16220\n",
            "Epoch 46 (train)3.1s: train loss = 5.22054 = (mf)2.89429 + (embed)2.32625\n",
            "Epoch 47 (train)3.1s: train loss = 5.09343 = (mf)2.74927 + (embed)2.34415\n",
            "Epoch 48 (train)3.5s: train loss = 5.33292 = (mf)2.96018 + (embed)2.37274\n",
            "Epoch 49 (train)3.5s: train loss = 5.27717 = (mf)2.88527 + (embed)2.39190\n",
            "Epoch 50 (train)3.0s + (eval)0.2s: train loss = 5.24279 = (mf)2.84045 + (embed)2.40234, recall = 0.11713, ndcg = 0.16916\n",
            "Training finished in 167.58 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.16916, Recall@20: 0.11713\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.6s: train loss = 0.44090 = (mf)0.38722 + (embed)0.05369\n",
            "Epoch 2 (train)3.6s: train loss = 0.80943 = (mf)0.63704 + (embed)0.17240\n",
            "Epoch 3 (train)3.8s: train loss = 1.14371 = (mf)0.84803 + (embed)0.29569\n",
            "Epoch 4 (train)4.1s: train loss = 1.33987 = (mf)0.93552 + (embed)0.40435\n",
            "Epoch 5 (train)3.6s + (eval)0.7s: train loss = 1.59281 = (mf)1.09111 + (embed)0.50170, recall = 0.11502, ndcg = 0.18400\n",
            "Epoch 6 (train)4.0s: train loss = 1.81494 = (mf)1.22375 + (embed)0.59120\n",
            "Epoch 7 (train)4.4s: train loss = 2.08160 = (mf)1.40548 + (embed)0.67612\n",
            "Epoch 8 (train)3.8s: train loss = 2.07580 = (mf)1.31737 + (embed)0.75842\n",
            "Epoch 9 (train)3.5s: train loss = 2.43009 = (mf)1.59707 + (embed)0.83302\n",
            "Epoch 10 (train)4.4s + (eval)0.2s: train loss = 2.35476 = (mf)1.45860 + (embed)0.89616, recall = 0.10446, ndcg = 0.16265\n",
            "Epoch 11 (train)3.6s: train loss = 2.45450 = (mf)1.50454 + (embed)0.94996\n",
            "Epoch 12 (train)3.5s: train loss = 2.44930 = (mf)1.44399 + (embed)1.00531\n",
            "Epoch 13 (train)4.4s: train loss = 2.77214 = (mf)1.71912 + (embed)1.05302\n",
            "Epoch 14 (train)3.5s: train loss = 2.72794 = (mf)1.62791 + (embed)1.10002\n",
            "Epoch 15 (train)3.5s + (eval)0.5s: train loss = 2.75854 = (mf)1.61529 + (embed)1.14325, recall = 0.11748, ndcg = 0.18222\n",
            "Epoch 16 (train)4.4s: train loss = 2.85340 = (mf)1.67599 + (embed)1.17741\n",
            "Epoch 17 (train)3.6s: train loss = 2.86320 = (mf)1.64933 + (embed)1.21387\n",
            "Epoch 18 (train)3.5s: train loss = 2.84599 = (mf)1.59391 + (embed)1.25208\n",
            "Epoch 19 (train)3.9s: train loss = 2.91956 = (mf)1.63676 + (embed)1.28281\n",
            "Epoch 20 (train)3.9s + (eval)0.2s: train loss = 2.99054 = (mf)1.68130 + (embed)1.30925, recall = 0.10848, ndcg = 0.16690\n",
            "Epoch 21 (train)3.5s: train loss = 3.01681 = (mf)1.67223 + (embed)1.34459\n",
            "Epoch 22 (train)3.5s: train loss = 3.12974 = (mf)1.76064 + (embed)1.36909\n",
            "Epoch 23 (train)4.5s: train loss = 3.05688 = (mf)1.66439 + (embed)1.39249\n",
            "Epoch 24 (train)3.5s: train loss = 3.00626 = (mf)1.58408 + (embed)1.42217\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 3.17786 = (mf)1.73546 + (embed)1.44240, recall = 0.10242, ndcg = 0.16067\n",
            "Epoch 26 (train)4.4s: train loss = 3.28559 = (mf)1.81843 + (embed)1.46716\n",
            "Epoch 27 (train)3.5s: train loss = 3.14490 = (mf)1.64971 + (embed)1.49519\n",
            "Epoch 28 (train)3.5s: train loss = 3.22116 = (mf)1.70373 + (embed)1.51743\n",
            "Epoch 29 (train)4.2s: train loss = 3.16029 = (mf)1.62713 + (embed)1.53315\n",
            "Epoch 30 (train)3.7s + (eval)0.5s: train loss = 3.21061 = (mf)1.66459 + (embed)1.54601, recall = 0.09918, ndcg = 0.15316\n",
            "Epoch 31 (train)3.5s: train loss = 3.05346 = (mf)1.49141 + (embed)1.56204\n",
            "Epoch 32 (train)3.9s: train loss = 3.26812 = (mf)1.69527 + (embed)1.57285\n",
            "Epoch 33 (train)4.1s: train loss = 3.25019 = (mf)1.66523 + (embed)1.58496\n",
            "Epoch 34 (train)3.6s: train loss = 3.18110 = (mf)1.58056 + (embed)1.60053\n",
            "Epoch 35 (train)3.7s + (eval)0.3s: train loss = 3.33109 = (mf)1.72280 + (embed)1.60828, recall = 0.10805, ndcg = 0.16364\n",
            "Epoch 36 (train)4.3s: train loss = 3.24197 = (mf)1.62470 + (embed)1.61727\n",
            "Epoch 37 (train)3.5s: train loss = 3.27530 = (mf)1.64788 + (embed)1.62741\n",
            "Epoch 38 (train)3.5s: train loss = 3.11053 = (mf)1.46394 + (embed)1.64659\n",
            "Epoch 39 (train)4.3s: train loss = 3.27949 = (mf)1.62631 + (embed)1.65317\n",
            "Epoch 40 (train)3.5s + (eval)0.2s: train loss = 3.18403 = (mf)1.52202 + (embed)1.66201, recall = 0.11613, ndcg = 0.16858\n",
            "Epoch 41 (train)3.5s: train loss = 3.29382 = (mf)1.61825 + (embed)1.67558\n",
            "Epoch 42 (train)4.1s: train loss = 3.35544 = (mf)1.67074 + (embed)1.68470\n",
            "Epoch 43 (train)3.7s: train loss = 3.28785 = (mf)1.58082 + (embed)1.70703\n",
            "Epoch 44 (train)3.5s: train loss = 3.13936 = (mf)1.41875 + (embed)1.72061\n",
            "Epoch 45 (train)3.5s + (eval)0.3s: train loss = 3.24689 = (mf)1.51625 + (embed)1.73063, recall = 0.10619, ndcg = 0.15637\n",
            "Epoch 46 (train)4.2s: train loss = 3.32606 = (mf)1.58388 + (embed)1.74218\n",
            "Epoch 47 (train)3.5s: train loss = 3.22720 = (mf)1.47025 + (embed)1.75695\n",
            "Epoch 48 (train)3.5s: train loss = 3.36409 = (mf)1.59628 + (embed)1.76781\n",
            "Epoch 49 (train)4.4s: train loss = 3.40285 = (mf)1.62396 + (embed)1.77889\n",
            "Epoch 50 (train)3.6s + (eval)0.6s: train loss = 3.28123 = (mf)1.49475 + (embed)1.78648, recall = 0.10470, ndcg = 0.16787\n",
            "Training finished in 194.68 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.16787, Recall@20: 0.10470\n",
            "--- Trying combination: Epochs=50, TopK=20, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.37932 = (mf)0.33520 + (embed)0.04412\n",
            "Epoch 2 (train)4.1s: train loss = 0.53890 = (mf)0.39308 + (embed)0.14581\n",
            "Epoch 3 (train)4.8s: train loss = 0.79850 = (mf)0.54713 + (embed)0.25137\n",
            "Epoch 4 (train)4.4s: train loss = 0.99365 = (mf)0.64743 + (embed)0.34623\n",
            "Epoch 5 (train)4.1s + (eval)0.7s: train loss = 1.13097 = (mf)0.69744 + (embed)0.43353, recall = 0.09996, ndcg = 0.16997\n",
            "Epoch 6 (train)4.9s: train loss = 1.31601 = (mf)0.80387 + (embed)0.51213\n",
            "Epoch 7 (train)4.1s: train loss = 1.52752 = (mf)0.93910 + (embed)0.58842\n",
            "Epoch 8 (train)4.1s: train loss = 1.58770 = (mf)0.92986 + (embed)0.65784\n",
            "Epoch 9 (train)4.9s: train loss = 1.63802 = (mf)0.91795 + (embed)0.72007\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 1.69232 = (mf)0.91655 + (embed)0.77577, recall = 0.11610, ndcg = 0.18238\n",
            "Epoch 11 (train)4.1s: train loss = 1.82455 = (mf)1.00702 + (embed)0.81753\n",
            "Epoch 12 (train)4.9s: train loss = 1.80020 = (mf)0.94607 + (embed)0.85413\n",
            "Epoch 13 (train)4.1s: train loss = 1.94451 = (mf)1.05702 + (embed)0.88749\n",
            "Epoch 14 (train)4.5s: train loss = 1.91997 = (mf)0.98931 + (embed)0.93066\n",
            "Epoch 15 (train)4.6s + (eval)0.2s: train loss = 2.03098 = (mf)1.06856 + (embed)0.96242, recall = 0.10067, ndcg = 0.15556\n",
            "Epoch 16 (train)4.1s: train loss = 2.16866 = (mf)1.17806 + (embed)0.99060\n",
            "Epoch 17 (train)4.8s: train loss = 2.02302 = (mf)1.00124 + (embed)1.02178\n",
            "Epoch 18 (train)4.2s: train loss = 2.22962 = (mf)1.17677 + (embed)1.05285\n",
            "Epoch 19 (train)4.1s: train loss = 2.21365 = (mf)1.12678 + (embed)1.08687\n",
            "Epoch 20 (train)4.9s + (eval)0.6s: train loss = 2.17913 = (mf)1.06437 + (embed)1.11476, recall = 0.11278, ndcg = 0.16974\n",
            "Epoch 21 (train)4.2s: train loss = 2.35300 = (mf)1.21700 + (embed)1.13601\n",
            "Epoch 22 (train)4.1s: train loss = 2.24650 = (mf)1.08750 + (embed)1.15900\n",
            "Epoch 23 (train)4.9s: train loss = 2.27206 = (mf)1.09614 + (embed)1.17593\n",
            "Epoch 24 (train)4.1s: train loss = 2.29693 = (mf)1.10776 + (embed)1.18917\n",
            "Epoch 25 (train)4.0s + (eval)0.2s: train loss = 2.31669 = (mf)1.11458 + (embed)1.20211, recall = 0.11853, ndcg = 0.17510\n",
            "Epoch 26 (train)4.9s: train loss = 2.33391 = (mf)1.12545 + (embed)1.20846\n",
            "Epoch 27 (train)4.1s: train loss = 2.25251 = (mf)1.03196 + (embed)1.22056\n",
            "Epoch 28 (train)4.3s: train loss = 2.36552 = (mf)1.13092 + (embed)1.23461\n",
            "Epoch 29 (train)4.7s: train loss = 2.33270 = (mf)1.08238 + (embed)1.25032\n",
            "Epoch 30 (train)4.0s + (eval)0.2s: train loss = 2.29531 = (mf)1.03886 + (embed)1.25645, recall = 0.11145, ndcg = 0.16858\n",
            "Epoch 31 (train)4.6s: train loss = 2.32434 = (mf)1.06071 + (embed)1.26364\n",
            "Epoch 32 (train)4.4s: train loss = 2.39238 = (mf)1.12146 + (embed)1.27091\n",
            "Epoch 33 (train)4.1s: train loss = 2.33818 = (mf)1.06156 + (embed)1.27662\n",
            "Epoch 34 (train)4.9s: train loss = 2.37036 = (mf)1.08468 + (embed)1.28568\n",
            "Epoch 35 (train)4.1s + (eval)0.6s: train loss = 2.34467 = (mf)1.05138 + (embed)1.29329, recall = 0.11413, ndcg = 0.17576\n",
            "Epoch 36 (train)4.1s: train loss = 2.41593 = (mf)1.11707 + (embed)1.29886\n",
            "Epoch 37 (train)4.9s: train loss = 2.47980 = (mf)1.16221 + (embed)1.31759\n",
            "Epoch 38 (train)4.1s: train loss = 2.45389 = (mf)1.12399 + (embed)1.32990\n",
            "Epoch 39 (train)4.2s: train loss = 2.25160 = (mf)0.91080 + (embed)1.34080\n",
            "Epoch 40 (train)5.0s + (eval)0.2s: train loss = 2.46639 = (mf)1.11788 + (embed)1.34851, recall = 0.09860, ndcg = 0.15722\n",
            "Epoch 41 (train)4.1s: train loss = 2.41360 = (mf)1.05667 + (embed)1.35693\n",
            "Epoch 42 (train)4.0s: train loss = 2.35752 = (mf)0.99105 + (embed)1.36647\n",
            "Epoch 43 (train)4.8s: train loss = 2.42528 = (mf)1.05237 + (embed)1.37291\n",
            "Epoch 44 (train)4.0s: train loss = 2.33519 = (mf)0.95713 + (embed)1.37806\n",
            "Epoch 45 (train)4.1s + (eval)0.7s: train loss = 2.45501 = (mf)1.07114 + (embed)1.38387, recall = 0.10994, ndcg = 0.16535\n",
            "Epoch 46 (train)4.5s: train loss = 2.34144 = (mf)0.95725 + (embed)1.38419\n",
            "Epoch 47 (train)4.1s: train loss = 2.35571 = (mf)0.97086 + (embed)1.38485\n",
            "Epoch 48 (train)4.7s: train loss = 2.37953 = (mf)0.98749 + (embed)1.39204\n",
            "Epoch 49 (train)4.4s: train loss = 2.44507 = (mf)1.04507 + (embed)1.40000\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 2.33416 = (mf)0.94109 + (embed)1.39307, recall = 0.11288, ndcg = 0.16239\n",
            "Training finished in 224.07 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.16239, Recall@20: 0.11288\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.9s: train loss = 0.42006 = (mf)0.41983 + (embed)0.00024\n",
            "Epoch 2 (train)2.9s: train loss = 0.21627 = (mf)0.21562 + (embed)0.00065\n",
            "Epoch 3 (train)3.8s: train loss = 0.20178 = (mf)0.20102 + (embed)0.00075\n",
            "Epoch 4 (train)3.0s: train loss = 0.18376 = (mf)0.18288 + (embed)0.00087\n",
            "Epoch 5 (train)2.9s + (eval)0.7s: train loss = 0.16810 = (mf)0.16709 + (embed)0.00101, recall = 0.23927, ndcg = 0.23851\n",
            "Epoch 6 (train)3.0s: train loss = 0.15624 = (mf)0.15510 + (embed)0.00114\n",
            "Epoch 7 (train)3.8s: train loss = 0.14611 = (mf)0.14484 + (embed)0.00127\n",
            "Epoch 8 (train)3.0s: train loss = 0.13760 = (mf)0.13617 + (embed)0.00142\n",
            "Epoch 9 (train)3.0s: train loss = 0.13201 = (mf)0.13045 + (embed)0.00156\n",
            "Epoch 10 (train)3.1s + (eval)0.4s: train loss = 0.12605 = (mf)0.12435 + (embed)0.00170, recall = 0.26857, ndcg = 0.26677\n",
            "Epoch 11 (train)3.7s: train loss = 0.12259 = (mf)0.12073 + (embed)0.00186\n",
            "Epoch 12 (train)3.0s: train loss = 0.11489 = (mf)0.11288 + (embed)0.00202\n",
            "Epoch 13 (train)3.1s: train loss = 0.10585 = (mf)0.10367 + (embed)0.00218\n",
            "Epoch 14 (train)3.4s: train loss = 0.09850 = (mf)0.09614 + (embed)0.00236\n",
            "Epoch 15 (train)3.5s + (eval)0.6s: train loss = 0.09972 = (mf)0.09721 + (embed)0.00251, recall = 0.29248, ndcg = 0.28857\n",
            "Epoch 16 (train)3.0s: train loss = 0.09169 = (mf)0.08901 + (embed)0.00268\n",
            "Epoch 17 (train)3.0s: train loss = 0.08863 = (mf)0.08578 + (embed)0.00285\n",
            "Epoch 18 (train)3.8s: train loss = 0.08321 = (mf)0.08020 + (embed)0.00301\n",
            "Epoch 19 (train)3.0s: train loss = 0.08082 = (mf)0.07764 + (embed)0.00318\n",
            "Epoch 20 (train)3.0s + (eval)0.3s: train loss = 0.07688 = (mf)0.07353 + (embed)0.00334, recall = 0.31135, ndcg = 0.30751\n",
            "Epoch 21 (train)3.0s: train loss = 0.07179 = (mf)0.06830 + (embed)0.00349\n",
            "Epoch 22 (train)4.0s: train loss = 0.06927 = (mf)0.06562 + (embed)0.00365\n",
            "Epoch 23 (train)3.0s: train loss = 0.06934 = (mf)0.06553 + (embed)0.00381\n",
            "Epoch 24 (train)3.0s: train loss = 0.06421 = (mf)0.06025 + (embed)0.00396\n",
            "Epoch 25 (train)3.0s + (eval)0.3s: train loss = 0.06358 = (mf)0.05947 + (embed)0.00411, recall = 0.32321, ndcg = 0.32182\n",
            "Epoch 26 (train)3.8s: train loss = 0.06213 = (mf)0.05789 + (embed)0.00425\n",
            "Epoch 27 (train)2.9s: train loss = 0.06066 = (mf)0.05627 + (embed)0.00439\n",
            "Epoch 28 (train)3.0s: train loss = 0.05877 = (mf)0.05425 + (embed)0.00452\n",
            "Epoch 29 (train)3.0s: train loss = 0.05627 = (mf)0.05160 + (embed)0.00466\n",
            "Epoch 30 (train)3.8s + (eval)0.6s: train loss = 0.05469 = (mf)0.04992 + (embed)0.00477, recall = 0.32986, ndcg = 0.33223\n",
            "Epoch 31 (train)3.0s: train loss = 0.05324 = (mf)0.04833 + (embed)0.00491\n",
            "Epoch 32 (train)2.9s: train loss = 0.05177 = (mf)0.04672 + (embed)0.00505\n",
            "Epoch 33 (train)3.1s: train loss = 0.05128 = (mf)0.04613 + (embed)0.00515\n",
            "Epoch 34 (train)3.5s: train loss = 0.05034 = (mf)0.04508 + (embed)0.00527\n",
            "Epoch 35 (train)2.9s + (eval)0.3s: train loss = 0.04840 = (mf)0.04301 + (embed)0.00539, recall = 0.33665, ndcg = 0.33748\n",
            "Epoch 36 (train)2.9s: train loss = 0.04623 = (mf)0.04073 + (embed)0.00550\n",
            "Epoch 37 (train)3.2s: train loss = 0.04627 = (mf)0.04068 + (embed)0.00559\n",
            "Epoch 38 (train)3.5s: train loss = 0.04446 = (mf)0.03875 + (embed)0.00571\n",
            "Epoch 39 (train)2.9s: train loss = 0.04374 = (mf)0.03794 + (embed)0.00580\n",
            "Epoch 40 (train)3.0s + (eval)0.3s: train loss = 0.04259 = (mf)0.03670 + (embed)0.00589, recall = 0.34264, ndcg = 0.34284\n",
            "Epoch 41 (train)3.5s: train loss = 0.04216 = (mf)0.03616 + (embed)0.00600\n",
            "Epoch 42 (train)3.4s: train loss = 0.04200 = (mf)0.03590 + (embed)0.00609\n",
            "Epoch 43 (train)3.0s: train loss = 0.04034 = (mf)0.03415 + (embed)0.00619\n",
            "Epoch 44 (train)3.0s: train loss = 0.04090 = (mf)0.03464 + (embed)0.00626\n",
            "Epoch 45 (train)3.5s + (eval)0.8s: train loss = 0.04038 = (mf)0.03405 + (embed)0.00634, recall = 0.34813, ndcg = 0.34813\n",
            "Epoch 46 (train)2.9s: train loss = 0.03831 = (mf)0.03188 + (embed)0.00644\n",
            "Epoch 47 (train)2.9s: train loss = 0.03784 = (mf)0.03133 + (embed)0.00652\n",
            "Epoch 48 (train)2.9s: train loss = 0.03784 = (mf)0.03124 + (embed)0.00660\n",
            "Epoch 49 (train)3.8s: train loss = 0.03724 = (mf)0.03057 + (embed)0.00667\n",
            "Epoch 50 (train)2.9s + (eval)0.3s: train loss = 0.03658 = (mf)0.02983 + (embed)0.00676, recall = 0.35275, ndcg = 0.34917\n",
            "Training finished in 164.87 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.34917, Recall@50: 0.35275\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.1s: train loss = 0.41733 = (mf)0.41709 + (embed)0.00025\n",
            "Epoch 2 (train)3.5s: train loss = 0.22665 = (mf)0.22601 + (embed)0.00064\n",
            "Epoch 3 (train)3.5s: train loss = 0.21715 = (mf)0.21641 + (embed)0.00074\n",
            "Epoch 4 (train)4.4s: train loss = 0.20574 = (mf)0.20488 + (embed)0.00086\n",
            "Epoch 5 (train)3.5s + (eval)1.1s: train loss = 0.19811 = (mf)0.19711 + (embed)0.00099, recall = 0.21000, ndcg = 0.21024\n",
            "Epoch 6 (train)3.6s: train loss = 0.18016 = (mf)0.17899 + (embed)0.00116\n",
            "Epoch 7 (train)4.4s: train loss = 0.16777 = (mf)0.16642 + (embed)0.00135\n",
            "Epoch 8 (train)3.5s: train loss = 0.15419 = (mf)0.15264 + (embed)0.00155\n",
            "Epoch 9 (train)3.5s: train loss = 0.14392 = (mf)0.14216 + (embed)0.00176\n",
            "Epoch 10 (train)3.9s + (eval)0.4s: train loss = 0.13732 = (mf)0.13538 + (embed)0.00194, recall = 0.25348, ndcg = 0.25272\n",
            "Epoch 11 (train)3.9s: train loss = 0.13585 = (mf)0.13375 + (embed)0.00210\n",
            "Epoch 12 (train)3.5s: train loss = 0.12926 = (mf)0.12702 + (embed)0.00224\n",
            "Epoch 13 (train)3.5s: train loss = 0.12382 = (mf)0.12141 + (embed)0.00241\n",
            "Epoch 14 (train)4.3s: train loss = 0.12167 = (mf)0.11912 + (embed)0.00256\n",
            "Epoch 15 (train)3.5s + (eval)0.6s: train loss = 0.11473 = (mf)0.11203 + (embed)0.00270, recall = 0.26791, ndcg = 0.26987\n",
            "Epoch 16 (train)3.6s: train loss = 0.11098 = (mf)0.10812 + (embed)0.00287\n",
            "Epoch 17 (train)4.6s: train loss = 0.10782 = (mf)0.10477 + (embed)0.00305\n",
            "Epoch 18 (train)3.7s: train loss = 0.10339 = (mf)0.10017 + (embed)0.00322\n",
            "Epoch 19 (train)3.7s: train loss = 0.09955 = (mf)0.09614 + (embed)0.00340\n",
            "Epoch 20 (train)4.4s + (eval)0.3s: train loss = 0.09370 = (mf)0.09011 + (embed)0.00360, recall = 0.28877, ndcg = 0.28738\n",
            "Epoch 21 (train)3.6s: train loss = 0.09089 = (mf)0.08712 + (embed)0.00377\n",
            "Epoch 22 (train)3.6s: train loss = 0.08867 = (mf)0.08471 + (embed)0.00396\n",
            "Epoch 23 (train)4.2s: train loss = 0.08628 = (mf)0.08213 + (embed)0.00415\n",
            "Epoch 24 (train)3.7s: train loss = 0.08227 = (mf)0.07794 + (embed)0.00433\n",
            "Epoch 25 (train)3.6s + (eval)0.2s: train loss = 0.08010 = (mf)0.07558 + (embed)0.00451, recall = 0.30556, ndcg = 0.30444\n",
            "Epoch 26 (train)3.9s: train loss = 0.07780 = (mf)0.07312 + (embed)0.00468\n",
            "Epoch 27 (train)4.1s: train loss = 0.07709 = (mf)0.07226 + (embed)0.00483\n",
            "Epoch 28 (train)3.6s: train loss = 0.07347 = (mf)0.06846 + (embed)0.00501\n",
            "Epoch 29 (train)3.6s: train loss = 0.07041 = (mf)0.06524 + (embed)0.00517\n",
            "Epoch 30 (train)4.4s + (eval)0.6s: train loss = 0.06904 = (mf)0.06370 + (embed)0.00534, recall = 0.32204, ndcg = 0.32049\n",
            "Epoch 31 (train)3.6s: train loss = 0.06725 = (mf)0.06176 + (embed)0.00548\n",
            "Epoch 32 (train)3.5s: train loss = 0.06548 = (mf)0.05984 + (embed)0.00563\n",
            "Epoch 33 (train)4.4s: train loss = 0.06568 = (mf)0.05991 + (embed)0.00577\n",
            "Epoch 34 (train)3.5s: train loss = 0.06254 = (mf)0.05662 + (embed)0.00592\n",
            "Epoch 35 (train)3.5s + (eval)0.2s: train loss = 0.06273 = (mf)0.05666 + (embed)0.00607, recall = 0.32860, ndcg = 0.32860\n",
            "Epoch 36 (train)4.4s: train loss = 0.06110 = (mf)0.05492 + (embed)0.00618\n",
            "Epoch 37 (train)3.5s: train loss = 0.05773 = (mf)0.05143 + (embed)0.00631\n",
            "Epoch 38 (train)3.4s: train loss = 0.05734 = (mf)0.05087 + (embed)0.00646\n",
            "Epoch 39 (train)3.7s: train loss = 0.05797 = (mf)0.05139 + (embed)0.00658\n",
            "Epoch 40 (train)4.1s + (eval)0.2s: train loss = 0.05716 = (mf)0.05047 + (embed)0.00670, recall = 0.33767, ndcg = 0.33955\n",
            "Epoch 41 (train)3.4s: train loss = 0.05532 = (mf)0.04851 + (embed)0.00681\n",
            "Epoch 42 (train)3.5s: train loss = 0.05392 = (mf)0.04701 + (embed)0.00692\n",
            "Epoch 43 (train)4.3s: train loss = 0.05331 = (mf)0.04628 + (embed)0.00703\n",
            "Epoch 44 (train)3.5s: train loss = 0.05150 = (mf)0.04435 + (embed)0.00715\n",
            "Epoch 45 (train)3.5s + (eval)0.3s: train loss = 0.05163 = (mf)0.04440 + (embed)0.00724, recall = 0.34066, ndcg = 0.34163\n",
            "Epoch 46 (train)4.3s: train loss = 0.05065 = (mf)0.04330 + (embed)0.00736\n",
            "Epoch 47 (train)3.5s: train loss = 0.04804 = (mf)0.04060 + (embed)0.00744\n",
            "Epoch 48 (train)3.5s: train loss = 0.04990 = (mf)0.04236 + (embed)0.00754\n",
            "Epoch 49 (train)3.6s: train loss = 0.04817 = (mf)0.04053 + (embed)0.00763\n",
            "Epoch 50 (train)4.1s + (eval)0.6s: train loss = 0.04707 = (mf)0.03935 + (embed)0.00772, recall = 0.34589, ndcg = 0.34956\n",
            "Training finished in 195.26 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.34956, Recall@50: 0.34589\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.8s: train loss = 0.41325 = (mf)0.41300 + (embed)0.00025\n",
            "Epoch 2 (train)4.1s: train loss = 0.24178 = (mf)0.24115 + (embed)0.00063\n",
            "Epoch 3 (train)4.1s: train loss = 0.22501 = (mf)0.22426 + (embed)0.00075\n",
            "Epoch 4 (train)4.9s: train loss = 0.21908 = (mf)0.21820 + (embed)0.00088\n",
            "Epoch 5 (train)4.1s + (eval)1.1s: train loss = 0.20387 = (mf)0.20281 + (embed)0.00107, recall = 0.20971, ndcg = 0.21167\n",
            "Epoch 6 (train)4.8s: train loss = 0.19017 = (mf)0.18892 + (embed)0.00126\n",
            "Epoch 7 (train)4.3s: train loss = 0.17734 = (mf)0.17587 + (embed)0.00148\n",
            "Epoch 8 (train)4.4s: train loss = 0.16851 = (mf)0.16682 + (embed)0.00170\n",
            "Epoch 9 (train)5.0s: train loss = 0.15746 = (mf)0.15555 + (embed)0.00190\n",
            "Epoch 10 (train)4.1s + (eval)0.3s: train loss = 0.14980 = (mf)0.14768 + (embed)0.00211, recall = 0.25406, ndcg = 0.25602\n",
            "Epoch 11 (train)4.2s: train loss = 0.14561 = (mf)0.14329 + (embed)0.00232\n",
            "Epoch 12 (train)5.1s: train loss = 0.14274 = (mf)0.14024 + (embed)0.00250\n",
            "Epoch 13 (train)4.2s: train loss = 0.13899 = (mf)0.13631 + (embed)0.00268\n",
            "Epoch 14 (train)4.3s: train loss = 0.13231 = (mf)0.12944 + (embed)0.00286\n",
            "Epoch 15 (train)4.8s + (eval)0.3s: train loss = 0.13124 = (mf)0.12820 + (embed)0.00304, recall = 0.26643, ndcg = 0.26514\n",
            "Epoch 16 (train)4.1s: train loss = 0.13049 = (mf)0.12729 + (embed)0.00320\n",
            "Epoch 17 (train)4.8s: train loss = 0.12087 = (mf)0.11748 + (embed)0.00340\n",
            "Epoch 18 (train)4.5s: train loss = 0.12151 = (mf)0.11794 + (embed)0.00357\n",
            "Epoch 19 (train)4.3s: train loss = 0.11686 = (mf)0.11310 + (embed)0.00376\n",
            "Epoch 20 (train)5.1s + (eval)0.6s: train loss = 0.11628 = (mf)0.11238 + (embed)0.00391, recall = 0.28031, ndcg = 0.27720\n",
            "Epoch 21 (train)4.1s: train loss = 0.11106 = (mf)0.10696 + (embed)0.00410\n",
            "Epoch 22 (train)4.1s: train loss = 0.10478 = (mf)0.10046 + (embed)0.00432\n",
            "Epoch 23 (train)4.9s: train loss = 0.10296 = (mf)0.09845 + (embed)0.00451\n",
            "Epoch 24 (train)4.1s: train loss = 0.10152 = (mf)0.09681 + (embed)0.00471\n",
            "Epoch 25 (train)4.1s + (eval)0.3s: train loss = 0.09903 = (mf)0.09413 + (embed)0.00490, recall = 0.29168, ndcg = 0.28828\n",
            "Epoch 26 (train)4.9s: train loss = 0.09520 = (mf)0.09012 + (embed)0.00508\n",
            "Epoch 27 (train)4.2s: train loss = 0.09357 = (mf)0.08829 + (embed)0.00528\n",
            "Epoch 28 (train)4.4s: train loss = 0.09026 = (mf)0.08477 + (embed)0.00549\n",
            "Epoch 29 (train)4.6s: train loss = 0.08740 = (mf)0.08170 + (embed)0.00570\n",
            "Epoch 30 (train)4.1s + (eval)0.3s: train loss = 0.08589 = (mf)0.08004 + (embed)0.00586, recall = 0.30614, ndcg = 0.30193\n",
            "Epoch 31 (train)4.8s: train loss = 0.08477 = (mf)0.07874 + (embed)0.00603\n",
            "Epoch 32 (train)4.2s: train loss = 0.08339 = (mf)0.07716 + (embed)0.00623\n",
            "Epoch 33 (train)4.1s: train loss = 0.08049 = (mf)0.07407 + (embed)0.00642\n",
            "Epoch 34 (train)4.9s: train loss = 0.07769 = (mf)0.07109 + (embed)0.00660\n",
            "Epoch 35 (train)4.1s + (eval)0.6s: train loss = 0.07766 = (mf)0.07088 + (embed)0.00678, recall = 0.31466, ndcg = 0.31501\n",
            "Epoch 36 (train)4.0s: train loss = 0.07540 = (mf)0.06843 + (embed)0.00697\n",
            "Epoch 37 (train)4.9s: train loss = 0.07397 = (mf)0.06685 + (embed)0.00711\n",
            "Epoch 38 (train)4.1s: train loss = 0.07516 = (mf)0.06788 + (embed)0.00728\n",
            "Epoch 39 (train)4.0s: train loss = 0.07172 = (mf)0.06431 + (embed)0.00740\n",
            "Epoch 40 (train)5.0s + (eval)0.3s: train loss = 0.07005 = (mf)0.06249 + (embed)0.00756, recall = 0.32569, ndcg = 0.32678\n",
            "Epoch 41 (train)4.1s: train loss = 0.06872 = (mf)0.06102 + (embed)0.00770\n",
            "Epoch 42 (train)4.2s: train loss = 0.06529 = (mf)0.05742 + (embed)0.00786\n",
            "Epoch 43 (train)4.9s: train loss = 0.06667 = (mf)0.05865 + (embed)0.00802\n",
            "Epoch 44 (train)4.1s: train loss = 0.06389 = (mf)0.05570 + (embed)0.00819\n",
            "Epoch 45 (train)4.3s + (eval)0.4s: train loss = 0.06396 = (mf)0.05565 + (embed)0.00832, recall = 0.33013, ndcg = 0.33400\n",
            "Epoch 46 (train)4.4s: train loss = 0.06315 = (mf)0.05469 + (embed)0.00847\n",
            "Epoch 47 (train)4.1s: train loss = 0.06242 = (mf)0.05383 + (embed)0.00859\n",
            "Epoch 48 (train)4.6s: train loss = 0.06196 = (mf)0.05324 + (embed)0.00872\n",
            "Epoch 49 (train)4.3s: train loss = 0.06018 = (mf)0.05136 + (embed)0.00881\n",
            "Epoch 50 (train)4.1s + (eval)0.6s: train loss = 0.05935 = (mf)0.05042 + (embed)0.00893, recall = 0.33589, ndcg = 0.34061\n",
            "Training finished in 226.10 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.34061, Recall@50: 0.33589\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.0s: train loss = 0.21946 = (mf)0.21782 + (embed)0.00164\n",
            "Epoch 2 (train)3.0s: train loss = 0.10874 = (mf)0.10517 + (embed)0.00356\n",
            "Epoch 3 (train)4.0s: train loss = 0.07891 = (mf)0.07358 + (embed)0.00533\n",
            "Epoch 4 (train)3.2s: train loss = 0.06261 = (mf)0.05566 + (embed)0.00695\n",
            "Epoch 5 (train)3.0s + (eval)1.1s: train loss = 0.05737 = (mf)0.04907 + (embed)0.00830, recall = 0.29712, ndcg = 0.28043\n",
            "Epoch 6 (train)3.5s: train loss = 0.05207 = (mf)0.04261 + (embed)0.00946\n",
            "Epoch 7 (train)3.3s: train loss = 0.05024 = (mf)0.03976 + (embed)0.01048\n",
            "Epoch 8 (train)3.0s: train loss = 0.04707 = (mf)0.03568 + (embed)0.01139\n",
            "Epoch 9 (train)3.0s: train loss = 0.04641 = (mf)0.03417 + (embed)0.01224\n",
            "Epoch 10 (train)3.7s + (eval)0.4s: train loss = 0.04727 = (mf)0.03429 + (embed)0.01297, recall = 0.28951, ndcg = 0.27592\n",
            "Epoch 11 (train)3.0s: train loss = 0.04476 = (mf)0.03101 + (embed)0.01375\n",
            "Epoch 12 (train)3.0s: train loss = 0.04569 = (mf)0.03128 + (embed)0.01441\n",
            "Epoch 13 (train)3.0s: train loss = 0.04475 = (mf)0.02976 + (embed)0.01499\n",
            "Epoch 14 (train)3.8s: train loss = 0.04468 = (mf)0.02915 + (embed)0.01553\n",
            "Epoch 15 (train)3.0s + (eval)0.3s: train loss = 0.04715 = (mf)0.03111 + (embed)0.01605, recall = 0.28092, ndcg = 0.26637\n",
            "Epoch 16 (train)3.0s: train loss = 0.04645 = (mf)0.02986 + (embed)0.01660\n",
            "Epoch 17 (train)3.0s: train loss = 0.04554 = (mf)0.02846 + (embed)0.01708\n",
            "Epoch 18 (train)3.9s: train loss = 0.04445 = (mf)0.02693 + (embed)0.01751\n",
            "Epoch 19 (train)3.0s: train loss = 0.04692 = (mf)0.02900 + (embed)0.01792\n",
            "Epoch 20 (train)3.0s + (eval)0.6s: train loss = 0.04702 = (mf)0.02873 + (embed)0.01829, recall = 0.26984, ndcg = 0.25740\n",
            "Epoch 21 (train)3.2s: train loss = 0.04597 = (mf)0.02729 + (embed)0.01868\n",
            "Epoch 22 (train)3.6s: train loss = 0.04675 = (mf)0.02774 + (embed)0.01901\n",
            "Epoch 23 (train)3.0s: train loss = 0.04785 = (mf)0.02847 + (embed)0.01938\n",
            "Epoch 24 (train)2.9s: train loss = 0.04819 = (mf)0.02845 + (embed)0.01974\n",
            "Epoch 25 (train)3.2s + (eval)0.4s: train loss = 0.04941 = (mf)0.02932 + (embed)0.02010, recall = 0.26729, ndcg = 0.25267\n",
            "Epoch 26 (train)3.5s: train loss = 0.04648 = (mf)0.02598 + (embed)0.02050\n",
            "Epoch 27 (train)2.9s: train loss = 0.04935 = (mf)0.02848 + (embed)0.02087\n",
            "Epoch 28 (train)2.9s: train loss = 0.04945 = (mf)0.02829 + (embed)0.02116\n",
            "Epoch 29 (train)3.2s: train loss = 0.04922 = (mf)0.02777 + (embed)0.02145\n",
            "Epoch 30 (train)3.5s + (eval)0.2s: train loss = 0.04791 = (mf)0.02616 + (embed)0.02175, recall = 0.25593, ndcg = 0.24195\n",
            "Epoch 31 (train)2.9s: train loss = 0.04794 = (mf)0.02600 + (embed)0.02195\n",
            "Epoch 32 (train)2.9s: train loss = 0.04978 = (mf)0.02760 + (embed)0.02218\n",
            "Epoch 33 (train)3.3s: train loss = 0.04987 = (mf)0.02738 + (embed)0.02250\n",
            "Epoch 34 (train)3.4s: train loss = 0.04974 = (mf)0.02700 + (embed)0.02274\n",
            "Epoch 35 (train)2.9s + (eval)0.6s: train loss = 0.04950 = (mf)0.02653 + (embed)0.02297, recall = 0.27052, ndcg = 0.25111\n",
            "Epoch 36 (train)3.0s: train loss = 0.05180 = (mf)0.02851 + (embed)0.02329\n",
            "Epoch 37 (train)3.8s: train loss = 0.05276 = (mf)0.02928 + (embed)0.02348\n",
            "Epoch 38 (train)3.1s: train loss = 0.04987 = (mf)0.02619 + (embed)0.02368\n",
            "Epoch 39 (train)3.0s: train loss = 0.05174 = (mf)0.02784 + (embed)0.02390\n",
            "Epoch 40 (train)3.0s + (eval)0.3s: train loss = 0.05182 = (mf)0.02767 + (embed)0.02415, recall = 0.25520, ndcg = 0.23930\n",
            "Epoch 41 (train)3.8s: train loss = 0.05194 = (mf)0.02751 + (embed)0.02443\n",
            "Epoch 42 (train)2.9s: train loss = 0.05055 = (mf)0.02605 + (embed)0.02449\n",
            "Epoch 43 (train)3.0s: train loss = 0.05340 = (mf)0.02878 + (embed)0.02462\n",
            "Epoch 44 (train)2.9s: train loss = 0.04975 = (mf)0.02501 + (embed)0.02474\n",
            "Epoch 45 (train)3.7s + (eval)0.3s: train loss = 0.05072 = (mf)0.02580 + (embed)0.02492, recall = 0.26326, ndcg = 0.24074\n",
            "Epoch 46 (train)2.9s: train loss = 0.04932 = (mf)0.02429 + (embed)0.02504\n",
            "Epoch 47 (train)2.9s: train loss = 0.05367 = (mf)0.02851 + (embed)0.02516\n",
            "Epoch 48 (train)3.0s: train loss = 0.05019 = (mf)0.02484 + (embed)0.02535\n",
            "Epoch 49 (train)3.8s: train loss = 0.05046 = (mf)0.02499 + (embed)0.02547\n",
            "Epoch 50 (train)3.0s + (eval)0.6s: train loss = 0.05097 = (mf)0.02547 + (embed)0.02550, recall = 0.25733, ndcg = 0.23604\n",
            "Training finished in 165.18 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.23604, Recall@50: 0.25733\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.2s: train loss = 0.23019 = (mf)0.22863 + (embed)0.00156\n",
            "Epoch 2 (train)3.6s: train loss = 0.12253 = (mf)0.11893 + (embed)0.00360\n",
            "Epoch 3 (train)3.6s: train loss = 0.09320 = (mf)0.08772 + (embed)0.00548\n",
            "Epoch 4 (train)4.4s: train loss = 0.07365 = (mf)0.06638 + (embed)0.00727\n",
            "Epoch 5 (train)3.6s + (eval)1.1s: train loss = 0.06546 = (mf)0.05659 + (embed)0.00887, recall = 0.30411, ndcg = 0.29208\n",
            "Epoch 6 (train)3.6s: train loss = 0.05794 = (mf)0.04773 + (embed)0.01021\n",
            "Epoch 7 (train)4.4s: train loss = 0.05284 = (mf)0.04150 + (embed)0.01134\n",
            "Epoch 8 (train)3.5s: train loss = 0.04826 = (mf)0.03593 + (embed)0.01232\n",
            "Epoch 9 (train)3.5s: train loss = 0.04843 = (mf)0.03522 + (embed)0.01321\n",
            "Epoch 10 (train)4.3s + (eval)0.3s: train loss = 0.04622 = (mf)0.03230 + (embed)0.01392, recall = 0.30946, ndcg = 0.29688\n",
            "Epoch 11 (train)3.6s: train loss = 0.04399 = (mf)0.02942 + (embed)0.01456\n",
            "Epoch 12 (train)3.5s: train loss = 0.04338 = (mf)0.02821 + (embed)0.01517\n",
            "Epoch 13 (train)4.3s: train loss = 0.04392 = (mf)0.02822 + (embed)0.01570\n",
            "Epoch 14 (train)3.7s: train loss = 0.04383 = (mf)0.02760 + (embed)0.01623\n",
            "Epoch 15 (train)3.6s + (eval)0.3s: train loss = 0.04350 = (mf)0.02679 + (embed)0.01671, recall = 0.30153, ndcg = 0.28673\n",
            "Epoch 16 (train)3.9s: train loss = 0.04352 = (mf)0.02648 + (embed)0.01703\n",
            "Epoch 17 (train)4.0s: train loss = 0.04288 = (mf)0.02538 + (embed)0.01750\n",
            "Epoch 18 (train)3.5s: train loss = 0.04114 = (mf)0.02334 + (embed)0.01779\n",
            "Epoch 19 (train)3.5s: train loss = 0.04122 = (mf)0.02317 + (embed)0.01805\n",
            "Epoch 20 (train)4.3s + (eval)0.6s: train loss = 0.04085 = (mf)0.02254 + (embed)0.01830, recall = 0.30413, ndcg = 0.29000\n",
            "Epoch 21 (train)3.5s: train loss = 0.04284 = (mf)0.02423 + (embed)0.01861\n",
            "Epoch 22 (train)3.5s: train loss = 0.04080 = (mf)0.02192 + (embed)0.01888\n",
            "Epoch 23 (train)4.3s: train loss = 0.04339 = (mf)0.02420 + (embed)0.01918\n",
            "Epoch 24 (train)3.5s: train loss = 0.04123 = (mf)0.02183 + (embed)0.01940\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 0.04164 = (mf)0.02208 + (embed)0.01957, recall = 0.30077, ndcg = 0.28291\n",
            "Epoch 26 (train)4.1s: train loss = 0.03950 = (mf)0.01972 + (embed)0.01977\n",
            "Epoch 27 (train)3.8s: train loss = 0.04066 = (mf)0.02066 + (embed)0.02000\n",
            "Epoch 28 (train)3.5s: train loss = 0.04042 = (mf)0.02028 + (embed)0.02014\n",
            "Epoch 29 (train)3.6s: train loss = 0.04127 = (mf)0.02091 + (embed)0.02036\n",
            "Epoch 30 (train)4.3s + (eval)0.2s: train loss = 0.04064 = (mf)0.02006 + (embed)0.02059, recall = 0.30402, ndcg = 0.28524\n",
            "Epoch 31 (train)3.5s: train loss = 0.04208 = (mf)0.02131 + (embed)0.02076\n",
            "Epoch 32 (train)3.5s: train loss = 0.04173 = (mf)0.02072 + (embed)0.02101\n",
            "Epoch 33 (train)4.3s: train loss = 0.04113 = (mf)0.01991 + (embed)0.02121\n",
            "Epoch 34 (train)3.5s: train loss = 0.04172 = (mf)0.02027 + (embed)0.02145\n",
            "Epoch 35 (train)3.5s + (eval)0.6s: train loss = 0.04169 = (mf)0.02015 + (embed)0.02153, recall = 0.30268, ndcg = 0.28393\n",
            "Epoch 36 (train)4.4s: train loss = 0.04090 = (mf)0.01916 + (embed)0.02174\n",
            "Epoch 37 (train)3.6s: train loss = 0.04055 = (mf)0.01863 + (embed)0.02193\n",
            "Epoch 38 (train)3.5s: train loss = 0.04233 = (mf)0.02028 + (embed)0.02204\n",
            "Epoch 39 (train)4.1s: train loss = 0.04044 = (mf)0.01824 + (embed)0.02219\n",
            "Epoch 40 (train)3.9s + (eval)0.3s: train loss = 0.04363 = (mf)0.02133 + (embed)0.02230, recall = 0.30564, ndcg = 0.28826\n",
            "Epoch 41 (train)3.6s: train loss = 0.04191 = (mf)0.01946 + (embed)0.02245\n",
            "Epoch 42 (train)3.8s: train loss = 0.04263 = (mf)0.01999 + (embed)0.02264\n",
            "Epoch 43 (train)4.3s: train loss = 0.04185 = (mf)0.01906 + (embed)0.02279\n",
            "Epoch 44 (train)3.6s: train loss = 0.03999 = (mf)0.01721 + (embed)0.02278\n",
            "Epoch 45 (train)3.5s + (eval)0.3s: train loss = 0.04321 = (mf)0.02038 + (embed)0.02282, recall = 0.29943, ndcg = 0.28099\n",
            "Epoch 46 (train)4.4s: train loss = 0.04117 = (mf)0.01828 + (embed)0.02290\n",
            "Epoch 47 (train)3.5s: train loss = 0.04106 = (mf)0.01798 + (embed)0.02307\n",
            "Epoch 48 (train)3.6s: train loss = 0.04103 = (mf)0.01787 + (embed)0.02315\n",
            "Epoch 49 (train)4.4s: train loss = 0.03911 = (mf)0.01583 + (embed)0.02328\n",
            "Epoch 50 (train)3.5s + (eval)0.6s: train loss = 0.04211 = (mf)0.01886 + (embed)0.02326, recall = 0.29356, ndcg = 0.27868\n",
            "Training finished in 195.90 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.27868, Recall@50: 0.29356\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.5s: train loss = 0.22986 = (mf)0.22814 + (embed)0.00172\n",
            "Epoch 2 (train)4.4s: train loss = 0.13481 = (mf)0.13077 + (embed)0.00404\n",
            "Epoch 3 (train)5.1s: train loss = 0.10018 = (mf)0.09402 + (embed)0.00616\n",
            "Epoch 4 (train)4.4s: train loss = 0.08439 = (mf)0.07628 + (embed)0.00811\n",
            "Epoch 5 (train)4.3s + (eval)1.5s: train loss = 0.07400 = (mf)0.06424 + (embed)0.00976, recall = 0.30968, ndcg = 0.30268\n",
            "Epoch 6 (train)4.6s: train loss = 0.06677 = (mf)0.05556 + (embed)0.01121\n",
            "Epoch 7 (train)4.1s: train loss = 0.06118 = (mf)0.04870 + (embed)0.01248\n",
            "Epoch 8 (train)4.7s: train loss = 0.05712 = (mf)0.04359 + (embed)0.01353\n",
            "Epoch 9 (train)4.3s: train loss = 0.05574 = (mf)0.04128 + (embed)0.01446\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 0.05217 = (mf)0.03694 + (embed)0.01523, recall = 0.31793, ndcg = 0.31010\n",
            "Epoch 11 (train)4.9s: train loss = 0.05007 = (mf)0.03416 + (embed)0.01591\n",
            "Epoch 12 (train)4.2s: train loss = 0.04857 = (mf)0.03206 + (embed)0.01651\n",
            "Epoch 13 (train)4.2s: train loss = 0.04902 = (mf)0.03204 + (embed)0.01699\n",
            "Epoch 14 (train)4.9s: train loss = 0.04719 = (mf)0.02968 + (embed)0.01751\n",
            "Epoch 15 (train)4.1s + (eval)0.3s: train loss = 0.04581 = (mf)0.02776 + (embed)0.01805, recall = 0.31907, ndcg = 0.30713\n",
            "Epoch 16 (train)4.2s: train loss = 0.04710 = (mf)0.02870 + (embed)0.01840\n",
            "Epoch 17 (train)5.0s: train loss = 0.04566 = (mf)0.02690 + (embed)0.01876\n",
            "Epoch 18 (train)4.1s: train loss = 0.04351 = (mf)0.02445 + (embed)0.01906\n",
            "Epoch 19 (train)4.3s: train loss = 0.04378 = (mf)0.02437 + (embed)0.01941\n",
            "Epoch 20 (train)4.7s + (eval)0.6s: train loss = 0.04447 = (mf)0.02482 + (embed)0.01964, recall = 0.31547, ndcg = 0.29963\n",
            "Epoch 21 (train)4.0s: train loss = 0.04513 = (mf)0.02522 + (embed)0.01991\n",
            "Epoch 22 (train)4.7s: train loss = 0.04347 = (mf)0.02329 + (embed)0.02018\n",
            "Epoch 23 (train)4.3s: train loss = 0.04288 = (mf)0.02244 + (embed)0.02044\n",
            "Epoch 24 (train)4.0s: train loss = 0.04234 = (mf)0.02178 + (embed)0.02057\n",
            "Epoch 25 (train)5.0s + (eval)0.3s: train loss = 0.04185 = (mf)0.02113 + (embed)0.02073, recall = 0.31298, ndcg = 0.29647\n",
            "Epoch 26 (train)4.2s: train loss = 0.04244 = (mf)0.02159 + (embed)0.02085\n",
            "Epoch 27 (train)4.1s: train loss = 0.04076 = (mf)0.01971 + (embed)0.02105\n",
            "Epoch 28 (train)4.9s: train loss = 0.04081 = (mf)0.01964 + (embed)0.02118\n",
            "Epoch 29 (train)4.0s: train loss = 0.04024 = (mf)0.01892 + (embed)0.02131\n",
            "Epoch 30 (train)4.0s + (eval)0.3s: train loss = 0.03984 = (mf)0.01850 + (embed)0.02135, recall = 0.31539, ndcg = 0.29984\n",
            "Epoch 31 (train)4.9s: train loss = 0.04039 = (mf)0.01894 + (embed)0.02145\n",
            "Epoch 32 (train)4.1s: train loss = 0.04108 = (mf)0.01948 + (embed)0.02160\n",
            "Epoch 33 (train)4.1s: train loss = 0.04077 = (mf)0.01909 + (embed)0.02168\n",
            "Epoch 34 (train)4.9s: train loss = 0.04167 = (mf)0.01978 + (embed)0.02190\n",
            "Epoch 35 (train)4.1s + (eval)0.6s: train loss = 0.04058 = (mf)0.01860 + (embed)0.02198, recall = 0.30779, ndcg = 0.28992\n",
            "Epoch 36 (train)4.6s: train loss = 0.04055 = (mf)0.01844 + (embed)0.02211\n",
            "Epoch 37 (train)4.5s: train loss = 0.04100 = (mf)0.01877 + (embed)0.02223\n",
            "Epoch 38 (train)4.2s: train loss = 0.04033 = (mf)0.01798 + (embed)0.02235\n",
            "Epoch 39 (train)5.0s: train loss = 0.04039 = (mf)0.01794 + (embed)0.02245\n",
            "Epoch 40 (train)4.2s + (eval)0.3s: train loss = 0.04054 = (mf)0.01796 + (embed)0.02258, recall = 0.32106, ndcg = 0.30580\n",
            "Epoch 41 (train)4.1s: train loss = 0.04155 = (mf)0.01892 + (embed)0.02263\n",
            "Epoch 42 (train)4.9s: train loss = 0.04061 = (mf)0.01785 + (embed)0.02276\n",
            "Epoch 43 (train)4.1s: train loss = 0.04028 = (mf)0.01743 + (embed)0.02285\n",
            "Epoch 44 (train)4.1s: train loss = 0.04002 = (mf)0.01713 + (embed)0.02289\n",
            "Epoch 45 (train)4.9s + (eval)0.2s: train loss = 0.04103 = (mf)0.01798 + (embed)0.02305, recall = 0.32012, ndcg = 0.30347\n",
            "Epoch 46 (train)4.1s: train loss = 0.04092 = (mf)0.01783 + (embed)0.02309\n",
            "Epoch 47 (train)4.1s: train loss = 0.04043 = (mf)0.01724 + (embed)0.02319\n",
            "Epoch 48 (train)5.0s: train loss = 0.03957 = (mf)0.01632 + (embed)0.02325\n",
            "Epoch 49 (train)4.1s: train loss = 0.03919 = (mf)0.01586 + (embed)0.02333\n",
            "Epoch 50 (train)4.2s + (eval)0.4s: train loss = 0.04017 = (mf)0.01685 + (embed)0.02333, recall = 0.31466, ndcg = 0.30381\n",
            "Training finished in 227.04 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.30381, Recall@50: 0.31466\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.0s: train loss = 0.60181 = (mf)0.53526 + (embed)0.06655\n",
            "Epoch 2 (train)3.8s: train loss = 1.28793 = (mf)1.07938 + (embed)0.20855\n",
            "Epoch 3 (train)2.9s: train loss = 1.68053 = (mf)1.33835 + (embed)0.34218\n",
            "Epoch 4 (train)2.9s: train loss = 2.09106 = (mf)1.62414 + (embed)0.46692\n",
            "Epoch 5 (train)2.9s + (eval)0.9s: train loss = 2.33762 = (mf)1.76080 + (embed)0.57681, recall = 0.19655, ndcg = 0.19136\n",
            "Epoch 6 (train)3.7s: train loss = 2.67969 = (mf)1.99888 + (embed)0.68081\n",
            "Epoch 7 (train)2.9s: train loss = 2.99634 = (mf)2.21715 + (embed)0.77919\n",
            "Epoch 8 (train)2.9s: train loss = 3.33549 = (mf)2.46203 + (embed)0.87346\n",
            "Epoch 9 (train)3.0s: train loss = 3.30245 = (mf)2.34443 + (embed)0.95802\n",
            "Epoch 10 (train)3.8s + (eval)0.6s: train loss = 3.67384 = (mf)2.63879 + (embed)1.03505, recall = 0.18365, ndcg = 0.17812\n",
            "Epoch 11 (train)2.9s: train loss = 3.66839 = (mf)2.55988 + (embed)1.10851\n",
            "Epoch 12 (train)3.0s: train loss = 3.63913 = (mf)2.45965 + (embed)1.17948\n",
            "Epoch 13 (train)3.4s: train loss = 3.93603 = (mf)2.69100 + (embed)1.24503\n",
            "Epoch 14 (train)3.4s: train loss = 3.95641 = (mf)2.64887 + (embed)1.30755\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 4.31530 = (mf)2.95376 + (embed)1.36154, recall = 0.19890, ndcg = 0.18760\n",
            "Epoch 16 (train)2.9s: train loss = 4.09750 = (mf)2.68257 + (embed)1.41494\n",
            "Epoch 17 (train)3.4s: train loss = 4.27142 = (mf)2.80716 + (embed)1.46426\n",
            "Epoch 18 (train)3.3s: train loss = 4.27068 = (mf)2.75934 + (embed)1.51134\n",
            "Epoch 19 (train)2.9s: train loss = 4.19276 = (mf)2.63418 + (embed)1.55858\n",
            "Epoch 20 (train)2.9s + (eval)0.2s: train loss = 4.64446 = (mf)3.03997 + (embed)1.60449, recall = 0.20878, ndcg = 0.19682\n",
            "Epoch 21 (train)3.5s: train loss = 4.53229 = (mf)2.88571 + (embed)1.64658\n",
            "Epoch 22 (train)3.1s: train loss = 4.74613 = (mf)3.04625 + (embed)1.69988\n",
            "Epoch 23 (train)2.9s: train loss = 4.70360 = (mf)2.96730 + (embed)1.73630\n",
            "Epoch 24 (train)2.9s: train loss = 4.69087 = (mf)2.92414 + (embed)1.76673\n",
            "Epoch 25 (train)3.5s + (eval)0.8s: train loss = 4.82342 = (mf)3.01551 + (embed)1.80791, recall = 0.21507, ndcg = 0.20207\n",
            "Epoch 26 (train)2.9s: train loss = 4.74852 = (mf)2.91127 + (embed)1.83725\n",
            "Epoch 27 (train)2.9s: train loss = 4.96662 = (mf)3.09907 + (embed)1.86755\n",
            "Epoch 28 (train)2.9s: train loss = 5.24104 = (mf)3.33961 + (embed)1.90144\n",
            "Epoch 29 (train)3.7s: train loss = 4.63861 = (mf)2.71200 + (embed)1.92662\n",
            "Epoch 30 (train)3.0s + (eval)0.2s: train loss = 5.03600 = (mf)3.08047 + (embed)1.95553, recall = 0.20608, ndcg = 0.19176\n",
            "Epoch 31 (train)3.0s: train loss = 4.75167 = (mf)2.77145 + (embed)1.98021\n",
            "Epoch 32 (train)3.0s: train loss = 5.17938 = (mf)3.16910 + (embed)2.01028\n",
            "Epoch 33 (train)3.8s: train loss = 4.87344 = (mf)2.84010 + (embed)2.03333\n",
            "Epoch 34 (train)2.9s: train loss = 4.96136 = (mf)2.90876 + (embed)2.05259\n",
            "Epoch 35 (train)2.9s + (eval)0.2s: train loss = 5.02564 = (mf)2.94656 + (embed)2.07908, recall = 0.20593, ndcg = 0.18776\n",
            "Epoch 36 (train)2.9s: train loss = 5.06508 = (mf)2.96060 + (embed)2.10448\n",
            "Epoch 37 (train)3.8s: train loss = 5.08641 = (mf)2.95794 + (embed)2.12848\n",
            "Epoch 38 (train)3.0s: train loss = 4.89351 = (mf)2.74562 + (embed)2.14789\n",
            "Epoch 39 (train)2.9s: train loss = 4.74494 = (mf)2.58396 + (embed)2.16098\n",
            "Epoch 40 (train)2.9s + (eval)0.6s: train loss = 5.02289 = (mf)2.84146 + (embed)2.18143, recall = 0.20674, ndcg = 0.19311\n",
            "Epoch 41 (train)3.8s: train loss = 5.03480 = (mf)2.82736 + (embed)2.20744\n",
            "Epoch 42 (train)2.9s: train loss = 4.97629 = (mf)2.75146 + (embed)2.22483\n",
            "Epoch 43 (train)2.9s: train loss = 5.32113 = (mf)3.08412 + (embed)2.23702\n",
            "Epoch 44 (train)2.9s: train loss = 4.96827 = (mf)2.71638 + (embed)2.25189\n",
            "Epoch 45 (train)3.8s + (eval)0.3s: train loss = 4.89500 = (mf)2.61839 + (embed)2.27661, recall = 0.20876, ndcg = 0.19436\n",
            "Epoch 46 (train)3.0s: train loss = 5.30109 = (mf)2.99845 + (embed)2.30264\n",
            "Epoch 47 (train)3.0s: train loss = 5.10456 = (mf)2.78275 + (embed)2.32181\n",
            "Epoch 48 (train)3.1s: train loss = 5.49452 = (mf)3.15489 + (embed)2.33963\n",
            "Epoch 49 (train)3.7s: train loss = 5.24415 = (mf)2.88385 + (embed)2.36030\n",
            "Epoch 50 (train)3.0s + (eval)0.2s: train loss = 5.26852 = (mf)2.88428 + (embed)2.38424, recall = 0.21100, ndcg = 0.19736\n",
            "Training finished in 162.59 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19736, Recall@50: 0.21100\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.7s: train loss = 0.41877 = (mf)0.36717 + (embed)0.05160\n",
            "Epoch 2 (train)3.6s: train loss = 0.77830 = (mf)0.60933 + (embed)0.16896\n",
            "Epoch 3 (train)4.0s: train loss = 1.11249 = (mf)0.81878 + (embed)0.29371\n",
            "Epoch 4 (train)3.8s: train loss = 1.35525 = (mf)0.95141 + (embed)0.40384\n",
            "Epoch 5 (train)3.5s + (eval)0.8s: train loss = 1.58071 = (mf)1.07859 + (embed)0.50212, recall = 0.18253, ndcg = 0.17586\n",
            "Epoch 6 (train)3.8s: train loss = 1.83667 = (mf)1.23797 + (embed)0.59870\n",
            "Epoch 7 (train)4.1s: train loss = 1.97310 = (mf)1.28456 + (embed)0.68855\n",
            "Epoch 8 (train)3.5s: train loss = 2.16880 = (mf)1.40297 + (embed)0.76583\n",
            "Epoch 9 (train)3.5s: train loss = 2.18675 = (mf)1.35479 + (embed)0.83196\n",
            "Epoch 10 (train)4.4s + (eval)0.6s: train loss = 2.35149 = (mf)1.45895 + (embed)0.89254, recall = 0.17686, ndcg = 0.17163\n",
            "Epoch 11 (train)3.5s: train loss = 2.69044 = (mf)1.73590 + (embed)0.95454\n",
            "Epoch 12 (train)3.5s: train loss = 2.55958 = (mf)1.54566 + (embed)1.01392\n",
            "Epoch 13 (train)4.3s: train loss = 2.67758 = (mf)1.61141 + (embed)1.06618\n",
            "Epoch 14 (train)3.5s: train loss = 2.65555 = (mf)1.54291 + (embed)1.11264\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 2.79006 = (mf)1.64702 + (embed)1.14304, recall = 0.17612, ndcg = 0.16937\n",
            "Epoch 16 (train)4.0s: train loss = 2.83292 = (mf)1.65112 + (embed)1.18180\n",
            "Epoch 17 (train)3.7s: train loss = 2.79747 = (mf)1.57747 + (embed)1.22001\n",
            "Epoch 18 (train)3.5s: train loss = 2.91432 = (mf)1.66155 + (embed)1.25277\n",
            "Epoch 19 (train)3.5s: train loss = 3.02657 = (mf)1.74265 + (embed)1.28392\n",
            "Epoch 20 (train)4.4s + (eval)0.2s: train loss = 2.92521 = (mf)1.60918 + (embed)1.31603, recall = 0.19444, ndcg = 0.18025\n",
            "Epoch 21 (train)3.5s: train loss = 3.05671 = (mf)1.71321 + (embed)1.34350\n",
            "Epoch 22 (train)3.5s: train loss = 3.06831 = (mf)1.69405 + (embed)1.37426\n",
            "Epoch 23 (train)4.4s: train loss = 3.15473 = (mf)1.75100 + (embed)1.40373\n",
            "Epoch 24 (train)3.5s: train loss = 3.12998 = (mf)1.70109 + (embed)1.42889\n",
            "Epoch 25 (train)3.5s + (eval)0.6s: train loss = 3.13292 = (mf)1.67335 + (embed)1.45957, recall = 0.20237, ndcg = 0.18869\n",
            "Epoch 26 (train)4.3s: train loss = 3.19697 = (mf)1.71813 + (embed)1.47885\n",
            "Epoch 27 (train)3.5s: train loss = 3.02871 = (mf)1.52979 + (embed)1.49892\n",
            "Epoch 28 (train)3.4s: train loss = 3.23757 = (mf)1.72429 + (embed)1.51329\n",
            "Epoch 29 (train)3.6s: train loss = 3.05551 = (mf)1.51974 + (embed)1.53577\n",
            "Epoch 30 (train)4.2s + (eval)0.2s: train loss = 3.28955 = (mf)1.73497 + (embed)1.55457, recall = 0.21018, ndcg = 0.19386\n",
            "Epoch 31 (train)3.5s: train loss = 3.14697 = (mf)1.57757 + (embed)1.56939\n",
            "Epoch 32 (train)3.5s: train loss = 3.26310 = (mf)1.68411 + (embed)1.57899\n",
            "Epoch 33 (train)4.3s: train loss = 3.06090 = (mf)1.46335 + (embed)1.59755\n",
            "Epoch 34 (train)3.6s: train loss = 3.24842 = (mf)1.63592 + (embed)1.61251\n",
            "Epoch 35 (train)3.6s + (eval)0.3s: train loss = 3.30253 = (mf)1.67388 + (embed)1.62865, recall = 0.20020, ndcg = 0.18327\n",
            "Epoch 36 (train)4.3s: train loss = 3.24480 = (mf)1.60927 + (embed)1.63554\n",
            "Epoch 37 (train)3.5s: train loss = 3.30875 = (mf)1.66291 + (embed)1.64585\n",
            "Epoch 38 (train)3.5s: train loss = 3.34591 = (mf)1.68296 + (embed)1.66295\n",
            "Epoch 39 (train)4.0s: train loss = 3.19800 = (mf)1.52565 + (embed)1.67235\n",
            "Epoch 40 (train)4.0s + (eval)0.6s: train loss = 3.42481 = (mf)1.73392 + (embed)1.69089, recall = 0.20851, ndcg = 0.19070\n",
            "Epoch 41 (train)3.5s: train loss = 3.24026 = (mf)1.54229 + (embed)1.69798\n",
            "Epoch 42 (train)3.6s: train loss = 3.39068 = (mf)1.68392 + (embed)1.70676\n",
            "Epoch 43 (train)4.3s: train loss = 3.32129 = (mf)1.59977 + (embed)1.72152\n",
            "Epoch 44 (train)3.5s: train loss = 3.25769 = (mf)1.53516 + (embed)1.72253\n",
            "Epoch 45 (train)3.5s + (eval)0.2s: train loss = 3.21567 = (mf)1.48001 + (embed)1.73566, recall = 0.18666, ndcg = 0.16717\n",
            "Epoch 46 (train)4.3s: train loss = 3.36621 = (mf)1.62343 + (embed)1.74277\n",
            "Epoch 47 (train)3.5s: train loss = 3.22551 = (mf)1.47308 + (embed)1.75243\n",
            "Epoch 48 (train)3.5s: train loss = 3.45273 = (mf)1.69531 + (embed)1.75742\n",
            "Epoch 49 (train)4.1s: train loss = 3.21210 = (mf)1.44881 + (embed)1.76329\n",
            "Epoch 50 (train)3.7s + (eval)0.3s: train loss = 3.41112 = (mf)1.64361 + (embed)1.76751, recall = 0.21052, ndcg = 0.19091\n",
            "Training finished in 192.85 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19091, Recall@50: 0.21052\n",
            "--- Trying combination: Epochs=50, TopK=50, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.7s: train loss = 0.35726 = (mf)0.31443 + (embed)0.04282\n",
            "Epoch 2 (train)4.1s: train loss = 0.55242 = (mf)0.40623 + (embed)0.14619\n",
            "Epoch 3 (train)4.9s: train loss = 0.82129 = (mf)0.56188 + (embed)0.25941\n",
            "Epoch 4 (train)4.1s: train loss = 0.98034 = (mf)0.62313 + (embed)0.35721\n",
            "Epoch 5 (train)4.0s + (eval)0.8s: train loss = 1.14084 = (mf)0.69495 + (embed)0.44589, recall = 0.17848, ndcg = 0.17194\n",
            "Epoch 6 (train)4.9s: train loss = 1.27651 = (mf)0.75881 + (embed)0.51769\n",
            "Epoch 7 (train)4.1s: train loss = 1.46791 = (mf)0.87702 + (embed)0.59089\n",
            "Epoch 8 (train)4.1s: train loss = 1.54227 = (mf)0.88743 + (embed)0.65484\n",
            "Epoch 9 (train)5.0s: train loss = 1.58269 = (mf)0.87538 + (embed)0.70731\n",
            "Epoch 10 (train)4.1s + (eval)0.3s: train loss = 1.69258 = (mf)0.94132 + (embed)0.75126, recall = 0.19986, ndcg = 0.18452\n",
            "Epoch 11 (train)4.3s: train loss = 1.74264 = (mf)0.95220 + (embed)0.79043\n",
            "Epoch 12 (train)4.8s: train loss = 1.84806 = (mf)1.01070 + (embed)0.83736\n",
            "Epoch 13 (train)4.1s: train loss = 1.93205 = (mf)1.05132 + (embed)0.88073\n",
            "Epoch 14 (train)4.4s: train loss = 1.99175 = (mf)1.07699 + (embed)0.91476\n",
            "Epoch 15 (train)4.5s + (eval)0.6s: train loss = 1.96385 = (mf)1.01008 + (embed)0.95377, recall = 0.19060, ndcg = 0.17588\n",
            "Epoch 16 (train)4.1s: train loss = 2.02693 = (mf)1.04349 + (embed)0.98344\n",
            "Epoch 17 (train)4.9s: train loss = 2.08547 = (mf)1.07135 + (embed)1.01412\n",
            "Epoch 18 (train)4.1s: train loss = 2.20537 = (mf)1.15654 + (embed)1.04883\n",
            "Epoch 19 (train)4.1s: train loss = 2.14945 = (mf)1.07387 + (embed)1.07558\n",
            "Epoch 20 (train)4.9s + (eval)0.3s: train loss = 2.13514 = (mf)1.03455 + (embed)1.10059, recall = 0.19834, ndcg = 0.18531\n",
            "Epoch 21 (train)4.2s: train loss = 2.14140 = (mf)1.01832 + (embed)1.12309\n",
            "Epoch 22 (train)4.2s: train loss = 2.25709 = (mf)1.11682 + (embed)1.14027\n",
            "Epoch 23 (train)5.0s: train loss = 2.26590 = (mf)1.10830 + (embed)1.15760\n",
            "Epoch 24 (train)4.1s: train loss = 2.17199 = (mf)0.99856 + (embed)1.17343\n",
            "Epoch 25 (train)4.2s + (eval)0.4s: train loss = 2.19955 = (mf)1.01021 + (embed)1.18934, recall = 0.19352, ndcg = 0.17490\n",
            "Epoch 26 (train)4.9s: train loss = 2.23429 = (mf)1.03277 + (embed)1.20152\n",
            "Epoch 27 (train)4.2s: train loss = 2.28624 = (mf)1.06628 + (embed)1.21996\n",
            "Epoch 28 (train)4.9s: train loss = 2.38373 = (mf)1.14523 + (embed)1.23850\n",
            "Epoch 29 (train)4.5s: train loss = 2.34822 = (mf)1.09306 + (embed)1.25516\n",
            "Epoch 30 (train)4.2s + (eval)0.6s: train loss = 2.35770 = (mf)1.09075 + (embed)1.26695, recall = 0.18574, ndcg = 0.17022\n",
            "Epoch 31 (train)5.0s: train loss = 2.35245 = (mf)1.07249 + (embed)1.27996\n",
            "Epoch 32 (train)4.3s: train loss = 2.30700 = (mf)1.01190 + (embed)1.29510\n",
            "Epoch 33 (train)4.3s: train loss = 2.36177 = (mf)1.06439 + (embed)1.29738\n",
            "Epoch 34 (train)5.2s: train loss = 2.31793 = (mf)1.01108 + (embed)1.30685\n",
            "Epoch 35 (train)4.3s + (eval)0.3s: train loss = 2.44308 = (mf)1.12528 + (embed)1.31780, recall = 0.19821, ndcg = 0.17855\n",
            "Epoch 36 (train)4.5s: train loss = 2.46058 = (mf)1.13702 + (embed)1.32356\n",
            "Epoch 37 (train)4.7s: train loss = 2.40074 = (mf)1.06781 + (embed)1.33293\n",
            "Epoch 38 (train)4.1s: train loss = 2.31319 = (mf)0.97511 + (embed)1.33808\n",
            "Epoch 39 (train)4.7s: train loss = 2.39377 = (mf)1.06093 + (embed)1.33284\n",
            "Epoch 40 (train)4.3s + (eval)0.3s: train loss = 2.35821 = (mf)1.02418 + (embed)1.33403, recall = 0.20207, ndcg = 0.17782\n",
            "Epoch 41 (train)4.1s: train loss = 2.24780 = (mf)0.91117 + (embed)1.33663\n",
            "Epoch 42 (train)5.0s: train loss = 2.41553 = (mf)1.06957 + (embed)1.34596\n",
            "Epoch 43 (train)4.1s: train loss = 2.36459 = (mf)1.00930 + (embed)1.35529\n",
            "Epoch 44 (train)4.1s: train loss = 2.35841 = (mf)0.99194 + (embed)1.36648\n",
            "Epoch 45 (train)4.9s + (eval)0.6s: train loss = 2.40750 = (mf)1.04067 + (embed)1.36683, recall = 0.19769, ndcg = 0.18100\n",
            "Epoch 46 (train)4.1s: train loss = 2.42667 = (mf)1.05249 + (embed)1.37417\n",
            "Epoch 47 (train)4.1s: train loss = 2.34775 = (mf)0.96987 + (embed)1.37788\n",
            "Epoch 48 (train)4.9s: train loss = 2.45279 = (mf)1.06722 + (embed)1.38557\n",
            "Epoch 49 (train)4.0s: train loss = 2.52006 = (mf)1.12378 + (embed)1.39629\n",
            "Epoch 50 (train)4.0s + (eval)0.4s: train loss = 2.34594 = (mf)0.93971 + (embed)1.40623, recall = 0.20760, ndcg = 0.19102\n",
            "Training finished in 226.61 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19102, Recall@50: 0.20760\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.9s: train loss = 0.42361 = (mf)0.42338 + (embed)0.00023\n",
            "Epoch 2 (train)3.8s: train loss = 0.22201 = (mf)0.22136 + (embed)0.00065\n",
            "Epoch 3 (train)2.9s: train loss = 0.20627 = (mf)0.20552 + (embed)0.00075\n",
            "Epoch 4 (train)2.9s: train loss = 0.18143 = (mf)0.18054 + (embed)0.00088\n",
            "Epoch 5 (train)2.9s + (eval)1.2s: train loss = 0.16350 = (mf)0.16246 + (embed)0.00104, recall = 0.09316, ndcg = 0.24615\n",
            "Epoch 6 (train)3.6s: train loss = 0.15013 = (mf)0.14894 + (embed)0.00119\n",
            "Epoch 7 (train)3.0s: train loss = 0.14173 = (mf)0.14038 + (embed)0.00135\n",
            "Epoch 8 (train)3.0s: train loss = 0.13253 = (mf)0.13102 + (embed)0.00151\n",
            "Epoch 9 (train)3.1s: train loss = 0.12850 = (mf)0.12686 + (embed)0.00164\n",
            "Epoch 10 (train)3.7s + (eval)0.2s: train loss = 0.11953 = (mf)0.11774 + (embed)0.00179, recall = 0.10578, ndcg = 0.27189\n",
            "Epoch 11 (train)2.9s: train loss = 0.11249 = (mf)0.11055 + (embed)0.00194\n",
            "Epoch 12 (train)2.9s: train loss = 0.11119 = (mf)0.10911 + (embed)0.00208\n",
            "Epoch 13 (train)3.1s: train loss = 0.10705 = (mf)0.10483 + (embed)0.00222\n",
            "Epoch 14 (train)3.5s: train loss = 0.10295 = (mf)0.10059 + (embed)0.00235\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 0.09463 = (mf)0.09212 + (embed)0.00251, recall = 0.11239, ndcg = 0.27654\n",
            "Epoch 16 (train)2.9s: train loss = 0.09184 = (mf)0.08918 + (embed)0.00266\n",
            "Epoch 17 (train)3.3s: train loss = 0.08728 = (mf)0.08445 + (embed)0.00282\n",
            "Epoch 18 (train)3.5s: train loss = 0.08685 = (mf)0.08389 + (embed)0.00296\n",
            "Epoch 19 (train)3.0s: train loss = 0.08000 = (mf)0.07688 + (embed)0.00312\n",
            "Epoch 20 (train)2.9s + (eval)0.6s: train loss = 0.07679 = (mf)0.07350 + (embed)0.00328, recall = 0.12000, ndcg = 0.29495\n",
            "Epoch 21 (train)3.5s: train loss = 0.07305 = (mf)0.06961 + (embed)0.00344\n",
            "Epoch 22 (train)3.3s: train loss = 0.07181 = (mf)0.06822 + (embed)0.00360\n",
            "Epoch 23 (train)2.9s: train loss = 0.06867 = (mf)0.06492 + (embed)0.00375\n",
            "Epoch 24 (train)3.0s: train loss = 0.06548 = (mf)0.06159 + (embed)0.00389\n",
            "Epoch 25 (train)3.6s + (eval)0.3s: train loss = 0.06190 = (mf)0.05785 + (embed)0.00405, recall = 0.12687, ndcg = 0.31633\n",
            "Epoch 26 (train)3.1s: train loss = 0.06187 = (mf)0.05768 + (embed)0.00419\n",
            "Epoch 27 (train)3.0s: train loss = 0.05901 = (mf)0.05469 + (embed)0.00432\n",
            "Epoch 28 (train)3.0s: train loss = 0.05898 = (mf)0.05451 + (embed)0.00447\n",
            "Epoch 29 (train)3.8s: train loss = 0.05594 = (mf)0.05135 + (embed)0.00460\n",
            "Epoch 30 (train)2.9s + (eval)0.2s: train loss = 0.05528 = (mf)0.05055 + (embed)0.00472, recall = 0.13134, ndcg = 0.32398\n",
            "Epoch 31 (train)3.0s: train loss = 0.05209 = (mf)0.04724 + (embed)0.00486\n",
            "Epoch 32 (train)2.9s: train loss = 0.05382 = (mf)0.04884 + (embed)0.00498\n",
            "Epoch 33 (train)3.8s: train loss = 0.04957 = (mf)0.04446 + (embed)0.00511\n",
            "Epoch 34 (train)2.9s: train loss = 0.04896 = (mf)0.04372 + (embed)0.00524\n",
            "Epoch 35 (train)3.0s + (eval)0.6s: train loss = 0.04965 = (mf)0.04431 + (embed)0.00534, recall = 0.13548, ndcg = 0.33949\n",
            "Epoch 36 (train)2.9s: train loss = 0.04650 = (mf)0.04104 + (embed)0.00546\n",
            "Epoch 37 (train)3.8s: train loss = 0.04506 = (mf)0.03950 + (embed)0.00557\n",
            "Epoch 38 (train)3.0s: train loss = 0.04342 = (mf)0.03775 + (embed)0.00567\n",
            "Epoch 39 (train)2.9s: train loss = 0.04459 = (mf)0.03881 + (embed)0.00577\n",
            "Epoch 40 (train)2.9s + (eval)0.2s: train loss = 0.04305 = (mf)0.03719 + (embed)0.00586, recall = 0.14001, ndcg = 0.34577\n",
            "Epoch 41 (train)3.8s: train loss = 0.04405 = (mf)0.03808 + (embed)0.00597\n",
            "Epoch 42 (train)3.0s: train loss = 0.04307 = (mf)0.03701 + (embed)0.00605\n",
            "Epoch 43 (train)3.0s: train loss = 0.04114 = (mf)0.03497 + (embed)0.00616\n",
            "Epoch 44 (train)3.0s: train loss = 0.03963 = (mf)0.03337 + (embed)0.00626\n",
            "Epoch 45 (train)3.8s + (eval)0.2s: train loss = 0.03898 = (mf)0.03265 + (embed)0.00633, recall = 0.13854, ndcg = 0.34053\n",
            "Epoch 46 (train)3.0s: train loss = 0.03978 = (mf)0.03336 + (embed)0.00642\n",
            "Epoch 47 (train)3.0s: train loss = 0.03843 = (mf)0.03194 + (embed)0.00649\n",
            "Epoch 48 (train)3.0s: train loss = 0.03736 = (mf)0.03077 + (embed)0.00659\n",
            "Epoch 49 (train)3.6s: train loss = 0.03674 = (mf)0.03007 + (embed)0.00667\n",
            "Epoch 50 (train)2.9s + (eval)0.2s: train loss = 0.03657 = (mf)0.02983 + (embed)0.00674, recall = 0.13896, ndcg = 0.33985\n",
            "Epoch 51 (train)2.9s: train loss = 0.03559 = (mf)0.02877 + (embed)0.00682\n",
            "Epoch 52 (train)3.1s: train loss = 0.03558 = (mf)0.02870 + (embed)0.00688\n",
            "Epoch 53 (train)3.5s: train loss = 0.03472 = (mf)0.02777 + (embed)0.00695\n",
            "Epoch 54 (train)2.9s: train loss = 0.03446 = (mf)0.02744 + (embed)0.00702\n",
            "Epoch 55 (train)2.9s + (eval)0.5s: train loss = 0.03535 = (mf)0.02828 + (embed)0.00708, recall = 0.14042, ndcg = 0.34237\n",
            "Epoch 56 (train)3.4s: train loss = 0.03399 = (mf)0.02687 + (embed)0.00712\n",
            "Epoch 57 (train)3.4s: train loss = 0.03364 = (mf)0.02645 + (embed)0.00719\n",
            "Epoch 58 (train)3.0s: train loss = 0.03254 = (mf)0.02529 + (embed)0.00725\n",
            "Epoch 59 (train)2.9s: train loss = 0.03182 = (mf)0.02449 + (embed)0.00732\n",
            "Epoch 60 (train)3.4s + (eval)0.3s: train loss = 0.03208 = (mf)0.02470 + (embed)0.00738, recall = 0.14063, ndcg = 0.34333\n",
            "Epoch 61 (train)3.3s: train loss = 0.03250 = (mf)0.02507 + (embed)0.00742\n",
            "Epoch 62 (train)2.9s: train loss = 0.03106 = (mf)0.02357 + (embed)0.00749\n",
            "Epoch 63 (train)2.9s: train loss = 0.03036 = (mf)0.02284 + (embed)0.00752\n",
            "Epoch 64 (train)3.5s: train loss = 0.02982 = (mf)0.02224 + (embed)0.00758\n",
            "Epoch 65 (train)3.3s + (eval)0.2s: train loss = 0.03022 = (mf)0.02259 + (embed)0.00762, recall = 0.14115, ndcg = 0.34418\n",
            "Epoch 66 (train)2.9s: train loss = 0.02986 = (mf)0.02220 + (embed)0.00767\n",
            "Epoch 67 (train)2.9s: train loss = 0.02980 = (mf)0.02207 + (embed)0.00774\n",
            "Epoch 68 (train)3.5s: train loss = 0.03024 = (mf)0.02249 + (embed)0.00775\n",
            "Epoch 69 (train)3.1s: train loss = 0.03025 = (mf)0.02248 + (embed)0.00777\n",
            "Epoch 70 (train)2.9s + (eval)0.6s: train loss = 0.02858 = (mf)0.02075 + (embed)0.00783, recall = 0.14404, ndcg = 0.34277\n",
            "Epoch 71 (train)2.9s: train loss = 0.02749 = (mf)0.01962 + (embed)0.00788\n",
            "Epoch 72 (train)3.8s: train loss = 0.02886 = (mf)0.02096 + (embed)0.00790\n",
            "Epoch 73 (train)3.0s: train loss = 0.02804 = (mf)0.02010 + (embed)0.00795\n",
            "Epoch 74 (train)2.9s: train loss = 0.02777 = (mf)0.01980 + (embed)0.00797\n",
            "Epoch 75 (train)2.9s + (eval)0.2s: train loss = 0.02752 = (mf)0.01951 + (embed)0.00801, recall = 0.14256, ndcg = 0.33938\n",
            "Epoch 76 (train)3.8s: train loss = 0.02836 = (mf)0.02030 + (embed)0.00806\n",
            "Epoch 77 (train)2.9s: train loss = 0.02786 = (mf)0.01977 + (embed)0.00808\n",
            "Epoch 78 (train)2.9s: train loss = 0.02757 = (mf)0.01946 + (embed)0.00812\n",
            "Epoch 79 (train)2.9s: train loss = 0.02778 = (mf)0.01963 + (embed)0.00815\n",
            "Epoch 80 (train)3.8s + (eval)0.2s: train loss = 0.02669 = (mf)0.01851 + (embed)0.00818, recall = 0.14502, ndcg = 0.34178\n",
            "Epoch 81 (train)2.9s: train loss = 0.02550 = (mf)0.01731 + (embed)0.00819\n",
            "Epoch 82 (train)2.9s: train loss = 0.02755 = (mf)0.01931 + (embed)0.00823\n",
            "Epoch 83 (train)2.9s: train loss = 0.02548 = (mf)0.01721 + (embed)0.00826\n",
            "Epoch 84 (train)3.8s: train loss = 0.02560 = (mf)0.01731 + (embed)0.00829\n",
            "Epoch 85 (train)2.9s + (eval)0.5s: train loss = 0.02613 = (mf)0.01782 + (embed)0.00831, recall = 0.14086, ndcg = 0.33897\n",
            "Epoch 86 (train)2.9s: train loss = 0.02578 = (mf)0.01746 + (embed)0.00832\n",
            "Epoch 87 (train)3.0s: train loss = 0.02590 = (mf)0.01755 + (embed)0.00835\n",
            "Epoch 88 (train)3.9s: train loss = 0.02507 = (mf)0.01670 + (embed)0.00838\n",
            "Epoch 89 (train)3.0s: train loss = 0.02516 = (mf)0.01675 + (embed)0.00841\n",
            "Epoch 90 (train)3.0s + (eval)0.2s: train loss = 0.02563 = (mf)0.01720 + (embed)0.00843, recall = 0.14080, ndcg = 0.33468\n",
            "Epoch 91 (train)3.0s: train loss = 0.02457 = (mf)0.01609 + (embed)0.00847\n",
            "Epoch 92 (train)3.8s: train loss = 0.02525 = (mf)0.01677 + (embed)0.00849\n",
            "Epoch 93 (train)2.9s: train loss = 0.02407 = (mf)0.01556 + (embed)0.00851\n",
            "Epoch 94 (train)3.0s: train loss = 0.02376 = (mf)0.01522 + (embed)0.00853\n",
            "Epoch 95 (train)3.1s + (eval)0.3s: train loss = 0.02369 = (mf)0.01514 + (embed)0.00855, recall = 0.14299, ndcg = 0.33801\n",
            "Epoch 96 (train)3.6s: train loss = 0.02331 = (mf)0.01475 + (embed)0.00856\n",
            "Epoch 97 (train)2.9s: train loss = 0.02389 = (mf)0.01532 + (embed)0.00857\n",
            "Epoch 98 (train)2.9s: train loss = 0.02397 = (mf)0.01538 + (embed)0.00859\n",
            "Epoch 99 (train)3.0s: train loss = 0.02389 = (mf)0.01529 + (embed)0.00860\n",
            "Epoch 100 (train)3.7s + (eval)0.5s: train loss = 0.02331 = (mf)0.01470 + (embed)0.00862, recall = 0.14265, ndcg = 0.33575\n",
            "Training finished in 323.68 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.33575, Recall@10: 0.14265\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.3s: train loss = 0.41714 = (mf)0.41689 + (embed)0.00025\n",
            "Epoch 2 (train)3.4s: train loss = 0.22996 = (mf)0.22931 + (embed)0.00065\n",
            "Epoch 3 (train)3.5s: train loss = 0.21041 = (mf)0.20963 + (embed)0.00078\n",
            "Epoch 4 (train)4.3s: train loss = 0.18889 = (mf)0.18795 + (embed)0.00094\n",
            "Epoch 5 (train)3.6s + (eval)1.2s: train loss = 0.17294 = (mf)0.17181 + (embed)0.00114, recall = 0.10127, ndcg = 0.26022\n",
            "Epoch 6 (train)3.5s: train loss = 0.16215 = (mf)0.16083 + (embed)0.00132\n",
            "Epoch 7 (train)4.3s: train loss = 0.15089 = (mf)0.14939 + (embed)0.00150\n",
            "Epoch 8 (train)3.6s: train loss = 0.14331 = (mf)0.14164 + (embed)0.00168\n",
            "Epoch 9 (train)3.5s: train loss = 0.13941 = (mf)0.13760 + (embed)0.00181\n",
            "Epoch 10 (train)3.7s + (eval)0.3s: train loss = 0.13187 = (mf)0.12991 + (embed)0.00196, recall = 0.10152, ndcg = 0.26898\n",
            "Epoch 11 (train)4.0s: train loss = 0.13085 = (mf)0.12874 + (embed)0.00211\n",
            "Epoch 12 (train)3.5s: train loss = 0.12311 = (mf)0.12085 + (embed)0.00226\n",
            "Epoch 13 (train)3.5s: train loss = 0.12105 = (mf)0.11864 + (embed)0.00242\n",
            "Epoch 14 (train)4.3s: train loss = 0.11455 = (mf)0.11199 + (embed)0.00256\n",
            "Epoch 15 (train)3.4s + (eval)0.2s: train loss = 0.11142 = (mf)0.10871 + (embed)0.00271, recall = 0.10735, ndcg = 0.27317\n",
            "Epoch 16 (train)3.5s: train loss = 0.10746 = (mf)0.10458 + (embed)0.00289\n",
            "Epoch 17 (train)4.3s: train loss = 0.10565 = (mf)0.10261 + (embed)0.00304\n",
            "Epoch 18 (train)3.6s: train loss = 0.10078 = (mf)0.09757 + (embed)0.00321\n",
            "Epoch 19 (train)3.6s: train loss = 0.09839 = (mf)0.09502 + (embed)0.00337\n",
            "Epoch 20 (train)3.7s + (eval)0.3s: train loss = 0.09271 = (mf)0.08917 + (embed)0.00354, recall = 0.10969, ndcg = 0.27883\n",
            "Epoch 21 (train)4.0s: train loss = 0.09210 = (mf)0.08839 + (embed)0.00371\n",
            "Epoch 22 (train)3.5s: train loss = 0.08722 = (mf)0.08334 + (embed)0.00387\n",
            "Epoch 23 (train)3.5s: train loss = 0.08484 = (mf)0.08080 + (embed)0.00404\n",
            "Epoch 24 (train)4.4s: train loss = 0.08124 = (mf)0.07702 + (embed)0.00422\n",
            "Epoch 25 (train)3.5s + (eval)0.5s: train loss = 0.08137 = (mf)0.07696 + (embed)0.00441, recall = 0.11485, ndcg = 0.29600\n",
            "Epoch 26 (train)3.4s: train loss = 0.07833 = (mf)0.07377 + (embed)0.00457\n",
            "Epoch 27 (train)4.3s: train loss = 0.07650 = (mf)0.07176 + (embed)0.00474\n",
            "Epoch 28 (train)3.5s: train loss = 0.07202 = (mf)0.06710 + (embed)0.00492\n",
            "Epoch 29 (train)3.5s: train loss = 0.06989 = (mf)0.06479 + (embed)0.00510\n",
            "Epoch 30 (train)3.9s + (eval)0.3s: train loss = 0.06790 = (mf)0.06262 + (embed)0.00527, recall = 0.11982, ndcg = 0.30635\n",
            "Epoch 31 (train)3.9s: train loss = 0.06739 = (mf)0.06194 + (embed)0.00545\n",
            "Epoch 32 (train)3.5s: train loss = 0.06406 = (mf)0.05845 + (embed)0.00561\n",
            "Epoch 33 (train)3.5s: train loss = 0.06334 = (mf)0.05761 + (embed)0.00573\n",
            "Epoch 34 (train)4.4s: train loss = 0.06061 = (mf)0.05472 + (embed)0.00589\n",
            "Epoch 35 (train)3.6s + (eval)0.2s: train loss = 0.05949 = (mf)0.05348 + (embed)0.00601, recall = 0.12850, ndcg = 0.32559\n",
            "Epoch 36 (train)3.6s: train loss = 0.06092 = (mf)0.05476 + (embed)0.00616\n",
            "Epoch 37 (train)4.3s: train loss = 0.05851 = (mf)0.05218 + (embed)0.00633\n",
            "Epoch 38 (train)3.5s: train loss = 0.05721 = (mf)0.05075 + (embed)0.00646\n",
            "Epoch 39 (train)3.5s: train loss = 0.05731 = (mf)0.05073 + (embed)0.00658\n",
            "Epoch 40 (train)4.2s + (eval)0.8s: train loss = 0.05561 = (mf)0.04890 + (embed)0.00670, recall = 0.13016, ndcg = 0.33100\n",
            "Epoch 41 (train)3.5s: train loss = 0.05274 = (mf)0.04592 + (embed)0.00682\n",
            "Epoch 42 (train)3.5s: train loss = 0.05409 = (mf)0.04713 + (embed)0.00695\n",
            "Epoch 43 (train)3.8s: train loss = 0.05332 = (mf)0.04621 + (embed)0.00711\n",
            "Epoch 44 (train)4.1s: train loss = 0.05069 = (mf)0.04350 + (embed)0.00719\n",
            "Epoch 45 (train)3.5s + (eval)0.2s: train loss = 0.05075 = (mf)0.04343 + (embed)0.00732, recall = 0.13530, ndcg = 0.34122\n",
            "Epoch 46 (train)3.5s: train loss = 0.05059 = (mf)0.04316 + (embed)0.00743\n",
            "Epoch 47 (train)4.4s: train loss = 0.04930 = (mf)0.04177 + (embed)0.00753\n",
            "Epoch 48 (train)3.5s: train loss = 0.04813 = (mf)0.04050 + (embed)0.00762\n",
            "Epoch 49 (train)3.5s: train loss = 0.04749 = (mf)0.03977 + (embed)0.00772\n",
            "Epoch 50 (train)4.3s + (eval)0.2s: train loss = 0.04642 = (mf)0.03860 + (embed)0.00781, recall = 0.13680, ndcg = 0.34145\n",
            "Epoch 51 (train)3.5s: train loss = 0.04576 = (mf)0.03785 + (embed)0.00791\n",
            "Epoch 52 (train)3.5s: train loss = 0.04624 = (mf)0.03825 + (embed)0.00800\n",
            "Epoch 53 (train)3.8s: train loss = 0.04417 = (mf)0.03609 + (embed)0.00808\n",
            "Epoch 54 (train)4.0s: train loss = 0.04559 = (mf)0.03743 + (embed)0.00816\n",
            "Epoch 55 (train)3.5s + (eval)0.6s: train loss = 0.04360 = (mf)0.03535 + (embed)0.00825, recall = 0.14073, ndcg = 0.34755\n",
            "Epoch 56 (train)3.5s: train loss = 0.04383 = (mf)0.03549 + (embed)0.00834\n",
            "Epoch 57 (train)4.3s: train loss = 0.04273 = (mf)0.03432 + (embed)0.00841\n",
            "Epoch 58 (train)3.5s: train loss = 0.04321 = (mf)0.03472 + (embed)0.00849\n",
            "Epoch 59 (train)3.5s: train loss = 0.04175 = (mf)0.03318 + (embed)0.00857\n",
            "Epoch 60 (train)4.3s + (eval)0.2s: train loss = 0.04241 = (mf)0.03376 + (embed)0.00864, recall = 0.14170, ndcg = 0.34535\n",
            "Epoch 61 (train)3.5s: train loss = 0.04183 = (mf)0.03312 + (embed)0.00871\n",
            "Epoch 62 (train)3.4s: train loss = 0.03999 = (mf)0.03119 + (embed)0.00879\n",
            "Epoch 63 (train)3.9s: train loss = 0.04141 = (mf)0.03259 + (embed)0.00882\n",
            "Epoch 64 (train)3.9s: train loss = 0.04056 = (mf)0.03169 + (embed)0.00887\n",
            "Epoch 65 (train)3.5s + (eval)0.2s: train loss = 0.03890 = (mf)0.02994 + (embed)0.00895, recall = 0.13861, ndcg = 0.34160\n",
            "Epoch 66 (train)3.5s: train loss = 0.03945 = (mf)0.03043 + (embed)0.00902\n",
            "Epoch 67 (train)4.3s: train loss = 0.03860 = (mf)0.02951 + (embed)0.00908\n",
            "Epoch 68 (train)3.5s: train loss = 0.03821 = (mf)0.02906 + (embed)0.00915\n",
            "Epoch 69 (train)3.5s: train loss = 0.03620 = (mf)0.02700 + (embed)0.00920\n",
            "Epoch 70 (train)4.3s + (eval)0.6s: train loss = 0.03761 = (mf)0.02834 + (embed)0.00927, recall = 0.14000, ndcg = 0.34123\n",
            "Epoch 71 (train)3.4s: train loss = 0.03635 = (mf)0.02704 + (embed)0.00931\n",
            "Epoch 72 (train)3.5s: train loss = 0.03577 = (mf)0.02641 + (embed)0.00936\n",
            "Epoch 73 (train)4.1s: train loss = 0.03696 = (mf)0.02755 + (embed)0.00941\n",
            "Epoch 74 (train)3.7s: train loss = 0.03721 = (mf)0.02774 + (embed)0.00946\n",
            "Epoch 75 (train)3.5s + (eval)0.2s: train loss = 0.03526 = (mf)0.02576 + (embed)0.00950, recall = 0.14209, ndcg = 0.34585\n",
            "Epoch 76 (train)3.7s: train loss = 0.03597 = (mf)0.02643 + (embed)0.00954\n",
            "Epoch 77 (train)4.2s: train loss = 0.03612 = (mf)0.02652 + (embed)0.00960\n",
            "Epoch 78 (train)3.5s: train loss = 0.03571 = (mf)0.02609 + (embed)0.00961\n",
            "Epoch 79 (train)3.5s: train loss = 0.03439 = (mf)0.02471 + (embed)0.00967\n",
            "Epoch 80 (train)4.3s + (eval)0.2s: train loss = 0.03431 = (mf)0.02460 + (embed)0.00972, recall = 0.14147, ndcg = 0.34413\n",
            "Epoch 81 (train)3.5s: train loss = 0.03438 = (mf)0.02462 + (embed)0.00975\n",
            "Epoch 82 (train)3.5s: train loss = 0.03473 = (mf)0.02494 + (embed)0.00979\n",
            "Epoch 83 (train)4.1s: train loss = 0.03378 = (mf)0.02395 + (embed)0.00983\n",
            "Epoch 84 (train)3.7s: train loss = 0.03431 = (mf)0.02444 + (embed)0.00987\n",
            "Epoch 85 (train)3.5s + (eval)0.6s: train loss = 0.03338 = (mf)0.02346 + (embed)0.00992, recall = 0.14395, ndcg = 0.34393\n",
            "Epoch 86 (train)3.9s: train loss = 0.03274 = (mf)0.02280 + (embed)0.00994\n",
            "Epoch 87 (train)4.1s: train loss = 0.03303 = (mf)0.02305 + (embed)0.00998\n",
            "Epoch 88 (train)3.5s: train loss = 0.03234 = (mf)0.02232 + (embed)0.01001\n",
            "Epoch 89 (train)3.5s: train loss = 0.03295 = (mf)0.02289 + (embed)0.01005\n",
            "Epoch 90 (train)4.4s + (eval)0.2s: train loss = 0.03168 = (mf)0.02162 + (embed)0.01006, recall = 0.14294, ndcg = 0.34289\n",
            "Epoch 91 (train)3.5s: train loss = 0.03224 = (mf)0.02214 + (embed)0.01010\n",
            "Epoch 92 (train)3.5s: train loss = 0.03231 = (mf)0.02219 + (embed)0.01012\n",
            "Epoch 93 (train)4.3s: train loss = 0.03164 = (mf)0.02150 + (embed)0.01015\n",
            "Epoch 94 (train)3.5s: train loss = 0.03207 = (mf)0.02187 + (embed)0.01020\n",
            "Epoch 95 (train)3.5s + (eval)0.2s: train loss = 0.03156 = (mf)0.02134 + (embed)0.01022, recall = 0.14557, ndcg = 0.34652\n",
            "Epoch 96 (train)3.9s: train loss = 0.03052 = (mf)0.02028 + (embed)0.01024\n",
            "Epoch 97 (train)3.9s: train loss = 0.03068 = (mf)0.02040 + (embed)0.01029\n",
            "Epoch 98 (train)3.5s: train loss = 0.03072 = (mf)0.02041 + (embed)0.01031\n",
            "Epoch 99 (train)3.5s: train loss = 0.02970 = (mf)0.01938 + (embed)0.01032\n",
            "Epoch 100 (train)4.3s + (eval)0.2s: train loss = 0.03084 = (mf)0.02048 + (embed)0.01036, recall = 0.14263, ndcg = 0.34077\n",
            "Training finished in 383.00 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.34077, Recall@10: 0.14263\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.9s: train loss = 0.41324 = (mf)0.41299 + (embed)0.00025\n",
            "Epoch 2 (train)4.2s: train loss = 0.23844 = (mf)0.23780 + (embed)0.00064\n",
            "Epoch 3 (train)4.4s: train loss = 0.22589 = (mf)0.22514 + (embed)0.00075\n",
            "Epoch 4 (train)4.7s: train loss = 0.21777 = (mf)0.21690 + (embed)0.00087\n",
            "Epoch 5 (train)4.1s + (eval)0.8s: train loss = 0.21110 = (mf)0.21007 + (embed)0.00102, recall = 0.07056, ndcg = 0.20461\n",
            "Epoch 6 (train)4.9s: train loss = 0.19928 = (mf)0.19807 + (embed)0.00121\n",
            "Epoch 7 (train)4.1s: train loss = 0.18821 = (mf)0.18679 + (embed)0.00142\n",
            "Epoch 8 (train)4.1s: train loss = 0.17479 = (mf)0.17315 + (embed)0.00164\n",
            "Epoch 9 (train)4.9s: train loss = 0.16252 = (mf)0.16064 + (embed)0.00188\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 0.14981 = (mf)0.14768 + (embed)0.00213, recall = 0.09399, ndcg = 0.25658\n",
            "Epoch 11 (train)4.1s: train loss = 0.14510 = (mf)0.14275 + (embed)0.00235\n",
            "Epoch 12 (train)4.9s: train loss = 0.13966 = (mf)0.13711 + (embed)0.00256\n",
            "Epoch 13 (train)4.1s: train loss = 0.13782 = (mf)0.13508 + (embed)0.00274\n",
            "Epoch 14 (train)4.1s: train loss = 0.13356 = (mf)0.13066 + (embed)0.00289\n",
            "Epoch 15 (train)5.0s + (eval)0.6s: train loss = 0.12878 = (mf)0.12572 + (embed)0.00306, recall = 0.10217, ndcg = 0.26481\n",
            "Epoch 16 (train)4.1s: train loss = 0.12741 = (mf)0.12417 + (embed)0.00323\n",
            "Epoch 17 (train)4.5s: train loss = 0.12450 = (mf)0.12111 + (embed)0.00338\n",
            "Epoch 18 (train)4.6s: train loss = 0.12167 = (mf)0.11811 + (embed)0.00357\n",
            "Epoch 19 (train)4.1s: train loss = 0.11730 = (mf)0.11355 + (embed)0.00375\n",
            "Epoch 20 (train)4.6s + (eval)0.3s: train loss = 0.11457 = (mf)0.11064 + (embed)0.00393, recall = 0.10637, ndcg = 0.27416\n",
            "Epoch 21 (train)4.3s: train loss = 0.10868 = (mf)0.10455 + (embed)0.00413\n",
            "Epoch 22 (train)4.1s: train loss = 0.10506 = (mf)0.10072 + (embed)0.00435\n",
            "Epoch 23 (train)4.9s: train loss = 0.10321 = (mf)0.09866 + (embed)0.00455\n",
            "Epoch 24 (train)4.1s: train loss = 0.10153 = (mf)0.09679 + (embed)0.00475\n",
            "Epoch 25 (train)4.2s + (eval)0.2s: train loss = 0.09724 = (mf)0.09230 + (embed)0.00494, recall = 0.11291, ndcg = 0.28643\n",
            "Epoch 26 (train)5.2s: train loss = 0.09381 = (mf)0.08869 + (embed)0.00512\n",
            "Epoch 27 (train)4.3s: train loss = 0.09173 = (mf)0.08642 + (embed)0.00531\n",
            "Epoch 28 (train)4.3s: train loss = 0.09137 = (mf)0.08585 + (embed)0.00552\n",
            "Epoch 29 (train)5.0s: train loss = 0.08751 = (mf)0.08181 + (embed)0.00570\n",
            "Epoch 30 (train)4.1s + (eval)0.6s: train loss = 0.08675 = (mf)0.08085 + (embed)0.00590, recall = 0.11511, ndcg = 0.29364\n",
            "Epoch 31 (train)4.6s: train loss = 0.08444 = (mf)0.07836 + (embed)0.00607\n",
            "Epoch 32 (train)4.4s: train loss = 0.08173 = (mf)0.07548 + (embed)0.00625\n",
            "Epoch 33 (train)4.1s: train loss = 0.07971 = (mf)0.07331 + (embed)0.00640\n",
            "Epoch 34 (train)4.8s: train loss = 0.07864 = (mf)0.07205 + (embed)0.00659\n",
            "Epoch 35 (train)4.2s + (eval)0.2s: train loss = 0.07961 = (mf)0.07286 + (embed)0.00675, recall = 0.12516, ndcg = 0.31202\n",
            "Epoch 36 (train)4.0s: train loss = 0.07522 = (mf)0.06827 + (embed)0.00694\n",
            "Epoch 37 (train)4.9s: train loss = 0.07388 = (mf)0.06676 + (embed)0.00712\n",
            "Epoch 38 (train)4.1s: train loss = 0.07285 = (mf)0.06560 + (embed)0.00725\n",
            "Epoch 39 (train)4.1s: train loss = 0.07023 = (mf)0.06279 + (embed)0.00744\n",
            "Epoch 40 (train)5.0s + (eval)0.2s: train loss = 0.06972 = (mf)0.06215 + (embed)0.00757, recall = 0.12861, ndcg = 0.32293\n",
            "Epoch 41 (train)4.1s: train loss = 0.07185 = (mf)0.06415 + (embed)0.00769\n",
            "Epoch 42 (train)4.0s: train loss = 0.06745 = (mf)0.05962 + (embed)0.00784\n",
            "Epoch 43 (train)4.9s: train loss = 0.06728 = (mf)0.05931 + (embed)0.00798\n",
            "Epoch 44 (train)4.0s: train loss = 0.06483 = (mf)0.05671 + (embed)0.00811\n",
            "Epoch 45 (train)4.0s + (eval)0.7s: train loss = 0.06479 = (mf)0.05659 + (embed)0.00820, recall = 0.13201, ndcg = 0.33234\n",
            "Epoch 46 (train)4.7s: train loss = 0.06379 = (mf)0.05542 + (embed)0.00837\n",
            "Epoch 47 (train)4.0s: train loss = 0.06447 = (mf)0.05600 + (embed)0.00847\n",
            "Epoch 48 (train)4.4s: train loss = 0.06197 = (mf)0.05338 + (embed)0.00860\n",
            "Epoch 49 (train)4.6s: train loss = 0.06136 = (mf)0.05264 + (embed)0.00872\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 0.06006 = (mf)0.05122 + (embed)0.00885, recall = 0.13244, ndcg = 0.33243\n",
            "Epoch 51 (train)4.9s: train loss = 0.05951 = (mf)0.05058 + (embed)0.00893\n",
            "Epoch 52 (train)4.3s: train loss = 0.05867 = (mf)0.04963 + (embed)0.00904\n",
            "Epoch 53 (train)4.1s: train loss = 0.05797 = (mf)0.04882 + (embed)0.00916\n",
            "Epoch 54 (train)4.9s: train loss = 0.05782 = (mf)0.04859 + (embed)0.00923\n",
            "Epoch 55 (train)4.1s + (eval)0.2s: train loss = 0.05704 = (mf)0.04767 + (embed)0.00936, recall = 0.13691, ndcg = 0.33586\n",
            "Epoch 56 (train)4.1s: train loss = 0.05548 = (mf)0.04603 + (embed)0.00945\n",
            "Epoch 57 (train)5.0s: train loss = 0.05456 = (mf)0.04498 + (embed)0.00957\n",
            "Epoch 58 (train)4.1s: train loss = 0.05323 = (mf)0.04357 + (embed)0.00966\n",
            "Epoch 59 (train)4.1s: train loss = 0.05328 = (mf)0.04351 + (embed)0.00976\n",
            "Epoch 60 (train)4.9s + (eval)0.6s: train loss = 0.05465 = (mf)0.04480 + (embed)0.00985, recall = 0.13750, ndcg = 0.34253\n",
            "Epoch 61 (train)4.2s: train loss = 0.05378 = (mf)0.04387 + (embed)0.00991\n",
            "Epoch 62 (train)4.4s: train loss = 0.05264 = (mf)0.04262 + (embed)0.01002\n",
            "Epoch 63 (train)4.7s: train loss = 0.05188 = (mf)0.04175 + (embed)0.01012\n",
            "Epoch 64 (train)4.2s: train loss = 0.05315 = (mf)0.04298 + (embed)0.01017\n",
            "Epoch 65 (train)4.7s + (eval)0.3s: train loss = 0.05054 = (mf)0.04032 + (embed)0.01023, recall = 0.13870, ndcg = 0.34479\n",
            "Epoch 66 (train)4.3s: train loss = 0.04864 = (mf)0.03828 + (embed)0.01036\n",
            "Epoch 67 (train)4.1s: train loss = 0.04917 = (mf)0.03875 + (embed)0.01042\n",
            "Epoch 68 (train)4.9s: train loss = 0.04986 = (mf)0.03938 + (embed)0.01048\n",
            "Epoch 69 (train)4.0s: train loss = 0.04833 = (mf)0.03778 + (embed)0.01055\n",
            "Epoch 70 (train)4.0s + (eval)0.2s: train loss = 0.04761 = (mf)0.03697 + (embed)0.01064, recall = 0.13969, ndcg = 0.34928\n",
            "Epoch 71 (train)4.9s: train loss = 0.04738 = (mf)0.03667 + (embed)0.01071\n",
            "Epoch 72 (train)4.1s: train loss = 0.04818 = (mf)0.03742 + (embed)0.01077\n",
            "Epoch 73 (train)4.1s: train loss = 0.04835 = (mf)0.03756 + (embed)0.01079\n",
            "Epoch 74 (train)4.9s: train loss = 0.04617 = (mf)0.03532 + (embed)0.01085\n",
            "Epoch 75 (train)4.1s + (eval)0.2s: train loss = 0.04700 = (mf)0.03609 + (embed)0.01091, recall = 0.14091, ndcg = 0.34897\n",
            "Epoch 76 (train)4.0s: train loss = 0.04474 = (mf)0.03374 + (embed)0.01100\n",
            "Epoch 77 (train)4.9s: train loss = 0.04462 = (mf)0.03359 + (embed)0.01103\n",
            "Epoch 78 (train)4.1s: train loss = 0.04602 = (mf)0.03490 + (embed)0.01112\n",
            "Epoch 79 (train)4.2s: train loss = 0.04680 = (mf)0.03565 + (embed)0.01115\n",
            "Epoch 80 (train)4.8s + (eval)0.6s: train loss = 0.04412 = (mf)0.03291 + (embed)0.01121, recall = 0.14026, ndcg = 0.35026\n",
            "Epoch 81 (train)4.0s: train loss = 0.04397 = (mf)0.03269 + (embed)0.01128\n",
            "Epoch 82 (train)4.6s: train loss = 0.04481 = (mf)0.03349 + (embed)0.01132\n",
            "Epoch 83 (train)4.3s: train loss = 0.04379 = (mf)0.03241 + (embed)0.01138\n",
            "Epoch 84 (train)4.0s: train loss = 0.04212 = (mf)0.03068 + (embed)0.01144\n",
            "Epoch 85 (train)4.7s + (eval)0.3s: train loss = 0.04301 = (mf)0.03155 + (embed)0.01146, recall = 0.14023, ndcg = 0.34746\n",
            "Epoch 86 (train)4.2s: train loss = 0.04278 = (mf)0.03129 + (embed)0.01149\n",
            "Epoch 87 (train)4.0s: train loss = 0.04158 = (mf)0.03002 + (embed)0.01156\n",
            "Epoch 88 (train)4.8s: train loss = 0.04127 = (mf)0.02966 + (embed)0.01161\n",
            "Epoch 89 (train)4.0s: train loss = 0.04123 = (mf)0.02959 + (embed)0.01164\n",
            "Epoch 90 (train)4.0s + (eval)0.2s: train loss = 0.04024 = (mf)0.02854 + (embed)0.01170, recall = 0.14099, ndcg = 0.34908\n",
            "Epoch 91 (train)4.9s: train loss = 0.04097 = (mf)0.02927 + (embed)0.01170\n",
            "Epoch 92 (train)4.0s: train loss = 0.04181 = (mf)0.03008 + (embed)0.01174\n",
            "Epoch 93 (train)4.1s: train loss = 0.04106 = (mf)0.02927 + (embed)0.01180\n",
            "Epoch 94 (train)4.9s: train loss = 0.04070 = (mf)0.02884 + (embed)0.01185\n",
            "Epoch 95 (train)4.0s + (eval)0.6s: train loss = 0.04030 = (mf)0.02846 + (embed)0.01184, recall = 0.14340, ndcg = 0.34985\n",
            "Epoch 96 (train)4.0s: train loss = 0.03971 = (mf)0.02781 + (embed)0.01191\n",
            "Epoch 97 (train)4.9s: train loss = 0.03951 = (mf)0.02758 + (embed)0.01193\n",
            "Epoch 98 (train)4.1s: train loss = 0.03936 = (mf)0.02739 + (embed)0.01197\n",
            "Epoch 99 (train)4.2s: train loss = 0.03997 = (mf)0.02796 + (embed)0.01201\n",
            "Epoch 100 (train)4.8s + (eval)0.2s: train loss = 0.03825 = (mf)0.02619 + (embed)0.01206, recall = 0.14723, ndcg = 0.35768\n",
            "Training finished in 446.96 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.35768, Recall@10: 0.14723\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.8s: train loss = 0.21742 = (mf)0.21581 + (embed)0.00161\n",
            "Epoch 2 (train)3.0s: train loss = 0.10606 = (mf)0.10240 + (embed)0.00367\n",
            "Epoch 3 (train)3.0s: train loss = 0.07628 = (mf)0.07083 + (embed)0.00545\n",
            "Epoch 4 (train)3.4s: train loss = 0.06231 = (mf)0.05525 + (embed)0.00706\n",
            "Epoch 5 (train)3.3s + (eval)0.8s: train loss = 0.05757 = (mf)0.04926 + (embed)0.00832, recall = 0.10670, ndcg = 0.25863\n",
            "Epoch 6 (train)3.0s: train loss = 0.05259 = (mf)0.04306 + (embed)0.00952\n",
            "Epoch 7 (train)3.0s: train loss = 0.04760 = (mf)0.03700 + (embed)0.01060\n",
            "Epoch 8 (train)3.8s: train loss = 0.04633 = (mf)0.03476 + (embed)0.01157\n",
            "Epoch 9 (train)3.0s: train loss = 0.04827 = (mf)0.03594 + (embed)0.01234\n",
            "Epoch 10 (train)3.0s + (eval)0.2s: train loss = 0.04550 = (mf)0.03239 + (embed)0.01311, recall = 0.10514, ndcg = 0.25196\n",
            "Epoch 11 (train)3.0s: train loss = 0.04808 = (mf)0.03427 + (embed)0.01381\n",
            "Epoch 12 (train)3.9s: train loss = 0.04451 = (mf)0.03002 + (embed)0.01449\n",
            "Epoch 13 (train)3.0s: train loss = 0.04526 = (mf)0.03017 + (embed)0.01509\n",
            "Epoch 14 (train)2.9s: train loss = 0.04398 = (mf)0.02829 + (embed)0.01569\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 0.04763 = (mf)0.03146 + (embed)0.01617, recall = 0.10039, ndcg = 0.23114\n",
            "Epoch 16 (train)3.8s: train loss = 0.04564 = (mf)0.02894 + (embed)0.01669\n",
            "Epoch 17 (train)3.0s: train loss = 0.04524 = (mf)0.02814 + (embed)0.01710\n",
            "Epoch 18 (train)2.9s: train loss = 0.04656 = (mf)0.02897 + (embed)0.01760\n",
            "Epoch 19 (train)3.0s: train loss = 0.04761 = (mf)0.02955 + (embed)0.01806\n",
            "Epoch 20 (train)3.8s + (eval)0.6s: train loss = 0.04623 = (mf)0.02774 + (embed)0.01848, recall = 0.09383, ndcg = 0.22732\n",
            "Epoch 21 (train)2.9s: train loss = 0.04958 = (mf)0.03070 + (embed)0.01888\n",
            "Epoch 22 (train)2.9s: train loss = 0.04844 = (mf)0.02908 + (embed)0.01936\n",
            "Epoch 23 (train)3.1s: train loss = 0.04945 = (mf)0.02971 + (embed)0.01974\n",
            "Epoch 24 (train)3.7s: train loss = 0.04786 = (mf)0.02778 + (embed)0.02008\n",
            "Epoch 25 (train)2.9s + (eval)0.2s: train loss = 0.04755 = (mf)0.02713 + (embed)0.02042, recall = 0.09414, ndcg = 0.22396\n",
            "Epoch 26 (train)2.9s: train loss = 0.04783 = (mf)0.02704 + (embed)0.02080\n",
            "Epoch 27 (train)3.2s: train loss = 0.04940 = (mf)0.02825 + (embed)0.02115\n",
            "Epoch 28 (train)3.6s: train loss = 0.05174 = (mf)0.03035 + (embed)0.02139\n",
            "Epoch 29 (train)3.0s: train loss = 0.04991 = (mf)0.02825 + (embed)0.02166\n",
            "Epoch 30 (train)2.9s + (eval)0.2s: train loss = 0.04858 = (mf)0.02659 + (embed)0.02199, recall = 0.08907, ndcg = 0.20601\n",
            "Epoch 31 (train)3.2s: train loss = 0.04678 = (mf)0.02456 + (embed)0.02222\n",
            "Epoch 32 (train)3.4s: train loss = 0.04820 = (mf)0.02577 + (embed)0.02243\n",
            "Epoch 33 (train)2.9s: train loss = 0.05279 = (mf)0.03005 + (embed)0.02274\n",
            "Epoch 34 (train)2.9s: train loss = 0.04798 = (mf)0.02509 + (embed)0.02290\n",
            "Epoch 35 (train)3.2s + (eval)0.7s: train loss = 0.05027 = (mf)0.02714 + (embed)0.02313, recall = 0.09606, ndcg = 0.21896\n",
            "Epoch 36 (train)3.2s: train loss = 0.05056 = (mf)0.02713 + (embed)0.02343\n",
            "Epoch 37 (train)2.9s: train loss = 0.05171 = (mf)0.02810 + (embed)0.02361\n",
            "Epoch 38 (train)3.0s: train loss = 0.05031 = (mf)0.02645 + (embed)0.02386\n",
            "Epoch 39 (train)3.4s: train loss = 0.05097 = (mf)0.02696 + (embed)0.02402\n",
            "Epoch 40 (train)3.3s + (eval)0.2s: train loss = 0.05261 = (mf)0.02822 + (embed)0.02439, recall = 0.08621, ndcg = 0.19866\n",
            "Epoch 41 (train)2.9s: train loss = 0.05073 = (mf)0.02612 + (embed)0.02460\n",
            "Epoch 42 (train)2.9s: train loss = 0.05363 = (mf)0.02887 + (embed)0.02476\n",
            "Epoch 43 (train)3.4s: train loss = 0.05329 = (mf)0.02835 + (embed)0.02494\n",
            "Epoch 44 (train)3.3s: train loss = 0.05368 = (mf)0.02849 + (embed)0.02520\n",
            "Epoch 45 (train)2.9s + (eval)0.2s: train loss = 0.05035 = (mf)0.02501 + (embed)0.02534, recall = 0.08771, ndcg = 0.20555\n",
            "Epoch 46 (train)2.9s: train loss = 0.05130 = (mf)0.02579 + (embed)0.02551\n",
            "Epoch 47 (train)3.5s: train loss = 0.05395 = (mf)0.02820 + (embed)0.02575\n",
            "Epoch 48 (train)3.2s: train loss = 0.05534 = (mf)0.02939 + (embed)0.02595\n",
            "Epoch 49 (train)2.9s: train loss = 0.05601 = (mf)0.02983 + (embed)0.02618\n",
            "Epoch 50 (train)2.9s + (eval)0.6s: train loss = 0.05479 = (mf)0.02842 + (embed)0.02637, recall = 0.08773, ndcg = 0.20122\n",
            "Epoch 51 (train)3.6s: train loss = 0.05475 = (mf)0.02827 + (embed)0.02647\n",
            "Epoch 52 (train)3.0s: train loss = 0.05401 = (mf)0.02737 + (embed)0.02664\n",
            "Epoch 53 (train)2.9s: train loss = 0.05205 = (mf)0.02537 + (embed)0.02668\n",
            "Epoch 54 (train)3.0s: train loss = 0.05370 = (mf)0.02695 + (embed)0.02675\n",
            "Epoch 55 (train)3.7s + (eval)0.3s: train loss = 0.05158 = (mf)0.02471 + (embed)0.02688, recall = 0.09058, ndcg = 0.21589\n",
            "Epoch 56 (train)3.0s: train loss = 0.05192 = (mf)0.02494 + (embed)0.02698\n",
            "Epoch 57 (train)2.9s: train loss = 0.05432 = (mf)0.02724 + (embed)0.02708\n",
            "Epoch 58 (train)2.9s: train loss = 0.05444 = (mf)0.02730 + (embed)0.02714\n",
            "Epoch 59 (train)3.8s: train loss = 0.05288 = (mf)0.02570 + (embed)0.02718\n",
            "Epoch 60 (train)2.9s + (eval)0.2s: train loss = 0.05250 = (mf)0.02516 + (embed)0.02733, recall = 0.09145, ndcg = 0.21798\n",
            "Epoch 61 (train)2.9s: train loss = 0.05448 = (mf)0.02710 + (embed)0.02738\n",
            "Epoch 62 (train)3.0s: train loss = 0.05297 = (mf)0.02554 + (embed)0.02743\n",
            "Epoch 63 (train)3.8s: train loss = 0.05552 = (mf)0.02797 + (embed)0.02755\n",
            "Epoch 64 (train)3.0s: train loss = 0.05308 = (mf)0.02544 + (embed)0.02763\n",
            "Epoch 65 (train)3.0s + (eval)0.2s: train loss = 0.05385 = (mf)0.02611 + (embed)0.02774, recall = 0.08771, ndcg = 0.21445\n",
            "Epoch 66 (train)3.0s: train loss = 0.05502 = (mf)0.02716 + (embed)0.02787\n",
            "Epoch 67 (train)3.8s: train loss = 0.05310 = (mf)0.02508 + (embed)0.02801\n",
            "Epoch 68 (train)2.9s: train loss = 0.05400 = (mf)0.02603 + (embed)0.02797\n",
            "Epoch 69 (train)3.0s: train loss = 0.05314 = (mf)0.02516 + (embed)0.02798\n",
            "Epoch 70 (train)2.9s + (eval)0.5s: train loss = 0.05542 = (mf)0.02731 + (embed)0.02811, recall = 0.09037, ndcg = 0.21230\n",
            "Epoch 71 (train)3.8s: train loss = 0.05441 = (mf)0.02619 + (embed)0.02823\n",
            "Epoch 72 (train)2.9s: train loss = 0.05327 = (mf)0.02503 + (embed)0.02824\n",
            "Epoch 73 (train)2.9s: train loss = 0.05352 = (mf)0.02517 + (embed)0.02835\n",
            "Epoch 74 (train)3.0s: train loss = 0.05301 = (mf)0.02461 + (embed)0.02840\n",
            "Epoch 75 (train)3.9s + (eval)0.2s: train loss = 0.05053 = (mf)0.02217 + (embed)0.02835, recall = 0.09732, ndcg = 0.22468\n",
            "Epoch 76 (train)3.0s: train loss = 0.05351 = (mf)0.02513 + (embed)0.02838\n",
            "Epoch 77 (train)2.9s: train loss = 0.05240 = (mf)0.02399 + (embed)0.02841\n",
            "Epoch 78 (train)3.0s: train loss = 0.05207 = (mf)0.02368 + (embed)0.02839\n",
            "Epoch 79 (train)3.7s: train loss = 0.05346 = (mf)0.02504 + (embed)0.02842\n",
            "Epoch 80 (train)2.9s + (eval)0.2s: train loss = 0.05370 = (mf)0.02514 + (embed)0.02856, recall = 0.09355, ndcg = 0.22098\n",
            "Epoch 81 (train)3.0s: train loss = 0.05118 = (mf)0.02263 + (embed)0.02855\n",
            "Epoch 82 (train)3.1s: train loss = 0.05366 = (mf)0.02492 + (embed)0.02873\n",
            "Epoch 83 (train)3.6s: train loss = 0.05160 = (mf)0.02283 + (embed)0.02877\n",
            "Epoch 84 (train)2.9s: train loss = 0.05372 = (mf)0.02492 + (embed)0.02880\n",
            "Epoch 85 (train)2.9s + (eval)0.6s: train loss = 0.05502 = (mf)0.02617 + (embed)0.02885, recall = 0.08322, ndcg = 0.19497\n",
            "Epoch 86 (train)3.3s: train loss = 0.05320 = (mf)0.02430 + (embed)0.02891\n",
            "Epoch 87 (train)3.4s: train loss = 0.05142 = (mf)0.02240 + (embed)0.02902\n",
            "Epoch 88 (train)2.9s: train loss = 0.05334 = (mf)0.02424 + (embed)0.02911\n",
            "Epoch 89 (train)2.9s: train loss = 0.05180 = (mf)0.02265 + (embed)0.02914\n",
            "Epoch 90 (train)3.2s + (eval)0.3s: train loss = 0.05237 = (mf)0.02320 + (embed)0.02918, recall = 0.08581, ndcg = 0.20304\n",
            "Epoch 91 (train)3.3s: train loss = 0.05202 = (mf)0.02287 + (embed)0.02916\n",
            "Epoch 92 (train)2.9s: train loss = 0.05227 = (mf)0.02298 + (embed)0.02929\n",
            "Epoch 93 (train)2.9s: train loss = 0.05124 = (mf)0.02195 + (embed)0.02928\n",
            "Epoch 94 (train)3.3s: train loss = 0.05269 = (mf)0.02332 + (embed)0.02937\n",
            "Epoch 95 (train)3.4s + (eval)0.2s: train loss = 0.05423 = (mf)0.02489 + (embed)0.02934, recall = 0.09458, ndcg = 0.22160\n",
            "Epoch 96 (train)2.9s: train loss = 0.05338 = (mf)0.02409 + (embed)0.02929\n",
            "Epoch 97 (train)3.0s: train loss = 0.05742 = (mf)0.02807 + (embed)0.02935\n",
            "Epoch 98 (train)3.5s: train loss = 0.05543 = (mf)0.02609 + (embed)0.02934\n",
            "Epoch 99 (train)3.3s: train loss = 0.05078 = (mf)0.02132 + (embed)0.02947\n",
            "Epoch 100 (train)3.0s + (eval)0.6s: train loss = 0.05347 = (mf)0.02409 + (embed)0.02938, recall = 0.09183, ndcg = 0.21611\n",
            "Training finished in 323.37 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.21611, Recall@10: 0.09183\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.5s: train loss = 0.22714 = (mf)0.22554 + (embed)0.00161\n",
            "Epoch 2 (train)3.5s: train loss = 0.11995 = (mf)0.11629 + (embed)0.00366\n",
            "Epoch 3 (train)4.3s: train loss = 0.09047 = (mf)0.08495 + (embed)0.00552\n",
            "Epoch 4 (train)3.5s: train loss = 0.07485 = (mf)0.06757 + (embed)0.00728\n",
            "Epoch 5 (train)3.5s + (eval)0.8s: train loss = 0.06461 = (mf)0.05577 + (embed)0.00884, recall = 0.11593, ndcg = 0.28698\n",
            "Epoch 6 (train)4.3s: train loss = 0.05574 = (mf)0.04556 + (embed)0.01018\n",
            "Epoch 7 (train)3.5s: train loss = 0.05230 = (mf)0.04106 + (embed)0.01124\n",
            "Epoch 8 (train)3.6s: train loss = 0.05041 = (mf)0.03822 + (embed)0.01219\n",
            "Epoch 9 (train)4.0s: train loss = 0.04799 = (mf)0.03489 + (embed)0.01310\n",
            "Epoch 10 (train)3.8s + (eval)0.6s: train loss = 0.04547 = (mf)0.03161 + (embed)0.01386, recall = 0.11600, ndcg = 0.27788\n",
            "Epoch 11 (train)3.5s: train loss = 0.04388 = (mf)0.02936 + (embed)0.01452\n",
            "Epoch 12 (train)3.6s: train loss = 0.04172 = (mf)0.02661 + (embed)0.01511\n",
            "Epoch 13 (train)4.2s: train loss = 0.04182 = (mf)0.02618 + (embed)0.01564\n",
            "Epoch 14 (train)3.5s: train loss = 0.04405 = (mf)0.02800 + (embed)0.01605\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.04215 = (mf)0.02565 + (embed)0.01650, recall = 0.11123, ndcg = 0.25960\n",
            "Epoch 16 (train)4.4s: train loss = 0.04439 = (mf)0.02738 + (embed)0.01701\n",
            "Epoch 17 (train)3.5s: train loss = 0.04149 = (mf)0.02401 + (embed)0.01748\n",
            "Epoch 18 (train)3.6s: train loss = 0.04262 = (mf)0.02483 + (embed)0.01778\n",
            "Epoch 19 (train)4.5s: train loss = 0.04200 = (mf)0.02389 + (embed)0.01811\n",
            "Epoch 20 (train)3.7s + (eval)0.2s: train loss = 0.04224 = (mf)0.02379 + (embed)0.01845, recall = 0.11409, ndcg = 0.26755\n",
            "Epoch 21 (train)3.8s: train loss = 0.04054 = (mf)0.02186 + (embed)0.01868\n",
            "Epoch 22 (train)4.5s: train loss = 0.04151 = (mf)0.02264 + (embed)0.01888\n",
            "Epoch 23 (train)3.7s: train loss = 0.04080 = (mf)0.02172 + (embed)0.01908\n",
            "Epoch 24 (train)3.6s: train loss = 0.04049 = (mf)0.02116 + (embed)0.01933\n",
            "Epoch 25 (train)3.8s + (eval)0.7s: train loss = 0.04117 = (mf)0.02162 + (embed)0.01956, recall = 0.11286, ndcg = 0.24912\n",
            "Epoch 26 (train)4.0s: train loss = 0.04041 = (mf)0.02066 + (embed)0.01976\n",
            "Epoch 27 (train)3.7s: train loss = 0.04072 = (mf)0.02074 + (embed)0.01997\n",
            "Epoch 28 (train)3.9s: train loss = 0.04044 = (mf)0.02016 + (embed)0.02028\n",
            "Epoch 29 (train)4.4s: train loss = 0.04033 = (mf)0.01994 + (embed)0.02039\n",
            "Epoch 30 (train)3.6s + (eval)0.2s: train loss = 0.04070 = (mf)0.02014 + (embed)0.02056, recall = 0.10914, ndcg = 0.25216\n",
            "Epoch 31 (train)3.6s: train loss = 0.04124 = (mf)0.02052 + (embed)0.02071\n",
            "Epoch 32 (train)4.3s: train loss = 0.03996 = (mf)0.01905 + (embed)0.02091\n",
            "Epoch 33 (train)3.5s: train loss = 0.04146 = (mf)0.02044 + (embed)0.02101\n",
            "Epoch 34 (train)3.5s: train loss = 0.04117 = (mf)0.01996 + (embed)0.02122\n",
            "Epoch 35 (train)4.4s + (eval)0.2s: train loss = 0.04211 = (mf)0.02075 + (embed)0.02136, recall = 0.11087, ndcg = 0.25205\n",
            "Epoch 36 (train)3.4s: train loss = 0.04167 = (mf)0.02013 + (embed)0.02153\n",
            "Epoch 37 (train)3.5s: train loss = 0.04092 = (mf)0.01920 + (embed)0.02172\n",
            "Epoch 38 (train)3.9s: train loss = 0.04086 = (mf)0.01909 + (embed)0.02177\n",
            "Epoch 39 (train)4.0s: train loss = 0.04164 = (mf)0.01966 + (embed)0.02198\n",
            "Epoch 40 (train)3.5s + (eval)0.2s: train loss = 0.04058 = (mf)0.01842 + (embed)0.02216, recall = 0.10857, ndcg = 0.24655\n",
            "Epoch 41 (train)3.5s: train loss = 0.04014 = (mf)0.01777 + (embed)0.02237\n",
            "Epoch 42 (train)4.3s: train loss = 0.04258 = (mf)0.02006 + (embed)0.02252\n",
            "Epoch 43 (train)3.5s: train loss = 0.03879 = (mf)0.01609 + (embed)0.02270\n",
            "Epoch 44 (train)3.5s: train loss = 0.04238 = (mf)0.01964 + (embed)0.02274\n",
            "Epoch 45 (train)4.4s + (eval)0.6s: train loss = 0.04317 = (mf)0.02039 + (embed)0.02278, recall = 0.10984, ndcg = 0.25000\n",
            "Epoch 46 (train)3.5s: train loss = 0.04112 = (mf)0.01828 + (embed)0.02285\n",
            "Epoch 47 (train)3.5s: train loss = 0.04173 = (mf)0.01882 + (embed)0.02291\n",
            "Epoch 48 (train)4.1s: train loss = 0.04094 = (mf)0.01792 + (embed)0.02302\n",
            "Epoch 49 (train)3.7s: train loss = 0.04127 = (mf)0.01821 + (embed)0.02306\n",
            "Epoch 50 (train)3.5s + (eval)0.2s: train loss = 0.04055 = (mf)0.01749 + (embed)0.02306, recall = 0.10947, ndcg = 0.25155\n",
            "Epoch 51 (train)3.6s: train loss = 0.04036 = (mf)0.01723 + (embed)0.02313\n",
            "Epoch 52 (train)4.2s: train loss = 0.04065 = (mf)0.01749 + (embed)0.02316\n",
            "Epoch 53 (train)3.4s: train loss = 0.04235 = (mf)0.01909 + (embed)0.02326\n",
            "Epoch 54 (train)3.5s: train loss = 0.03975 = (mf)0.01640 + (embed)0.02335\n",
            "Epoch 55 (train)4.4s + (eval)0.2s: train loss = 0.04081 = (mf)0.01749 + (embed)0.02332, recall = 0.11024, ndcg = 0.24614\n",
            "Epoch 56 (train)3.5s: train loss = 0.04338 = (mf)0.01995 + (embed)0.02342\n",
            "Epoch 57 (train)3.5s: train loss = 0.04199 = (mf)0.01851 + (embed)0.02349\n",
            "Epoch 58 (train)4.2s: train loss = 0.04115 = (mf)0.01751 + (embed)0.02364\n",
            "Epoch 59 (train)3.8s: train loss = 0.04125 = (mf)0.01758 + (embed)0.02367\n",
            "Epoch 60 (train)3.5s + (eval)0.6s: train loss = 0.04276 = (mf)0.01894 + (embed)0.02382, recall = 0.11113, ndcg = 0.24838\n",
            "Epoch 61 (train)3.8s: train loss = 0.04087 = (mf)0.01704 + (embed)0.02383\n",
            "Epoch 62 (train)4.0s: train loss = 0.04166 = (mf)0.01779 + (embed)0.02387\n",
            "Epoch 63 (train)3.5s: train loss = 0.04185 = (mf)0.01795 + (embed)0.02390\n",
            "Epoch 64 (train)3.5s: train loss = 0.04167 = (mf)0.01764 + (embed)0.02402\n",
            "Epoch 65 (train)4.3s + (eval)0.2s: train loss = 0.04266 = (mf)0.01852 + (embed)0.02415, recall = 0.10638, ndcg = 0.24093\n",
            "Epoch 66 (train)3.5s: train loss = 0.04183 = (mf)0.01756 + (embed)0.02428\n",
            "Epoch 67 (train)3.5s: train loss = 0.04107 = (mf)0.01680 + (embed)0.02427\n",
            "Epoch 68 (train)4.4s: train loss = 0.04285 = (mf)0.01851 + (embed)0.02434\n",
            "Epoch 69 (train)3.6s: train loss = 0.04123 = (mf)0.01683 + (embed)0.02439\n",
            "Epoch 70 (train)3.5s + (eval)0.2s: train loss = 0.04166 = (mf)0.01723 + (embed)0.02443, recall = 0.10061, ndcg = 0.22919\n",
            "Epoch 71 (train)4.0s: train loss = 0.04127 = (mf)0.01685 + (embed)0.02442\n",
            "Epoch 72 (train)3.8s: train loss = 0.04246 = (mf)0.01802 + (embed)0.02445\n",
            "Epoch 73 (train)3.5s: train loss = 0.04213 = (mf)0.01761 + (embed)0.02452\n",
            "Epoch 74 (train)3.5s: train loss = 0.04196 = (mf)0.01744 + (embed)0.02452\n",
            "Epoch 75 (train)4.3s + (eval)0.6s: train loss = 0.04124 = (mf)0.01657 + (embed)0.02467, recall = 0.10969, ndcg = 0.24298\n",
            "Epoch 76 (train)3.5s: train loss = 0.04229 = (mf)0.01763 + (embed)0.02466\n",
            "Epoch 77 (train)3.5s: train loss = 0.04279 = (mf)0.01815 + (embed)0.02463\n",
            "Epoch 78 (train)4.3s: train loss = 0.04128 = (mf)0.01658 + (embed)0.02470\n",
            "Epoch 79 (train)3.5s: train loss = 0.04234 = (mf)0.01760 + (embed)0.02475\n",
            "Epoch 80 (train)3.5s + (eval)0.2s: train loss = 0.04142 = (mf)0.01669 + (embed)0.02473, recall = 0.10767, ndcg = 0.24202\n",
            "Epoch 81 (train)4.2s: train loss = 0.04139 = (mf)0.01671 + (embed)0.02468\n",
            "Epoch 82 (train)3.6s: train loss = 0.04137 = (mf)0.01668 + (embed)0.02469\n",
            "Epoch 83 (train)3.5s: train loss = 0.04102 = (mf)0.01640 + (embed)0.02462\n",
            "Epoch 84 (train)3.7s: train loss = 0.04167 = (mf)0.01707 + (embed)0.02460\n",
            "Epoch 85 (train)4.2s + (eval)0.2s: train loss = 0.04256 = (mf)0.01787 + (embed)0.02469, recall = 0.10899, ndcg = 0.23471\n",
            "Epoch 86 (train)3.5s: train loss = 0.04008 = (mf)0.01539 + (embed)0.02469\n",
            "Epoch 87 (train)3.6s: train loss = 0.04085 = (mf)0.01614 + (embed)0.02471\n",
            "Epoch 88 (train)4.3s: train loss = 0.04234 = (mf)0.01751 + (embed)0.02482\n",
            "Epoch 89 (train)3.4s: train loss = 0.04149 = (mf)0.01668 + (embed)0.02481\n",
            "Epoch 90 (train)3.4s + (eval)0.2s: train loss = 0.04000 = (mf)0.01522 + (embed)0.02478, recall = 0.10772, ndcg = 0.24428\n",
            "Epoch 91 (train)4.2s: train loss = 0.04156 = (mf)0.01683 + (embed)0.02473\n",
            "Epoch 92 (train)3.5s: train loss = 0.04110 = (mf)0.01637 + (embed)0.02472\n",
            "Epoch 93 (train)3.5s: train loss = 0.03984 = (mf)0.01510 + (embed)0.02474\n",
            "Epoch 94 (train)3.6s: train loss = 0.04067 = (mf)0.01599 + (embed)0.02468\n",
            "Epoch 95 (train)4.1s + (eval)0.6s: train loss = 0.04045 = (mf)0.01583 + (embed)0.02463, recall = 0.10694, ndcg = 0.24354\n",
            "Epoch 96 (train)3.5s: train loss = 0.04109 = (mf)0.01637 + (embed)0.02472\n",
            "Epoch 97 (train)3.5s: train loss = 0.04186 = (mf)0.01711 + (embed)0.02475\n",
            "Epoch 98 (train)4.3s: train loss = 0.03940 = (mf)0.01464 + (embed)0.02476\n",
            "Epoch 99 (train)3.5s: train loss = 0.04149 = (mf)0.01674 + (embed)0.02475\n",
            "Epoch 100 (train)3.4s + (eval)0.2s: train loss = 0.04199 = (mf)0.01709 + (embed)0.02490, recall = 0.10285, ndcg = 0.24070\n",
            "Training finished in 384.41 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.24070, Recall@10: 0.10285\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.2s: train loss = 0.24274 = (mf)0.24117 + (embed)0.00157\n",
            "Epoch 2 (train)4.8s: train loss = 0.13699 = (mf)0.13313 + (embed)0.00386\n",
            "Epoch 3 (train)4.1s: train loss = 0.10953 = (mf)0.10368 + (embed)0.00585\n",
            "Epoch 4 (train)4.1s: train loss = 0.08434 = (mf)0.07644 + (embed)0.00790\n",
            "Epoch 5 (train)4.9s + (eval)0.9s: train loss = 0.07306 = (mf)0.06341 + (embed)0.00965, recall = 0.11291, ndcg = 0.27972\n",
            "Epoch 6 (train)4.0s: train loss = 0.06957 = (mf)0.05849 + (embed)0.01109\n",
            "Epoch 7 (train)4.7s: train loss = 0.06240 = (mf)0.05009 + (embed)0.01230\n",
            "Epoch 8 (train)4.5s: train loss = 0.05816 = (mf)0.04485 + (embed)0.01330\n",
            "Epoch 9 (train)4.1s: train loss = 0.05588 = (mf)0.04159 + (embed)0.01429\n",
            "Epoch 10 (train)4.9s + (eval)0.2s: train loss = 0.05338 = (mf)0.03830 + (embed)0.01508, recall = 0.12203, ndcg = 0.30079\n",
            "Epoch 11 (train)4.1s: train loss = 0.05255 = (mf)0.03672 + (embed)0.01583\n",
            "Epoch 12 (train)4.1s: train loss = 0.04985 = (mf)0.03343 + (embed)0.01642\n",
            "Epoch 13 (train)5.0s: train loss = 0.04582 = (mf)0.02891 + (embed)0.01691\n",
            "Epoch 14 (train)4.2s: train loss = 0.04802 = (mf)0.03069 + (embed)0.01733\n",
            "Epoch 15 (train)4.2s + (eval)0.2s: train loss = 0.04788 = (mf)0.03002 + (embed)0.01786, recall = 0.12225, ndcg = 0.29036\n",
            "Epoch 16 (train)4.9s: train loss = 0.04619 = (mf)0.02794 + (embed)0.01825\n",
            "Epoch 17 (train)4.2s: train loss = 0.04437 = (mf)0.02579 + (embed)0.01859\n",
            "Epoch 18 (train)4.1s: train loss = 0.04515 = (mf)0.02622 + (embed)0.01893\n",
            "Epoch 19 (train)4.9s: train loss = 0.04497 = (mf)0.02572 + (embed)0.01924\n",
            "Epoch 20 (train)4.2s + (eval)0.6s: train loss = 0.04372 = (mf)0.02423 + (embed)0.01949, recall = 0.11859, ndcg = 0.27854\n",
            "Epoch 21 (train)4.6s: train loss = 0.04430 = (mf)0.02453 + (embed)0.01977\n",
            "Epoch 22 (train)4.4s: train loss = 0.04309 = (mf)0.02309 + (embed)0.02000\n",
            "Epoch 23 (train)4.1s: train loss = 0.04196 = (mf)0.02181 + (embed)0.02015\n",
            "Epoch 24 (train)4.8s: train loss = 0.04254 = (mf)0.02224 + (embed)0.02030\n",
            "Epoch 25 (train)4.1s + (eval)0.2s: train loss = 0.04181 = (mf)0.02132 + (embed)0.02049, recall = 0.12206, ndcg = 0.28570\n",
            "Epoch 26 (train)4.0s: train loss = 0.04191 = (mf)0.02124 + (embed)0.02067\n",
            "Epoch 27 (train)4.9s: train loss = 0.04046 = (mf)0.01959 + (embed)0.02087\n",
            "Epoch 28 (train)4.1s: train loss = 0.04166 = (mf)0.02066 + (embed)0.02100\n",
            "Epoch 29 (train)4.1s: train loss = 0.04194 = (mf)0.02075 + (embed)0.02120\n",
            "Epoch 30 (train)4.9s + (eval)0.2s: train loss = 0.04090 = (mf)0.01956 + (embed)0.02134, recall = 0.12222, ndcg = 0.28161\n",
            "Epoch 31 (train)4.1s: train loss = 0.04087 = (mf)0.01947 + (embed)0.02140\n",
            "Epoch 32 (train)4.1s: train loss = 0.04150 = (mf)0.01993 + (embed)0.02157\n",
            "Epoch 33 (train)4.9s: train loss = 0.04204 = (mf)0.02027 + (embed)0.02177\n",
            "Epoch 34 (train)4.1s: train loss = 0.04141 = (mf)0.01949 + (embed)0.02192\n",
            "Epoch 35 (train)4.1s + (eval)0.6s: train loss = 0.04150 = (mf)0.01943 + (embed)0.02207, recall = 0.12181, ndcg = 0.28028\n",
            "Epoch 36 (train)4.9s: train loss = 0.04142 = (mf)0.01922 + (embed)0.02220\n",
            "Epoch 37 (train)4.1s: train loss = 0.04097 = (mf)0.01866 + (embed)0.02231\n",
            "Epoch 38 (train)4.2s: train loss = 0.04132 = (mf)0.01894 + (embed)0.02238\n",
            "Epoch 39 (train)4.7s: train loss = 0.03974 = (mf)0.01728 + (embed)0.02246\n",
            "Epoch 40 (train)4.1s + (eval)0.2s: train loss = 0.04092 = (mf)0.01839 + (embed)0.02253, recall = 0.12585, ndcg = 0.29134\n",
            "Epoch 41 (train)4.5s: train loss = 0.03983 = (mf)0.01725 + (embed)0.02258\n",
            "Epoch 42 (train)4.3s: train loss = 0.04115 = (mf)0.01845 + (embed)0.02270\n",
            "Epoch 43 (train)4.1s: train loss = 0.04101 = (mf)0.01822 + (embed)0.02279\n",
            "Epoch 44 (train)4.7s: train loss = 0.04090 = (mf)0.01796 + (embed)0.02294\n",
            "Epoch 45 (train)4.2s + (eval)0.2s: train loss = 0.04170 = (mf)0.01869 + (embed)0.02301, recall = 0.12177, ndcg = 0.28007\n",
            "Epoch 46 (train)4.1s: train loss = 0.04047 = (mf)0.01732 + (embed)0.02315\n",
            "Epoch 47 (train)4.9s: train loss = 0.04089 = (mf)0.01770 + (embed)0.02319\n",
            "Epoch 48 (train)4.1s: train loss = 0.04064 = (mf)0.01740 + (embed)0.02324\n",
            "Epoch 49 (train)4.1s: train loss = 0.04088 = (mf)0.01754 + (embed)0.02334\n",
            "Epoch 50 (train)4.9s + (eval)0.6s: train loss = 0.04051 = (mf)0.01711 + (embed)0.02340, recall = 0.12964, ndcg = 0.28960\n",
            "Epoch 51 (train)4.1s: train loss = 0.04157 = (mf)0.01807 + (embed)0.02350\n",
            "Epoch 52 (train)4.1s: train loss = 0.04059 = (mf)0.01701 + (embed)0.02358\n",
            "Epoch 53 (train)5.0s: train loss = 0.03989 = (mf)0.01620 + (embed)0.02369\n",
            "Epoch 54 (train)4.1s: train loss = 0.03986 = (mf)0.01619 + (embed)0.02368\n",
            "Epoch 55 (train)4.1s + (eval)0.3s: train loss = 0.03973 = (mf)0.01614 + (embed)0.02359, recall = 0.11755, ndcg = 0.26436\n",
            "Epoch 56 (train)4.8s: train loss = 0.03983 = (mf)0.01621 + (embed)0.02362\n",
            "Epoch 57 (train)4.1s: train loss = 0.03960 = (mf)0.01591 + (embed)0.02368\n",
            "Epoch 58 (train)4.6s: train loss = 0.03998 = (mf)0.01625 + (embed)0.02374\n",
            "Epoch 59 (train)4.5s: train loss = 0.04025 = (mf)0.01645 + (embed)0.02381\n",
            "Epoch 60 (train)4.0s + (eval)0.2s: train loss = 0.04030 = (mf)0.01648 + (embed)0.02382, recall = 0.11913, ndcg = 0.26959\n",
            "Epoch 61 (train)4.8s: train loss = 0.04208 = (mf)0.01809 + (embed)0.02399\n",
            "Epoch 62 (train)4.1s: train loss = 0.04060 = (mf)0.01651 + (embed)0.02408\n",
            "Epoch 63 (train)4.0s: train loss = 0.03974 = (mf)0.01569 + (embed)0.02405\n",
            "Epoch 64 (train)4.9s: train loss = 0.04082 = (mf)0.01671 + (embed)0.02411\n",
            "Epoch 65 (train)4.0s + (eval)0.2s: train loss = 0.03884 = (mf)0.01470 + (embed)0.02414, recall = 0.12293, ndcg = 0.27467\n",
            "Epoch 66 (train)4.0s: train loss = 0.04038 = (mf)0.01621 + (embed)0.02416\n",
            "Epoch 67 (train)4.9s: train loss = 0.03957 = (mf)0.01539 + (embed)0.02418\n",
            "Epoch 68 (train)4.1s: train loss = 0.03874 = (mf)0.01461 + (embed)0.02413\n",
            "Epoch 69 (train)4.1s: train loss = 0.03960 = (mf)0.01540 + (embed)0.02420\n",
            "Epoch 70 (train)4.9s + (eval)0.6s: train loss = 0.04112 = (mf)0.01689 + (embed)0.02424, recall = 0.11737, ndcg = 0.27039\n",
            "Epoch 71 (train)4.0s: train loss = 0.04062 = (mf)0.01633 + (embed)0.02428\n",
            "Epoch 72 (train)4.0s: train loss = 0.04059 = (mf)0.01625 + (embed)0.02434\n",
            "Epoch 73 (train)4.9s: train loss = 0.03999 = (mf)0.01559 + (embed)0.02440\n",
            "Epoch 74 (train)4.0s: train loss = 0.03920 = (mf)0.01485 + (embed)0.02435\n",
            "Epoch 75 (train)4.1s + (eval)0.3s: train loss = 0.03999 = (mf)0.01563 + (embed)0.02436, recall = 0.11853, ndcg = 0.25990\n",
            "Epoch 76 (train)4.7s: train loss = 0.04047 = (mf)0.01603 + (embed)0.02444\n",
            "Epoch 77 (train)4.0s: train loss = 0.03970 = (mf)0.01525 + (embed)0.02445\n",
            "Epoch 78 (train)4.3s: train loss = 0.03977 = (mf)0.01538 + (embed)0.02439\n",
            "Epoch 79 (train)4.6s: train loss = 0.04013 = (mf)0.01568 + (embed)0.02445\n",
            "Epoch 80 (train)4.0s + (eval)0.2s: train loss = 0.03878 = (mf)0.01435 + (embed)0.02443, recall = 0.12370, ndcg = 0.28179\n",
            "Epoch 81 (train)4.6s: train loss = 0.03938 = (mf)0.01503 + (embed)0.02436\n",
            "Epoch 82 (train)4.4s: train loss = 0.03904 = (mf)0.01473 + (embed)0.02431\n",
            "Epoch 83 (train)4.1s: train loss = 0.03930 = (mf)0.01496 + (embed)0.02435\n",
            "Epoch 84 (train)4.9s: train loss = 0.03879 = (mf)0.01442 + (embed)0.02437\n",
            "Epoch 85 (train)4.1s + (eval)0.6s: train loss = 0.03950 = (mf)0.01513 + (embed)0.02437, recall = 0.12328, ndcg = 0.27820\n",
            "Epoch 86 (train)4.0s: train loss = 0.03982 = (mf)0.01543 + (embed)0.02439\n",
            "Epoch 87 (train)4.9s: train loss = 0.03890 = (mf)0.01445 + (embed)0.02445\n",
            "Epoch 88 (train)4.0s: train loss = 0.03996 = (mf)0.01561 + (embed)0.02436\n",
            "Epoch 89 (train)4.1s: train loss = 0.04046 = (mf)0.01600 + (embed)0.02446\n",
            "Epoch 90 (train)4.9s + (eval)0.2s: train loss = 0.03935 = (mf)0.01479 + (embed)0.02456, recall = 0.12067, ndcg = 0.27090\n",
            "Epoch 91 (train)4.1s: train loss = 0.03999 = (mf)0.01541 + (embed)0.02458\n",
            "Epoch 92 (train)4.1s: train loss = 0.04150 = (mf)0.01687 + (embed)0.02463\n",
            "Epoch 93 (train)4.9s: train loss = 0.04127 = (mf)0.01647 + (embed)0.02481\n",
            "Epoch 94 (train)4.1s: train loss = 0.04063 = (mf)0.01571 + (embed)0.02492\n",
            "Epoch 95 (train)4.4s + (eval)0.3s: train loss = 0.04181 = (mf)0.01684 + (embed)0.02496, recall = 0.12050, ndcg = 0.27138\n",
            "Epoch 96 (train)4.7s: train loss = 0.04004 = (mf)0.01508 + (embed)0.02496\n",
            "Epoch 97 (train)4.1s: train loss = 0.04028 = (mf)0.01532 + (embed)0.02496\n",
            "Epoch 98 (train)4.5s: train loss = 0.03948 = (mf)0.01461 + (embed)0.02486\n",
            "Epoch 99 (train)4.4s: train loss = 0.04003 = (mf)0.01513 + (embed)0.02489\n",
            "Epoch 100 (train)4.0s + (eval)0.2s: train loss = 0.04046 = (mf)0.01551 + (embed)0.02495, recall = 0.11568, ndcg = 0.26850\n",
            "Training finished in 443.88 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.26850, Recall@10: 0.11568\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)3.8s: train loss = 0.57821 = (mf)0.51566 + (embed)0.06256\n",
            "Epoch 2 (train)3.0s: train loss = 1.28211 = (mf)1.07642 + (embed)0.20568\n",
            "Epoch 3 (train)3.7s: train loss = 1.67643 = (mf)1.33312 + (embed)0.34330\n",
            "Epoch 4 (train)2.9s: train loss = 2.01320 = (mf)1.55001 + (embed)0.46319\n",
            "Epoch 5 (train)2.9s + (eval)0.9s: train loss = 2.37045 = (mf)1.79845 + (embed)0.57200, recall = 0.06605, ndcg = 0.17390\n",
            "Epoch 6 (train)3.5s: train loss = 2.65490 = (mf)1.98316 + (embed)0.67174\n",
            "Epoch 7 (train)3.2s: train loss = 2.84531 = (mf)2.07517 + (embed)0.77014\n",
            "Epoch 8 (train)2.9s: train loss = 3.11358 = (mf)2.24771 + (embed)0.86587\n",
            "Epoch 9 (train)3.0s: train loss = 3.34226 = (mf)2.39030 + (embed)0.95196\n",
            "Epoch 10 (train)3.4s + (eval)0.8s: train loss = 3.45469 = (mf)2.41592 + (embed)1.03877, recall = 0.05678, ndcg = 0.16110\n",
            "Epoch 11 (train)2.9s: train loss = 3.47181 = (mf)2.36007 + (embed)1.11174\n",
            "Epoch 12 (train)2.9s: train loss = 3.81357 = (mf)2.62726 + (embed)1.18631\n",
            "Epoch 13 (train)2.9s: train loss = 3.91550 = (mf)2.66639 + (embed)1.24911\n",
            "Epoch 14 (train)3.7s: train loss = 4.00197 = (mf)2.69096 + (embed)1.31101\n",
            "Epoch 15 (train)3.0s + (eval)0.2s: train loss = 4.16739 = (mf)2.79753 + (embed)1.36986, recall = 0.06325, ndcg = 0.16230\n",
            "Epoch 16 (train)2.9s: train loss = 4.33089 = (mf)2.91309 + (embed)1.41780\n",
            "Epoch 17 (train)2.9s: train loss = 4.31790 = (mf)2.84128 + (embed)1.47662\n",
            "Epoch 18 (train)3.7s: train loss = 4.53298 = (mf)3.00919 + (embed)1.52379\n",
            "Epoch 19 (train)3.0s: train loss = 4.45585 = (mf)2.88161 + (embed)1.57424\n",
            "Epoch 20 (train)3.0s + (eval)0.2s: train loss = 4.62248 = (mf)2.99895 + (embed)1.62353, recall = 0.06439, ndcg = 0.16450\n",
            "Epoch 21 (train)2.9s: train loss = 4.69906 = (mf)3.02339 + (embed)1.67567\n",
            "Epoch 22 (train)3.8s: train loss = 4.78228 = (mf)3.06128 + (embed)1.72100\n",
            "Epoch 23 (train)3.0s: train loss = 4.62909 = (mf)2.86808 + (embed)1.76102\n",
            "Epoch 24 (train)2.9s: train loss = 4.62231 = (mf)2.81907 + (embed)1.80324\n",
            "Epoch 25 (train)2.9s + (eval)0.6s: train loss = 4.59586 = (mf)2.75529 + (embed)1.84057, recall = 0.07072, ndcg = 0.18079\n",
            "Epoch 26 (train)3.8s: train loss = 5.07179 = (mf)3.19618 + (embed)1.87562\n",
            "Epoch 27 (train)2.9s: train loss = 5.03047 = (mf)3.11527 + (embed)1.91520\n",
            "Epoch 28 (train)2.9s: train loss = 4.57388 = (mf)2.62948 + (embed)1.94440\n",
            "Epoch 29 (train)2.9s: train loss = 4.92930 = (mf)2.95881 + (embed)1.97049\n",
            "Epoch 30 (train)3.8s + (eval)0.2s: train loss = 4.78299 = (mf)2.78729 + (embed)1.99570, recall = 0.07302, ndcg = 0.18311\n",
            "Epoch 31 (train)2.9s: train loss = 5.33337 = (mf)3.30127 + (embed)2.03210\n",
            "Epoch 32 (train)2.9s: train loss = 4.97228 = (mf)2.91553 + (embed)2.05675\n",
            "Epoch 33 (train)3.0s: train loss = 5.13409 = (mf)3.05781 + (embed)2.07628\n",
            "Epoch 34 (train)3.8s: train loss = 4.91509 = (mf)2.81887 + (embed)2.09621\n",
            "Epoch 35 (train)3.0s + (eval)0.2s: train loss = 5.22267 = (mf)3.10400 + (embed)2.11868, recall = 0.06874, ndcg = 0.17413\n",
            "Epoch 36 (train)3.0s: train loss = 5.17930 = (mf)3.03752 + (embed)2.14178\n",
            "Epoch 37 (train)3.1s: train loss = 5.09544 = (mf)2.92426 + (embed)2.17118\n",
            "Epoch 38 (train)3.7s: train loss = 5.18019 = (mf)2.98128 + (embed)2.19891\n",
            "Epoch 39 (train)2.9s: train loss = 5.63933 = (mf)3.41164 + (embed)2.22769\n",
            "Epoch 40 (train)2.9s + (eval)0.2s: train loss = 5.03092 = (mf)2.79622 + (embed)2.23470, recall = 0.06333, ndcg = 0.16869\n",
            "Epoch 41 (train)3.0s: train loss = 5.24394 = (mf)2.99648 + (embed)2.24746\n",
            "Epoch 42 (train)3.6s: train loss = 5.28864 = (mf)3.02673 + (embed)2.26191\n",
            "Epoch 43 (train)3.0s: train loss = 5.26656 = (mf)2.99273 + (embed)2.27383\n",
            "Epoch 44 (train)3.0s: train loss = 5.18750 = (mf)2.88641 + (embed)2.30109\n",
            "Epoch 45 (train)3.1s + (eval)0.7s: train loss = 5.12468 = (mf)2.81021 + (embed)2.31447, recall = 0.06915, ndcg = 0.18462\n",
            "Epoch 46 (train)3.4s: train loss = 5.07355 = (mf)2.73595 + (embed)2.33761\n",
            "Epoch 47 (train)3.0s: train loss = 5.37642 = (mf)3.02334 + (embed)2.35309\n",
            "Epoch 48 (train)2.9s: train loss = 5.40885 = (mf)3.03395 + (embed)2.37490\n",
            "Epoch 49 (train)3.4s: train loss = 5.46138 = (mf)3.06594 + (embed)2.39543\n",
            "Epoch 50 (train)3.4s + (eval)0.2s: train loss = 5.39926 = (mf)2.98799 + (embed)2.41127, recall = 0.06580, ndcg = 0.16261\n",
            "Epoch 51 (train)3.0s: train loss = 5.54671 = (mf)3.11667 + (embed)2.43004\n",
            "Epoch 52 (train)3.0s: train loss = 5.50162 = (mf)3.05599 + (embed)2.44563\n",
            "Epoch 53 (train)3.5s: train loss = 5.54027 = (mf)3.08548 + (embed)2.45478\n",
            "Epoch 54 (train)3.3s: train loss = 5.61150 = (mf)3.13077 + (embed)2.48073\n",
            "Epoch 55 (train)2.9s + (eval)0.2s: train loss = 5.47405 = (mf)2.98224 + (embed)2.49180, recall = 0.06060, ndcg = 0.15457\n",
            "Epoch 56 (train)3.0s: train loss = 5.20277 = (mf)2.68793 + (embed)2.51484\n",
            "Epoch 57 (train)3.7s: train loss = 5.16428 = (mf)2.63998 + (embed)2.52429\n",
            "Epoch 58 (train)3.1s: train loss = 5.44694 = (mf)2.91390 + (embed)2.53304\n",
            "Epoch 59 (train)2.9s: train loss = 5.55768 = (mf)3.01587 + (embed)2.54181\n",
            "Epoch 60 (train)2.9s + (eval)0.5s: train loss = 5.41073 = (mf)2.85583 + (embed)2.55490, recall = 0.07072, ndcg = 0.17435\n",
            "Epoch 61 (train)3.8s: train loss = 5.56811 = (mf)2.99973 + (embed)2.56838\n",
            "Epoch 62 (train)2.9s: train loss = 5.24530 = (mf)2.66187 + (embed)2.58343\n",
            "Epoch 63 (train)2.8s: train loss = 5.32134 = (mf)2.73772 + (embed)2.58362\n",
            "Epoch 64 (train)2.9s: train loss = 5.48281 = (mf)2.89596 + (embed)2.58686\n",
            "Epoch 65 (train)3.7s + (eval)0.2s: train loss = 5.23357 = (mf)2.63049 + (embed)2.60308, recall = 0.06884, ndcg = 0.17512\n",
            "Epoch 66 (train)2.9s: train loss = 5.13440 = (mf)2.51969 + (embed)2.61471\n",
            "Epoch 67 (train)2.9s: train loss = 5.07304 = (mf)2.45772 + (embed)2.61533\n",
            "Epoch 68 (train)2.9s: train loss = 5.35806 = (mf)2.73299 + (embed)2.62507\n",
            "Epoch 69 (train)3.7s: train loss = 5.38217 = (mf)2.75085 + (embed)2.63132\n",
            "Epoch 70 (train)3.1s + (eval)0.2s: train loss = 5.51727 = (mf)2.87156 + (embed)2.64571, recall = 0.07026, ndcg = 0.18003\n",
            "Epoch 71 (train)3.0s: train loss = 5.45103 = (mf)2.79844 + (embed)2.65260\n",
            "Epoch 72 (train)3.0s: train loss = 5.83228 = (mf)3.16808 + (embed)2.66420\n",
            "Epoch 73 (train)3.8s: train loss = 5.46330 = (mf)2.78542 + (embed)2.67788\n",
            "Epoch 74 (train)3.0s: train loss = 5.56618 = (mf)2.87851 + (embed)2.68767\n",
            "Epoch 75 (train)3.0s + (eval)0.2s: train loss = 5.38406 = (mf)2.69363 + (embed)2.69043, recall = 0.07032, ndcg = 0.17278\n",
            "Epoch 76 (train)2.9s: train loss = 5.45969 = (mf)2.76141 + (embed)2.69827\n",
            "Epoch 77 (train)3.8s: train loss = 5.64635 = (mf)2.93915 + (embed)2.70720\n",
            "Epoch 78 (train)2.9s: train loss = 5.48253 = (mf)2.75048 + (embed)2.73205\n",
            "Epoch 79 (train)2.9s: train loss = 5.44151 = (mf)2.70251 + (embed)2.73901\n",
            "Epoch 80 (train)2.9s + (eval)0.5s: train loss = 5.55253 = (mf)2.80710 + (embed)2.74543, recall = 0.07316, ndcg = 0.17604\n",
            "Epoch 81 (train)3.8s: train loss = 5.50526 = (mf)2.75533 + (embed)2.74994\n",
            "Epoch 82 (train)2.9s: train loss = 5.41393 = (mf)2.65415 + (embed)2.75978\n",
            "Epoch 83 (train)2.9s: train loss = 5.42743 = (mf)2.66117 + (embed)2.76626\n",
            "Epoch 84 (train)2.9s: train loss = 5.54962 = (mf)2.77035 + (embed)2.77926\n",
            "Epoch 85 (train)3.8s + (eval)0.2s: train loss = 5.50938 = (mf)2.71905 + (embed)2.79033, recall = 0.06359, ndcg = 0.16196\n",
            "Epoch 86 (train)3.0s: train loss = 5.39785 = (mf)2.60798 + (embed)2.78988\n",
            "Epoch 87 (train)3.0s: train loss = 5.29277 = (mf)2.48510 + (embed)2.80768\n",
            "Epoch 88 (train)3.0s: train loss = 5.84773 = (mf)3.03400 + (embed)2.81373\n",
            "Epoch 89 (train)3.7s: train loss = 5.50710 = (mf)2.68756 + (embed)2.81954\n",
            "Epoch 90 (train)2.9s + (eval)0.2s: train loss = 5.58946 = (mf)2.76632 + (embed)2.82313, recall = 0.06752, ndcg = 0.16816\n",
            "Epoch 91 (train)2.9s: train loss = 5.84847 = (mf)3.00832 + (embed)2.84015\n",
            "Epoch 92 (train)3.1s: train loss = 5.60562 = (mf)2.76253 + (embed)2.84308\n",
            "Epoch 93 (train)3.6s: train loss = 5.82854 = (mf)2.98338 + (embed)2.84516\n",
            "Epoch 94 (train)2.9s: train loss = 5.16006 = (mf)2.31468 + (embed)2.84538\n",
            "Epoch 95 (train)2.9s + (eval)0.6s: train loss = 5.41510 = (mf)2.57154 + (embed)2.84356, recall = 0.07266, ndcg = 0.17622\n",
            "Epoch 96 (train)3.3s: train loss = 5.51323 = (mf)2.66586 + (embed)2.84737\n",
            "Epoch 97 (train)3.4s: train loss = 5.45148 = (mf)2.59526 + (embed)2.85622\n",
            "Epoch 98 (train)2.9s: train loss = 5.70370 = (mf)2.83509 + (embed)2.86862\n",
            "Epoch 99 (train)3.0s: train loss = 5.39383 = (mf)2.52191 + (embed)2.87192\n",
            "Epoch 100 (train)3.3s + (eval)0.3s: train loss = 5.73983 = (mf)2.85731 + (embed)2.88252, recall = 0.07174, ndcg = 0.17146\n",
            "Training finished in 322.44 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.17146, Recall@10: 0.07174\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.43125 = (mf)0.37807 + (embed)0.05318\n",
            "Epoch 2 (train)3.7s: train loss = 0.79977 = (mf)0.62706 + (embed)0.17271\n",
            "Epoch 3 (train)3.5s: train loss = 1.16456 = (mf)0.86610 + (embed)0.29846\n",
            "Epoch 4 (train)3.8s: train loss = 1.34415 = (mf)0.93194 + (embed)0.41221\n",
            "Epoch 5 (train)4.2s + (eval)0.9s: train loss = 1.68672 = (mf)1.16736 + (embed)0.51936, recall = 0.06749, ndcg = 0.18671\n",
            "Epoch 6 (train)3.5s: train loss = 1.79096 = (mf)1.17962 + (embed)0.61134\n",
            "Epoch 7 (train)3.6s: train loss = 1.95276 = (mf)1.25925 + (embed)0.69351\n",
            "Epoch 8 (train)4.3s: train loss = 2.07602 = (mf)1.30691 + (embed)0.76911\n",
            "Epoch 9 (train)3.5s: train loss = 2.34710 = (mf)1.50850 + (embed)0.83859\n",
            "Epoch 10 (train)3.5s + (eval)0.6s: train loss = 2.33173 = (mf)1.42103 + (embed)0.91071, recall = 0.05974, ndcg = 0.15432\n",
            "Epoch 11 (train)4.4s: train loss = 2.65366 = (mf)1.68039 + (embed)0.97326\n",
            "Epoch 12 (train)3.5s: train loss = 2.51433 = (mf)1.48712 + (embed)1.02720\n",
            "Epoch 13 (train)3.6s: train loss = 2.57565 = (mf)1.50007 + (embed)1.07558\n",
            "Epoch 14 (train)4.4s: train loss = 2.72196 = (mf)1.59960 + (embed)1.12236\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 2.65247 = (mf)1.49606 + (embed)1.15641, recall = 0.05932, ndcg = 0.16200\n",
            "Epoch 16 (train)3.6s: train loss = 2.89192 = (mf)1.69071 + (embed)1.20121\n",
            "Epoch 17 (train)4.1s: train loss = 2.84207 = (mf)1.60164 + (embed)1.24043\n",
            "Epoch 18 (train)3.8s: train loss = 3.00949 = (mf)1.73425 + (embed)1.27524\n",
            "Epoch 19 (train)3.5s: train loss = 2.77664 = (mf)1.47199 + (embed)1.30465\n",
            "Epoch 20 (train)3.5s + (eval)0.2s: train loss = 2.98160 = (mf)1.64791 + (embed)1.33369, recall = 0.05861, ndcg = 0.16548\n",
            "Epoch 21 (train)4.3s: train loss = 3.08528 = (mf)1.73068 + (embed)1.35460\n",
            "Epoch 22 (train)3.5s: train loss = 3.10356 = (mf)1.71446 + (embed)1.38910\n",
            "Epoch 23 (train)3.5s: train loss = 3.14539 = (mf)1.73090 + (embed)1.41450\n",
            "Epoch 24 (train)4.4s: train loss = 3.03103 = (mf)1.58812 + (embed)1.44291\n",
            "Epoch 25 (train)3.5s + (eval)0.6s: train loss = 3.00232 = (mf)1.53466 + (embed)1.46766, recall = 0.05966, ndcg = 0.16040\n",
            "Epoch 26 (train)3.6s: train loss = 3.03073 = (mf)1.53584 + (embed)1.49489\n",
            "Epoch 27 (train)4.4s: train loss = 3.11329 = (mf)1.60005 + (embed)1.51324\n",
            "Epoch 28 (train)3.6s: train loss = 3.30879 = (mf)1.77393 + (embed)1.53486\n",
            "Epoch 29 (train)3.6s: train loss = 3.24844 = (mf)1.69488 + (embed)1.55356\n",
            "Epoch 30 (train)4.0s + (eval)0.3s: train loss = 3.18820 = (mf)1.61482 + (embed)1.57337, recall = 0.06202, ndcg = 0.16471\n",
            "Epoch 31 (train)3.8s: train loss = 3.13416 = (mf)1.54555 + (embed)1.58860\n",
            "Epoch 32 (train)3.5s: train loss = 3.15585 = (mf)1.55773 + (embed)1.59812\n",
            "Epoch 33 (train)3.5s: train loss = 3.30078 = (mf)1.69255 + (embed)1.60823\n",
            "Epoch 34 (train)4.4s: train loss = 3.23013 = (mf)1.60971 + (embed)1.62042\n",
            "Epoch 35 (train)3.5s + (eval)0.2s: train loss = 3.32603 = (mf)1.69203 + (embed)1.63401, recall = 0.05593, ndcg = 0.15552\n",
            "Epoch 36 (train)3.5s: train loss = 3.34474 = (mf)1.69047 + (embed)1.65427\n",
            "Epoch 37 (train)4.3s: train loss = 3.21042 = (mf)1.54506 + (embed)1.66536\n",
            "Epoch 38 (train)3.5s: train loss = 3.35063 = (mf)1.68031 + (embed)1.67032\n",
            "Epoch 39 (train)3.5s: train loss = 3.41684 = (mf)1.74388 + (embed)1.67296\n",
            "Epoch 40 (train)4.1s + (eval)0.3s: train loss = 3.33681 = (mf)1.65356 + (embed)1.68325, recall = 0.05289, ndcg = 0.14091\n",
            "Epoch 41 (train)3.7s: train loss = 3.14520 = (mf)1.44014 + (embed)1.70507\n",
            "Epoch 42 (train)3.7s: train loss = 3.24475 = (mf)1.53646 + (embed)1.70828\n",
            "Epoch 43 (train)3.9s: train loss = 3.46234 = (mf)1.73982 + (embed)1.72252\n",
            "Epoch 44 (train)4.2s: train loss = 3.29673 = (mf)1.56781 + (embed)1.72892\n",
            "Epoch 45 (train)3.5s + (eval)0.6s: train loss = 3.31674 = (mf)1.57580 + (embed)1.74094, recall = 0.06601, ndcg = 0.16333\n",
            "Epoch 46 (train)3.5s: train loss = 3.15263 = (mf)1.40436 + (embed)1.74827\n",
            "Epoch 47 (train)4.5s: train loss = 3.35655 = (mf)1.59831 + (embed)1.75824\n",
            "Epoch 48 (train)3.6s: train loss = 3.50751 = (mf)1.73760 + (embed)1.76991\n",
            "Epoch 49 (train)3.5s: train loss = 3.36740 = (mf)1.58643 + (embed)1.78098\n",
            "Epoch 50 (train)4.4s + (eval)0.2s: train loss = 3.42517 = (mf)1.63007 + (embed)1.79510, recall = 0.06297, ndcg = 0.16372\n",
            "Epoch 51 (train)3.6s: train loss = 3.44997 = (mf)1.64209 + (embed)1.80788\n",
            "Epoch 52 (train)3.6s: train loss = 3.52727 = (mf)1.69978 + (embed)1.82750\n",
            "Epoch 53 (train)4.2s: train loss = 3.26857 = (mf)1.43410 + (embed)1.83447\n",
            "Epoch 54 (train)3.8s: train loss = 3.30593 = (mf)1.47082 + (embed)1.83512\n",
            "Epoch 55 (train)3.6s + (eval)0.2s: train loss = 3.35134 = (mf)1.51419 + (embed)1.83714, recall = 0.05974, ndcg = 0.15825\n",
            "Epoch 56 (train)3.9s: train loss = 3.23832 = (mf)1.39952 + (embed)1.83880\n",
            "Epoch 57 (train)4.1s: train loss = 3.40062 = (mf)1.55447 + (embed)1.84615\n",
            "Epoch 58 (train)3.6s: train loss = 3.40337 = (mf)1.55338 + (embed)1.84999\n",
            "Epoch 59 (train)3.5s: train loss = 3.33889 = (mf)1.48371 + (embed)1.85518\n",
            "Epoch 60 (train)4.4s + (eval)0.6s: train loss = 3.30202 = (mf)1.43373 + (embed)1.86829, recall = 0.05289, ndcg = 0.14402\n",
            "Epoch 61 (train)3.5s: train loss = 3.34127 = (mf)1.46900 + (embed)1.87227\n",
            "Epoch 62 (train)3.6s: train loss = 3.36348 = (mf)1.49547 + (embed)1.86801\n",
            "Epoch 63 (train)4.4s: train loss = 3.36047 = (mf)1.48560 + (embed)1.87486\n",
            "Epoch 64 (train)3.5s: train loss = 3.47127 = (mf)1.59293 + (embed)1.87834\n",
            "Epoch 65 (train)3.5s + (eval)0.2s: train loss = 3.42540 = (mf)1.54232 + (embed)1.88308, recall = 0.06366, ndcg = 0.16392\n",
            "Epoch 66 (train)4.3s: train loss = 3.41584 = (mf)1.52629 + (embed)1.88955\n",
            "Epoch 67 (train)3.6s: train loss = 3.46723 = (mf)1.57950 + (embed)1.88773\n",
            "Epoch 68 (train)3.5s: train loss = 3.47621 = (mf)1.57629 + (embed)1.89992\n",
            "Epoch 69 (train)3.8s: train loss = 3.28789 = (mf)1.37898 + (embed)1.90892\n",
            "Epoch 70 (train)4.1s + (eval)0.2s: train loss = 3.37549 = (mf)1.46722 + (embed)1.90827, recall = 0.05972, ndcg = 0.15187\n",
            "Epoch 71 (train)3.5s: train loss = 3.39633 = (mf)1.48419 + (embed)1.91215\n",
            "Epoch 72 (train)3.5s: train loss = 3.42391 = (mf)1.50453 + (embed)1.91938\n",
            "Epoch 73 (train)4.4s: train loss = 3.45448 = (mf)1.53088 + (embed)1.92361\n",
            "Epoch 74 (train)3.5s: train loss = 3.39215 = (mf)1.47293 + (embed)1.91922\n",
            "Epoch 75 (train)3.5s + (eval)0.6s: train loss = 3.21179 = (mf)1.29048 + (embed)1.92132, recall = 0.06764, ndcg = 0.16883\n",
            "Epoch 76 (train)4.3s: train loss = 3.39282 = (mf)1.47660 + (embed)1.91622\n",
            "Epoch 77 (train)3.5s: train loss = 3.48477 = (mf)1.57030 + (embed)1.91447\n",
            "Epoch 78 (train)3.6s: train loss = 3.30786 = (mf)1.37901 + (embed)1.92885\n",
            "Epoch 79 (train)4.3s: train loss = 3.43470 = (mf)1.49778 + (embed)1.93692\n",
            "Epoch 80 (train)3.7s + (eval)0.2s: train loss = 3.37371 = (mf)1.44005 + (embed)1.93367, recall = 0.06375, ndcg = 0.16063\n",
            "Epoch 81 (train)3.6s: train loss = 3.39419 = (mf)1.46074 + (embed)1.93345\n",
            "Epoch 82 (train)4.0s: train loss = 3.27185 = (mf)1.33225 + (embed)1.93959\n",
            "Epoch 83 (train)4.1s: train loss = 3.39350 = (mf)1.45450 + (embed)1.93901\n",
            "Epoch 84 (train)3.6s: train loss = 3.26034 = (mf)1.31034 + (embed)1.95000\n",
            "Epoch 85 (train)3.7s + (eval)0.3s: train loss = 3.26566 = (mf)1.32014 + (embed)1.94552, recall = 0.05459, ndcg = 0.14118\n",
            "Epoch 86 (train)4.3s: train loss = 3.32218 = (mf)1.37274 + (embed)1.94944\n",
            "Epoch 87 (train)3.5s: train loss = 3.36899 = (mf)1.40948 + (embed)1.95951\n",
            "Epoch 88 (train)3.5s: train loss = 3.39182 = (mf)1.43922 + (embed)1.95260\n",
            "Epoch 89 (train)4.5s: train loss = 3.41819 = (mf)1.46594 + (embed)1.95225\n",
            "Epoch 90 (train)3.5s + (eval)0.2s: train loss = 3.49077 = (mf)1.52274 + (embed)1.96802, recall = 0.06103, ndcg = 0.14982\n",
            "Epoch 91 (train)3.5s: train loss = 3.41741 = (mf)1.44137 + (embed)1.97605\n",
            "Epoch 92 (train)4.3s: train loss = 3.35199 = (mf)1.37805 + (embed)1.97394\n",
            "Epoch 93 (train)3.5s: train loss = 3.43514 = (mf)1.45590 + (embed)1.97924\n",
            "Epoch 94 (train)3.5s: train loss = 3.40579 = (mf)1.42482 + (embed)1.98097\n",
            "Epoch 95 (train)3.8s + (eval)0.7s: train loss = 3.46852 = (mf)1.48495 + (embed)1.98357, recall = 0.06748, ndcg = 0.15661\n",
            "Epoch 96 (train)3.9s: train loss = 3.45750 = (mf)1.47255 + (embed)1.98495\n",
            "Epoch 97 (train)3.6s: train loss = 3.38222 = (mf)1.39612 + (embed)1.98610\n",
            "Epoch 98 (train)3.6s: train loss = 3.40906 = (mf)1.41370 + (embed)1.99535\n",
            "Epoch 99 (train)4.4s: train loss = 3.36511 = (mf)1.37106 + (embed)1.99405\n",
            "Epoch 100 (train)3.5s + (eval)0.2s: train loss = 3.51305 = (mf)1.52445 + (embed)1.98861, recall = 0.05581, ndcg = 0.14221\n",
            "Training finished in 389.08 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.14221, Recall@10: 0.05581\n",
            "--- Trying combination: Epochs=100, TopK=10, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.3s: train loss = 0.36268 = (mf)0.31965 + (embed)0.04303\n",
            "Epoch 2 (train)4.4s: train loss = 0.55432 = (mf)0.40963 + (embed)0.14469\n",
            "Epoch 3 (train)4.6s: train loss = 0.76065 = (mf)0.50788 + (embed)0.25277\n",
            "Epoch 4 (train)4.1s: train loss = 0.94215 = (mf)0.59724 + (embed)0.34491\n",
            "Epoch 5 (train)4.8s + (eval)1.5s: train loss = 1.12128 = (mf)0.69709 + (embed)0.42419, recall = 0.06312, ndcg = 0.16674\n",
            "Epoch 6 (train)4.2s: train loss = 1.37159 = (mf)0.85850 + (embed)0.51308\n",
            "Epoch 7 (train)4.2s: train loss = 1.44792 = (mf)0.85050 + (embed)0.59742\n",
            "Epoch 8 (train)5.0s: train loss = 1.56161 = (mf)0.88963 + (embed)0.67197\n",
            "Epoch 9 (train)4.1s: train loss = 1.70338 = (mf)0.97420 + (embed)0.72918\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 1.72400 = (mf)0.94341 + (embed)0.78059, recall = 0.06794, ndcg = 0.17914\n",
            "Epoch 11 (train)4.9s: train loss = 1.88443 = (mf)1.05263 + (embed)0.83180\n",
            "Epoch 12 (train)4.1s: train loss = 1.93621 = (mf)1.06193 + (embed)0.87428\n",
            "Epoch 13 (train)4.2s: train loss = 1.94255 = (mf)1.02433 + (embed)0.91822\n",
            "Epoch 14 (train)4.7s: train loss = 2.02395 = (mf)1.06617 + (embed)0.95778\n",
            "Epoch 15 (train)4.1s + (eval)0.2s: train loss = 1.95181 = (mf)0.96536 + (embed)0.98645, recall = 0.06413, ndcg = 0.16300\n",
            "Epoch 16 (train)4.5s: train loss = 2.08324 = (mf)1.06658 + (embed)1.01666\n",
            "Epoch 17 (train)4.4s: train loss = 2.05382 = (mf)1.00796 + (embed)1.04586\n",
            "Epoch 18 (train)4.0s: train loss = 2.11426 = (mf)1.04218 + (embed)1.07208\n",
            "Epoch 19 (train)4.8s: train loss = 2.15147 = (mf)1.05791 + (embed)1.09356\n",
            "Epoch 20 (train)4.3s + (eval)0.6s: train loss = 2.20120 = (mf)1.08593 + (embed)1.11528, recall = 0.05922, ndcg = 0.15374\n",
            "Epoch 21 (train)4.1s: train loss = 2.27328 = (mf)1.14316 + (embed)1.13012\n",
            "Epoch 22 (train)4.9s: train loss = 2.22170 = (mf)1.07082 + (embed)1.15088\n",
            "Epoch 23 (train)4.1s: train loss = 2.23139 = (mf)1.06033 + (embed)1.17106\n",
            "Epoch 24 (train)4.0s: train loss = 2.24932 = (mf)1.06255 + (embed)1.18677\n",
            "Epoch 25 (train)4.8s + (eval)0.2s: train loss = 2.26134 = (mf)1.05828 + (embed)1.20306, recall = 0.05753, ndcg = 0.14846\n",
            "Epoch 26 (train)4.0s: train loss = 2.40835 = (mf)1.18549 + (embed)1.22286\n",
            "Epoch 27 (train)4.0s: train loss = 2.19754 = (mf)0.96849 + (embed)1.22905\n",
            "Epoch 28 (train)4.9s: train loss = 2.31628 = (mf)1.07979 + (embed)1.23649\n",
            "Epoch 29 (train)4.0s: train loss = 2.38646 = (mf)1.13964 + (embed)1.24682\n",
            "Epoch 30 (train)4.0s + (eval)0.2s: train loss = 2.26370 = (mf)1.00936 + (embed)1.25434, recall = 0.05624, ndcg = 0.15123\n",
            "Epoch 31 (train)4.9s: train loss = 2.34210 = (mf)1.07725 + (embed)1.26486\n",
            "Epoch 32 (train)4.1s: train loss = 2.39894 = (mf)1.11882 + (embed)1.28012\n",
            "Epoch 33 (train)4.2s: train loss = 2.28441 = (mf)0.99800 + (embed)1.28640\n",
            "Epoch 34 (train)4.7s: train loss = 2.34539 = (mf)1.05174 + (embed)1.29365\n",
            "Epoch 35 (train)4.0s + (eval)0.6s: train loss = 2.37504 = (mf)1.06940 + (embed)1.30563, recall = 0.05981, ndcg = 0.14783\n",
            "Epoch 36 (train)4.6s: train loss = 2.36701 = (mf)1.05441 + (embed)1.31259\n",
            "Epoch 37 (train)4.2s: train loss = 2.34007 = (mf)1.02843 + (embed)1.31164\n",
            "Epoch 38 (train)4.1s: train loss = 2.44795 = (mf)1.12606 + (embed)1.32189\n",
            "Epoch 39 (train)4.8s: train loss = 2.36231 = (mf)1.03533 + (embed)1.32698\n",
            "Epoch 40 (train)4.1s + (eval)0.2s: train loss = 2.41367 = (mf)1.07743 + (embed)1.33624, recall = 0.05279, ndcg = 0.14281\n",
            "Epoch 41 (train)4.0s: train loss = 2.39699 = (mf)1.05100 + (embed)1.34599\n",
            "Epoch 42 (train)4.9s: train loss = 2.34964 = (mf)1.00361 + (embed)1.34603\n",
            "Epoch 43 (train)4.0s: train loss = 2.34142 = (mf)0.98677 + (embed)1.35465\n",
            "Epoch 44 (train)4.1s: train loss = 2.32893 = (mf)0.97355 + (embed)1.35537\n",
            "Epoch 45 (train)4.9s + (eval)0.2s: train loss = 2.32780 = (mf)0.96510 + (embed)1.36270, recall = 0.06203, ndcg = 0.15930\n",
            "Epoch 46 (train)4.1s: train loss = 2.31458 = (mf)0.95182 + (embed)1.36276\n",
            "Epoch 47 (train)4.1s: train loss = 2.29386 = (mf)0.92682 + (embed)1.36704\n",
            "Epoch 48 (train)4.9s: train loss = 2.30036 = (mf)0.93444 + (embed)1.36592\n",
            "Epoch 49 (train)4.1s: train loss = 2.30776 = (mf)0.93874 + (embed)1.36903\n",
            "Epoch 50 (train)4.1s + (eval)0.3s: train loss = 2.34385 = (mf)0.96967 + (embed)1.37418, recall = 0.06013, ndcg = 0.14569\n",
            "Epoch 51 (train)4.8s: train loss = 2.30900 = (mf)0.93141 + (embed)1.37759\n",
            "Epoch 52 (train)4.0s: train loss = 2.35814 = (mf)0.97080 + (embed)1.38734\n",
            "Epoch 53 (train)4.3s: train loss = 2.44017 = (mf)1.05296 + (embed)1.38721\n",
            "Epoch 54 (train)4.7s: train loss = 2.37681 = (mf)0.97368 + (embed)1.40313\n",
            "Epoch 55 (train)4.1s + (eval)0.6s: train loss = 2.38964 = (mf)0.98347 + (embed)1.40617, recall = 0.06426, ndcg = 0.15981\n",
            "Epoch 56 (train)4.8s: train loss = 2.36546 = (mf)0.96091 + (embed)1.40455\n",
            "Epoch 57 (train)4.2s: train loss = 2.34713 = (mf)0.94002 + (embed)1.40711\n",
            "Epoch 58 (train)4.1s: train loss = 2.34219 = (mf)0.93141 + (embed)1.41078\n",
            "Epoch 59 (train)4.9s: train loss = 2.31425 = (mf)0.89819 + (embed)1.41605\n",
            "Epoch 60 (train)4.1s + (eval)0.2s: train loss = 2.29411 = (mf)0.87284 + (embed)1.42128, recall = 0.05762, ndcg = 0.15985\n",
            "Epoch 61 (train)4.1s: train loss = 2.37912 = (mf)0.96298 + (embed)1.41614\n",
            "Epoch 62 (train)4.9s: train loss = 2.37308 = (mf)0.94880 + (embed)1.42428\n",
            "Epoch 63 (train)4.0s: train loss = 2.31171 = (mf)0.89397 + (embed)1.41774\n",
            "Epoch 64 (train)4.1s: train loss = 2.31628 = (mf)0.89439 + (embed)1.42188\n",
            "Epoch 65 (train)5.0s + (eval)0.2s: train loss = 2.28390 = (mf)0.86012 + (embed)1.42377, recall = 0.05743, ndcg = 0.13863\n",
            "Epoch 66 (train)4.2s: train loss = 2.45877 = (mf)1.02724 + (embed)1.43153\n",
            "Epoch 67 (train)4.3s: train loss = 2.44034 = (mf)0.99619 + (embed)1.44415\n",
            "Epoch 68 (train)5.1s: train loss = 2.31790 = (mf)0.87174 + (embed)1.44616\n",
            "Epoch 69 (train)4.3s: train loss = 2.35998 = (mf)0.91768 + (embed)1.44230\n",
            "Epoch 70 (train)4.6s + (eval)0.3s: train loss = 2.37368 = (mf)0.93033 + (embed)1.44335, recall = 0.04991, ndcg = 0.13087\n",
            "Epoch 71 (train)4.5s: train loss = 2.35892 = (mf)0.91509 + (embed)1.44384\n",
            "Epoch 72 (train)4.2s: train loss = 2.40011 = (mf)0.95853 + (embed)1.44158\n",
            "Epoch 73 (train)5.1s: train loss = 2.34354 = (mf)0.89245 + (embed)1.45109\n",
            "Epoch 74 (train)4.2s: train loss = 2.38216 = (mf)0.92971 + (embed)1.45245\n",
            "Epoch 75 (train)4.2s + (eval)0.6s: train loss = 2.45827 = (mf)1.00664 + (embed)1.45164, recall = 0.06639, ndcg = 0.16769\n",
            "Epoch 76 (train)5.1s: train loss = 2.34075 = (mf)0.88259 + (embed)1.45816\n",
            "Epoch 77 (train)4.2s: train loss = 2.42284 = (mf)0.97002 + (embed)1.45282\n",
            "Epoch 78 (train)4.2s: train loss = 2.36164 = (mf)0.90830 + (embed)1.45334\n",
            "Epoch 79 (train)5.1s: train loss = 2.38925 = (mf)0.92904 + (embed)1.46021\n",
            "Epoch 80 (train)4.2s + (eval)0.2s: train loss = 2.46959 = (mf)1.01210 + (embed)1.45749, recall = 0.04892, ndcg = 0.12096\n",
            "Epoch 81 (train)4.6s: train loss = 2.42403 = (mf)0.97059 + (embed)1.45344\n",
            "Epoch 82 (train)4.6s: train loss = 2.28922 = (mf)0.83236 + (embed)1.45687\n",
            "Epoch 83 (train)4.1s: train loss = 2.34994 = (mf)0.89600 + (embed)1.45395\n",
            "Epoch 84 (train)5.0s: train loss = 2.43290 = (mf)0.98318 + (embed)1.44972\n",
            "Epoch 85 (train)4.1s + (eval)0.2s: train loss = 2.43547 = (mf)0.98524 + (embed)1.45023, recall = 0.05847, ndcg = 0.14062\n",
            "Epoch 86 (train)4.1s: train loss = 2.31324 = (mf)0.86732 + (embed)1.44591\n",
            "Epoch 87 (train)5.0s: train loss = 2.44870 = (mf)0.99603 + (embed)1.45267\n",
            "Epoch 88 (train)4.1s: train loss = 2.27365 = (mf)0.82033 + (embed)1.45332\n",
            "Epoch 89 (train)4.2s: train loss = 2.37117 = (mf)0.91194 + (embed)1.45924\n",
            "Epoch 90 (train)5.0s + (eval)0.6s: train loss = 2.33895 = (mf)0.87791 + (embed)1.46104, recall = 0.06218, ndcg = 0.14773\n",
            "Epoch 91 (train)4.2s: train loss = 2.39604 = (mf)0.93985 + (embed)1.45619\n",
            "Epoch 92 (train)4.3s: train loss = 2.38868 = (mf)0.93370 + (embed)1.45498\n",
            "Epoch 93 (train)4.8s: train loss = 2.33890 = (mf)0.88308 + (embed)1.45582\n",
            "Epoch 94 (train)4.1s: train loss = 2.36770 = (mf)0.91938 + (embed)1.44832\n",
            "Epoch 95 (train)4.5s + (eval)0.3s: train loss = 2.38997 = (mf)0.93586 + (embed)1.45411, recall = 0.05195, ndcg = 0.12828\n",
            "Epoch 96 (train)4.4s: train loss = 2.37418 = (mf)0.92057 + (embed)1.45361\n",
            "Epoch 97 (train)4.1s: train loss = 2.29540 = (mf)0.84101 + (embed)1.45440\n",
            "Epoch 98 (train)4.8s: train loss = 2.36575 = (mf)0.91031 + (embed)1.45544\n",
            "Epoch 99 (train)4.2s: train loss = 2.37832 = (mf)0.92204 + (embed)1.45628\n",
            "Epoch 100 (train)4.2s + (eval)0.2s: train loss = 2.45426 = (mf)0.99228 + (embed)1.46198, recall = 0.05681, ndcg = 0.14152\n",
            "Training finished in 447.52 seconds.\n",
            "Evaluation results for this combination: NDCG@10: 0.14152, Recall@10: 0.05681\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.3s: train loss = 0.42517 = (mf)0.42494 + (embed)0.00023\n",
            "Epoch 2 (train)3.9s: train loss = 0.21990 = (mf)0.21925 + (embed)0.00065\n",
            "Epoch 3 (train)3.0s: train loss = 0.19244 = (mf)0.19167 + (embed)0.00077\n",
            "Epoch 4 (train)3.0s: train loss = 0.17070 = (mf)0.16978 + (embed)0.00092\n",
            "Epoch 5 (train)3.0s + (eval)1.0s: train loss = 0.15689 = (mf)0.15581 + (embed)0.00108, recall = 0.16012, ndcg = 0.24801\n",
            "Epoch 6 (train)3.8s: train loss = 0.14504 = (mf)0.14382 + (embed)0.00122\n",
            "Epoch 7 (train)2.9s: train loss = 0.13975 = (mf)0.13839 + (embed)0.00136\n",
            "Epoch 8 (train)3.0s: train loss = 0.13004 = (mf)0.12855 + (embed)0.00149\n",
            "Epoch 9 (train)3.0s: train loss = 0.12265 = (mf)0.12102 + (embed)0.00163\n",
            "Epoch 10 (train)3.7s + (eval)0.2s: train loss = 0.12069 = (mf)0.11893 + (embed)0.00176, recall = 0.16258, ndcg = 0.25514\n",
            "Epoch 11 (train)2.9s: train loss = 0.11345 = (mf)0.11156 + (embed)0.00189\n",
            "Epoch 12 (train)2.9s: train loss = 0.11132 = (mf)0.10928 + (embed)0.00203\n",
            "Epoch 13 (train)3.1s: train loss = 0.10922 = (mf)0.10706 + (embed)0.00217\n",
            "Epoch 14 (train)3.6s: train loss = 0.10203 = (mf)0.09971 + (embed)0.00231\n",
            "Epoch 15 (train)3.0s + (eval)0.2s: train loss = 0.09787 = (mf)0.09540 + (embed)0.00246, recall = 0.16848, ndcg = 0.26578\n",
            "Epoch 16 (train)2.9s: train loss = 0.09276 = (mf)0.09014 + (embed)0.00262\n",
            "Epoch 17 (train)3.2s: train loss = 0.08726 = (mf)0.08450 + (embed)0.00276\n",
            "Epoch 18 (train)3.5s: train loss = 0.08248 = (mf)0.07955 + (embed)0.00293\n",
            "Epoch 19 (train)3.0s: train loss = 0.08072 = (mf)0.07763 + (embed)0.00309\n",
            "Epoch 20 (train)3.0s + (eval)0.6s: train loss = 0.07589 = (mf)0.07264 + (embed)0.00325, recall = 0.18718, ndcg = 0.28740\n",
            "Epoch 21 (train)3.7s: train loss = 0.07368 = (mf)0.07027 + (embed)0.00341\n",
            "Epoch 22 (train)3.1s: train loss = 0.07016 = (mf)0.06659 + (embed)0.00357\n",
            "Epoch 23 (train)2.9s: train loss = 0.06766 = (mf)0.06394 + (embed)0.00372\n",
            "Epoch 24 (train)2.9s: train loss = 0.06483 = (mf)0.06096 + (embed)0.00387\n",
            "Epoch 25 (train)3.7s + (eval)0.3s: train loss = 0.06341 = (mf)0.05938 + (embed)0.00403, recall = 0.19894, ndcg = 0.30569\n",
            "Epoch 26 (train)4.9s: train loss = 0.06055 = (mf)0.05637 + (embed)0.00417\n",
            "Epoch 27 (train)4.0s: train loss = 0.05962 = (mf)0.05530 + (embed)0.00432\n",
            "Epoch 28 (train)3.7s: train loss = 0.05759 = (mf)0.05313 + (embed)0.00446\n",
            "Epoch 29 (train)3.0s: train loss = 0.05662 = (mf)0.05204 + (embed)0.00458\n",
            "Epoch 30 (train)3.0s + (eval)0.2s: train loss = 0.05461 = (mf)0.04989 + (embed)0.00472, recall = 0.20770, ndcg = 0.31583\n",
            "Epoch 31 (train)3.0s: train loss = 0.05365 = (mf)0.04879 + (embed)0.00486\n",
            "Epoch 32 (train)6.8s: train loss = 0.05046 = (mf)0.04547 + (embed)0.00499\n",
            "Epoch 33 (train)4.5s: train loss = 0.04877 = (mf)0.04364 + (embed)0.00513\n",
            "Epoch 34 (train)4.4s: train loss = 0.04850 = (mf)0.04325 + (embed)0.00525\n",
            "Epoch 35 (train)4.2s + (eval)0.4s: train loss = 0.04795 = (mf)0.04258 + (embed)0.00538, recall = 0.21519, ndcg = 0.32685\n",
            "Epoch 36 (train)4.2s: train loss = 0.04725 = (mf)0.04177 + (embed)0.00548\n",
            "Epoch 37 (train)3.2s: train loss = 0.04658 = (mf)0.04099 + (embed)0.00559\n",
            "Epoch 38 (train)3.7s: train loss = 0.04457 = (mf)0.03889 + (embed)0.00568\n",
            "Epoch 39 (train)3.0s: train loss = 0.04488 = (mf)0.03908 + (embed)0.00580\n",
            "Epoch 40 (train)4.1s + (eval)0.9s: train loss = 0.04266 = (mf)0.03676 + (embed)0.00589, recall = 0.21844, ndcg = 0.33039\n",
            "Epoch 41 (train)4.6s: train loss = 0.04142 = (mf)0.03542 + (embed)0.00600\n",
            "Epoch 42 (train)3.0s: train loss = 0.04220 = (mf)0.03611 + (embed)0.00608\n",
            "Epoch 43 (train)3.0s: train loss = 0.04103 = (mf)0.03485 + (embed)0.00617\n",
            "Epoch 44 (train)3.1s: train loss = 0.04098 = (mf)0.03472 + (embed)0.00626\n",
            "Epoch 45 (train)3.7s + (eval)0.2s: train loss = 0.03911 = (mf)0.03276 + (embed)0.00635, recall = 0.21866, ndcg = 0.32911\n",
            "Epoch 46 (train)3.0s: train loss = 0.03668 = (mf)0.03025 + (embed)0.00643\n",
            "Epoch 47 (train)3.0s: train loss = 0.03869 = (mf)0.03218 + (embed)0.00651\n",
            "Epoch 48 (train)3.5s: train loss = 0.03776 = (mf)0.03120 + (embed)0.00657\n",
            "Epoch 49 (train)3.5s: train loss = 0.03563 = (mf)0.02896 + (embed)0.00666\n",
            "Epoch 50 (train)3.1s + (eval)0.2s: train loss = 0.03677 = (mf)0.03002 + (embed)0.00674, recall = 0.21939, ndcg = 0.33242\n",
            "Epoch 51 (train)3.1s: train loss = 0.03583 = (mf)0.02901 + (embed)0.00682\n",
            "Epoch 52 (train)4.6s: train loss = 0.03581 = (mf)0.02891 + (embed)0.00690\n",
            "Epoch 53 (train)5.4s: train loss = 0.03421 = (mf)0.02726 + (embed)0.00695\n",
            "Epoch 54 (train)4.8s: train loss = 0.03345 = (mf)0.02643 + (embed)0.00701\n",
            "Epoch 55 (train)3.8s + (eval)0.6s: train loss = 0.03386 = (mf)0.02676 + (embed)0.00709, recall = 0.21850, ndcg = 0.33466\n",
            "Epoch 56 (train)3.2s: train loss = 0.03437 = (mf)0.02722 + (embed)0.00714\n",
            "Epoch 57 (train)3.1s: train loss = 0.03273 = (mf)0.02551 + (embed)0.00722\n",
            "Epoch 58 (train)3.8s: train loss = 0.03207 = (mf)0.02481 + (embed)0.00727\n",
            "Epoch 59 (train)3.3s: train loss = 0.03229 = (mf)0.02497 + (embed)0.00731\n",
            "Epoch 60 (train)3.1s + (eval)0.2s: train loss = 0.03160 = (mf)0.02423 + (embed)0.00737, recall = 0.22005, ndcg = 0.33334\n",
            "Epoch 61 (train)3.0s: train loss = 0.03120 = (mf)0.02377 + (embed)0.00743\n",
            "Epoch 62 (train)3.9s: train loss = 0.03080 = (mf)0.02332 + (embed)0.00748\n",
            "Epoch 63 (train)3.0s: train loss = 0.03100 = (mf)0.02346 + (embed)0.00753\n",
            "Epoch 64 (train)3.0s: train loss = 0.03072 = (mf)0.02314 + (embed)0.00758\n",
            "Epoch 65 (train)3.1s + (eval)0.2s: train loss = 0.03098 = (mf)0.02334 + (embed)0.00764, recall = 0.22127, ndcg = 0.33256\n",
            "Epoch 66 (train)3.9s: train loss = 0.03093 = (mf)0.02327 + (embed)0.00767\n",
            "Epoch 67 (train)3.0s: train loss = 0.02893 = (mf)0.02123 + (embed)0.00770\n",
            "Epoch 68 (train)3.0s: train loss = 0.03025 = (mf)0.02249 + (embed)0.00776\n",
            "Epoch 69 (train)3.0s: train loss = 0.02981 = (mf)0.02201 + (embed)0.00780\n",
            "Epoch 70 (train)3.8s + (eval)0.2s: train loss = 0.02980 = (mf)0.02195 + (embed)0.00784, recall = 0.22111, ndcg = 0.33187\n",
            "Epoch 71 (train)3.6s: train loss = 0.02941 = (mf)0.02152 + (embed)0.00789\n",
            "Epoch 72 (train)3.0s: train loss = 0.02807 = (mf)0.02015 + (embed)0.00791\n",
            "Epoch 73 (train)3.4s: train loss = 0.02934 = (mf)0.02140 + (embed)0.00795\n",
            "Epoch 74 (train)3.4s: train loss = 0.02859 = (mf)0.02061 + (embed)0.00798\n",
            "Epoch 75 (train)2.9s + (eval)0.6s: train loss = 0.02716 = (mf)0.01914 + (embed)0.00801, recall = 0.22367, ndcg = 0.33272\n",
            "Epoch 76 (train)3.0s: train loss = 0.02699 = (mf)0.01893 + (embed)0.00806\n",
            "Epoch 77 (train)3.7s: train loss = 0.02744 = (mf)0.01934 + (embed)0.00810\n",
            "Epoch 78 (train)3.1s: train loss = 0.02629 = (mf)0.01819 + (embed)0.00810\n",
            "Epoch 79 (train)3.0s: train loss = 0.02726 = (mf)0.01910 + (embed)0.00816\n",
            "Epoch 80 (train)2.9s + (eval)0.2s: train loss = 0.02642 = (mf)0.01825 + (embed)0.00817, recall = 0.22337, ndcg = 0.33684\n",
            "Epoch 81 (train)3.8s: train loss = 0.02654 = (mf)0.01832 + (embed)0.00822\n",
            "Epoch 82 (train)2.9s: train loss = 0.02668 = (mf)0.01844 + (embed)0.00824\n",
            "Epoch 83 (train)3.0s: train loss = 0.02675 = (mf)0.01848 + (embed)0.00826\n",
            "Epoch 84 (train)3.0s: train loss = 0.02605 = (mf)0.01775 + (embed)0.00830\n",
            "Epoch 85 (train)4.6s + (eval)0.2s: train loss = 0.02634 = (mf)0.01802 + (embed)0.00832, recall = 0.22423, ndcg = 0.33603\n",
            "Epoch 86 (train)2.9s: train loss = 0.02559 = (mf)0.01726 + (embed)0.00833\n",
            "Epoch 87 (train)2.9s: train loss = 0.02624 = (mf)0.01788 + (embed)0.00835\n",
            "Epoch 88 (train)2.9s: train loss = 0.02520 = (mf)0.01684 + (embed)0.00836\n",
            "Epoch 89 (train)3.7s: train loss = 0.02511 = (mf)0.01671 + (embed)0.00840\n",
            "Epoch 90 (train)2.9s + (eval)0.6s: train loss = 0.02446 = (mf)0.01603 + (embed)0.00843, recall = 0.22713, ndcg = 0.33944\n",
            "Epoch 91 (train)3.0s: train loss = 0.02550 = (mf)0.01706 + (embed)0.00844\n",
            "Epoch 92 (train)3.2s: train loss = 0.02508 = (mf)0.01662 + (embed)0.00846\n",
            "Epoch 93 (train)3.7s: train loss = 0.02451 = (mf)0.01603 + (embed)0.00848\n",
            "Epoch 94 (train)3.0s: train loss = 0.02482 = (mf)0.01631 + (embed)0.00851\n",
            "Epoch 95 (train)3.0s + (eval)0.2s: train loss = 0.02500 = (mf)0.01647 + (embed)0.00853, recall = 0.22633, ndcg = 0.33614\n",
            "Epoch 96 (train)3.3s: train loss = 0.02427 = (mf)0.01571 + (embed)0.00856\n",
            "Epoch 97 (train)3.5s: train loss = 0.02365 = (mf)0.01508 + (embed)0.00857\n",
            "Epoch 98 (train)2.9s: train loss = 0.02388 = (mf)0.01529 + (embed)0.00859\n",
            "Epoch 99 (train)2.9s: train loss = 0.02354 = (mf)0.01495 + (embed)0.00859\n",
            "Epoch 100 (train)3.3s + (eval)0.3s: train loss = 0.02366 = (mf)0.01506 + (embed)0.00860, recall = 0.22394, ndcg = 0.33565\n",
            "Training finished in 348.91 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.33565, Recall@20: 0.22394\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.4s: train loss = 0.41250 = (mf)0.41225 + (embed)0.00025\n",
            "Epoch 2 (train)3.6s: train loss = 0.22710 = (mf)0.22646 + (embed)0.00064\n",
            "Epoch 3 (train)3.6s: train loss = 0.21818 = (mf)0.21743 + (embed)0.00075\n",
            "Epoch 4 (train)4.4s: train loss = 0.20734 = (mf)0.20648 + (embed)0.00086\n",
            "Epoch 5 (train)3.6s + (eval)1.0s: train loss = 0.19949 = (mf)0.19850 + (embed)0.00099, recall = 0.11637, ndcg = 0.20068\n",
            "Epoch 6 (train)3.6s: train loss = 0.18664 = (mf)0.18549 + (embed)0.00115\n",
            "Epoch 7 (train)4.5s: train loss = 0.17095 = (mf)0.16962 + (embed)0.00133\n",
            "Epoch 8 (train)3.6s: train loss = 0.15614 = (mf)0.15459 + (embed)0.00155\n",
            "Epoch 9 (train)3.6s: train loss = 0.14323 = (mf)0.14147 + (embed)0.00175\n",
            "Epoch 10 (train)4.0s + (eval)0.3s: train loss = 0.14009 = (mf)0.13816 + (embed)0.00193, recall = 0.15206, ndcg = 0.24258\n",
            "Epoch 11 (train)3.8s: train loss = 0.13118 = (mf)0.12908 + (embed)0.00210\n",
            "Epoch 12 (train)4.8s: train loss = 0.13084 = (mf)0.12860 + (embed)0.00225\n",
            "Epoch 13 (train)4.2s: train loss = 0.12549 = (mf)0.12309 + (embed)0.00240\n",
            "Epoch 14 (train)3.8s: train loss = 0.12120 = (mf)0.11865 + (embed)0.00255\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.11754 = (mf)0.11483 + (embed)0.00270, recall = 0.15678, ndcg = 0.25157\n",
            "Epoch 16 (train)3.7s: train loss = 0.11108 = (mf)0.10820 + (embed)0.00287\n",
            "Epoch 17 (train)4.2s: train loss = 0.10705 = (mf)0.10402 + (embed)0.00303\n",
            "Epoch 18 (train)3.5s: train loss = 0.10383 = (mf)0.10064 + (embed)0.00319\n",
            "Epoch 19 (train)3.6s: train loss = 0.10139 = (mf)0.09803 + (embed)0.00336\n",
            "Epoch 20 (train)4.4s + (eval)0.6s: train loss = 0.09869 = (mf)0.09516 + (embed)0.00353, recall = 0.16973, ndcg = 0.26978\n",
            "Epoch 21 (train)3.6s: train loss = 0.09316 = (mf)0.08945 + (embed)0.00371\n",
            "Epoch 22 (train)3.5s: train loss = 0.09086 = (mf)0.08696 + (embed)0.00389\n",
            "Epoch 23 (train)4.4s: train loss = 0.08778 = (mf)0.08370 + (embed)0.00408\n",
            "Epoch 24 (train)3.5s: train loss = 0.08646 = (mf)0.08220 + (embed)0.00425\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 0.08195 = (mf)0.07753 + (embed)0.00442, recall = 0.18234, ndcg = 0.28547\n",
            "Epoch 26 (train)4.1s: train loss = 0.07941 = (mf)0.07480 + (embed)0.00461\n",
            "Epoch 27 (train)3.8s: train loss = 0.07677 = (mf)0.07200 + (embed)0.00477\n",
            "Epoch 28 (train)3.5s: train loss = 0.07390 = (mf)0.06896 + (embed)0.00494\n",
            "Epoch 29 (train)3.6s: train loss = 0.07200 = (mf)0.06689 + (embed)0.00510\n",
            "Epoch 30 (train)4.3s + (eval)0.2s: train loss = 0.07250 = (mf)0.06725 + (embed)0.00525, recall = 0.19153, ndcg = 0.29912\n",
            "Epoch 31 (train)3.6s: train loss = 0.06789 = (mf)0.06246 + (embed)0.00543\n",
            "Epoch 32 (train)3.6s: train loss = 0.06748 = (mf)0.06192 + (embed)0.00556\n",
            "Epoch 33 (train)4.4s: train loss = 0.06578 = (mf)0.06007 + (embed)0.00571\n",
            "Epoch 34 (train)3.5s: train loss = 0.06436 = (mf)0.05851 + (embed)0.00585\n",
            "Epoch 35 (train)3.5s + (eval)0.6s: train loss = 0.06359 = (mf)0.05761 + (embed)0.00598, recall = 0.20155, ndcg = 0.31210\n",
            "Epoch 36 (train)4.4s: train loss = 0.06279 = (mf)0.05668 + (embed)0.00611\n",
            "Epoch 37 (train)3.5s: train loss = 0.05932 = (mf)0.05307 + (embed)0.00625\n",
            "Epoch 38 (train)3.5s: train loss = 0.06026 = (mf)0.05389 + (embed)0.00637\n",
            "Epoch 39 (train)3.9s: train loss = 0.05794 = (mf)0.05144 + (embed)0.00650\n",
            "Epoch 40 (train)3.9s + (eval)0.2s: train loss = 0.05710 = (mf)0.05048 + (embed)0.00662, recall = 0.20770, ndcg = 0.32075\n",
            "Epoch 41 (train)3.5s: train loss = 0.05837 = (mf)0.05163 + (embed)0.00673\n",
            "Epoch 42 (train)3.5s: train loss = 0.05662 = (mf)0.04978 + (embed)0.00684\n",
            "Epoch 43 (train)4.4s: train loss = 0.05387 = (mf)0.04691 + (embed)0.00696\n",
            "Epoch 44 (train)3.6s: train loss = 0.05448 = (mf)0.04741 + (embed)0.00708\n",
            "Epoch 45 (train)3.6s + (eval)0.2s: train loss = 0.05149 = (mf)0.04428 + (embed)0.00721, recall = 0.21400, ndcg = 0.33058\n",
            "Epoch 46 (train)4.4s: train loss = 0.05133 = (mf)0.04402 + (embed)0.00731\n",
            "Epoch 47 (train)3.7s: train loss = 0.05031 = (mf)0.04289 + (embed)0.00741\n",
            "Epoch 48 (train)3.7s: train loss = 0.04995 = (mf)0.04244 + (embed)0.00751\n",
            "Epoch 49 (train)4.4s: train loss = 0.04959 = (mf)0.04199 + (embed)0.00760\n",
            "Epoch 50 (train)3.5s + (eval)0.2s: train loss = 0.04911 = (mf)0.04142 + (embed)0.00769, recall = 0.21933, ndcg = 0.33491\n",
            "Epoch 51 (train)3.6s: train loss = 0.04927 = (mf)0.04147 + (embed)0.00779\n",
            "Epoch 52 (train)4.1s: train loss = 0.04686 = (mf)0.03897 + (embed)0.00789\n",
            "Epoch 53 (train)3.9s: train loss = 0.04835 = (mf)0.04041 + (embed)0.00794\n",
            "Epoch 54 (train)3.6s: train loss = 0.04559 = (mf)0.03754 + (embed)0.00805\n",
            "Epoch 55 (train)3.5s + (eval)0.8s: train loss = 0.04538 = (mf)0.03725 + (embed)0.00813, recall = 0.21973, ndcg = 0.33584\n",
            "Epoch 56 (train)4.1s: train loss = 0.04433 = (mf)0.03613 + (embed)0.00819\n",
            "Epoch 57 (train)3.5s: train loss = 0.04520 = (mf)0.03690 + (embed)0.00831\n",
            "Epoch 58 (train)3.6s: train loss = 0.04454 = (mf)0.03618 + (embed)0.00836\n",
            "Epoch 59 (train)4.4s: train loss = 0.04260 = (mf)0.03418 + (embed)0.00842\n",
            "Epoch 60 (train)3.6s + (eval)0.2s: train loss = 0.04298 = (mf)0.03449 + (embed)0.00849, recall = 0.22197, ndcg = 0.33745\n",
            "Epoch 61 (train)3.6s: train loss = 0.04250 = (mf)0.03391 + (embed)0.00858\n",
            "Epoch 62 (train)4.4s: train loss = 0.04163 = (mf)0.03299 + (embed)0.00863\n",
            "Epoch 63 (train)3.6s: train loss = 0.04115 = (mf)0.03243 + (embed)0.00872\n",
            "Epoch 64 (train)3.5s: train loss = 0.04108 = (mf)0.03231 + (embed)0.00877\n",
            "Epoch 65 (train)3.9s + (eval)0.3s: train loss = 0.04084 = (mf)0.03202 + (embed)0.00882, recall = 0.22411, ndcg = 0.34050\n",
            "Epoch 66 (train)3.8s: train loss = 0.03933 = (mf)0.03046 + (embed)0.00888\n",
            "Epoch 67 (train)3.5s: train loss = 0.03904 = (mf)0.03009 + (embed)0.00896\n",
            "Epoch 68 (train)3.5s: train loss = 0.03995 = (mf)0.03093 + (embed)0.00902\n",
            "Epoch 69 (train)4.3s: train loss = 0.03908 = (mf)0.03002 + (embed)0.00907\n",
            "Epoch 70 (train)3.5s + (eval)0.2s: train loss = 0.03846 = (mf)0.02935 + (embed)0.00911, recall = 0.22731, ndcg = 0.34384\n",
            "Epoch 71 (train)3.5s: train loss = 0.03754 = (mf)0.02834 + (embed)0.00920\n",
            "Epoch 72 (train)4.4s: train loss = 0.03750 = (mf)0.02826 + (embed)0.00924\n",
            "Epoch 73 (train)3.5s: train loss = 0.03768 = (mf)0.02838 + (embed)0.00930\n",
            "Epoch 74 (train)3.5s: train loss = 0.03777 = (mf)0.02842 + (embed)0.00935\n",
            "Epoch 75 (train)4.1s + (eval)0.8s: train loss = 0.03753 = (mf)0.02814 + (embed)0.00939, recall = 0.22738, ndcg = 0.34221\n",
            "Epoch 76 (train)3.5s: train loss = 0.03680 = (mf)0.02733 + (embed)0.00947\n",
            "Epoch 77 (train)3.5s: train loss = 0.03642 = (mf)0.02692 + (embed)0.00951\n",
            "Epoch 78 (train)3.8s: train loss = 0.03576 = (mf)0.02622 + (embed)0.00954\n",
            "Epoch 79 (train)4.1s: train loss = 0.03479 = (mf)0.02521 + (embed)0.00958\n",
            "Epoch 80 (train)3.5s + (eval)0.2s: train loss = 0.03494 = (mf)0.02532 + (embed)0.00962, recall = 0.22733, ndcg = 0.34170\n",
            "Epoch 81 (train)3.5s: train loss = 0.03573 = (mf)0.02608 + (embed)0.00965\n",
            "Epoch 82 (train)4.3s: train loss = 0.03514 = (mf)0.02544 + (embed)0.00970\n",
            "Epoch 83 (train)3.5s: train loss = 0.03453 = (mf)0.02479 + (embed)0.00974\n",
            "Epoch 84 (train)3.5s: train loss = 0.03415 = (mf)0.02439 + (embed)0.00976\n",
            "Epoch 85 (train)4.3s + (eval)0.2s: train loss = 0.03432 = (mf)0.02452 + (embed)0.00980, recall = 0.22745, ndcg = 0.34069\n",
            "Epoch 86 (train)3.5s: train loss = 0.03440 = (mf)0.02457 + (embed)0.00983\n",
            "Epoch 87 (train)3.5s: train loss = 0.03272 = (mf)0.02285 + (embed)0.00987\n",
            "Epoch 88 (train)3.8s: train loss = 0.03362 = (mf)0.02374 + (embed)0.00989\n",
            "Epoch 89 (train)4.1s: train loss = 0.03266 = (mf)0.02272 + (embed)0.00994\n",
            "Epoch 90 (train)3.5s + (eval)0.6s: train loss = 0.03326 = (mf)0.02330 + (embed)0.00996, recall = 0.22595, ndcg = 0.33908\n",
            "Epoch 91 (train)3.6s: train loss = 0.03278 = (mf)0.02277 + (embed)0.01001\n",
            "Epoch 92 (train)4.3s: train loss = 0.03205 = (mf)0.02202 + (embed)0.01003\n",
            "Epoch 93 (train)3.6s: train loss = 0.03209 = (mf)0.02202 + (embed)0.01007\n",
            "Epoch 94 (train)3.6s: train loss = 0.03192 = (mf)0.02184 + (embed)0.01008\n",
            "Epoch 95 (train)4.4s + (eval)0.2s: train loss = 0.03086 = (mf)0.02076 + (embed)0.01011, recall = 0.22577, ndcg = 0.33909\n",
            "Epoch 96 (train)3.5s: train loss = 0.03199 = (mf)0.02182 + (embed)0.01017\n",
            "Epoch 97 (train)3.5s: train loss = 0.03246 = (mf)0.02229 + (embed)0.01016\n",
            "Epoch 98 (train)4.3s: train loss = 0.03112 = (mf)0.02092 + (embed)0.01020\n",
            "Epoch 99 (train)3.6s: train loss = 0.03115 = (mf)0.02094 + (embed)0.01021\n",
            "Epoch 100 (train)3.5s + (eval)0.2s: train loss = 0.03248 = (mf)0.02224 + (embed)0.01024, recall = 0.22555, ndcg = 0.33632\n",
            "Training finished in 390.03 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.33632, Recall@20: 0.22555\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.4s: train loss = 0.41285 = (mf)0.41260 + (embed)0.00025\n",
            "Epoch 2 (train)5.0s: train loss = 0.24034 = (mf)0.23971 + (embed)0.00063\n",
            "Epoch 3 (train)4.2s: train loss = 0.22420 = (mf)0.22346 + (embed)0.00074\n",
            "Epoch 4 (train)4.2s: train loss = 0.21937 = (mf)0.21850 + (embed)0.00087\n",
            "Epoch 5 (train)4.9s + (eval)1.4s: train loss = 0.21269 = (mf)0.21168 + (embed)0.00101, recall = 0.11462, ndcg = 0.19267\n",
            "Epoch 6 (train)4.1s: train loss = 0.20291 = (mf)0.20174 + (embed)0.00117\n",
            "Epoch 7 (train)4.7s: train loss = 0.19484 = (mf)0.19348 + (embed)0.00136\n",
            "Epoch 8 (train)4.3s: train loss = 0.18349 = (mf)0.18192 + (embed)0.00156\n",
            "Epoch 9 (train)4.0s: train loss = 0.16930 = (mf)0.16751 + (embed)0.00179\n",
            "Epoch 10 (train)4.9s + (eval)0.2s: train loss = 0.15836 = (mf)0.15633 + (embed)0.00203, recall = 0.15047, ndcg = 0.24076\n",
            "Epoch 11 (train)4.0s: train loss = 0.15314 = (mf)0.15087 + (embed)0.00227\n",
            "Epoch 12 (train)4.1s: train loss = 0.14679 = (mf)0.14431 + (embed)0.00248\n",
            "Epoch 13 (train)4.9s: train loss = 0.14081 = (mf)0.13812 + (embed)0.00269\n",
            "Epoch 14 (train)4.1s: train loss = 0.13568 = (mf)0.13280 + (embed)0.00287\n",
            "Epoch 15 (train)4.1s + (eval)0.2s: train loss = 0.13342 = (mf)0.13035 + (embed)0.00307, recall = 0.15775, ndcg = 0.25047\n",
            "Epoch 16 (train)4.9s: train loss = 0.12888 = (mf)0.12563 + (embed)0.00325\n",
            "Epoch 17 (train)4.2s: train loss = 0.12195 = (mf)0.11848 + (embed)0.00347\n",
            "Epoch 18 (train)4.1s: train loss = 0.12063 = (mf)0.11700 + (embed)0.00362\n",
            "Epoch 19 (train)5.0s: train loss = 0.11351 = (mf)0.10970 + (embed)0.00381\n",
            "Epoch 20 (train)4.2s + (eval)0.6s: train loss = 0.11514 = (mf)0.11112 + (embed)0.00402, recall = 0.16684, ndcg = 0.26415\n",
            "Epoch 21 (train)4.7s: train loss = 0.10687 = (mf)0.10265 + (embed)0.00421\n",
            "Epoch 22 (train)4.6s: train loss = 0.10620 = (mf)0.10178 + (embed)0.00442\n",
            "Epoch 23 (train)4.1s: train loss = 0.10503 = (mf)0.10042 + (embed)0.00461\n",
            "Epoch 24 (train)4.9s: train loss = 0.09723 = (mf)0.09243 + (embed)0.00480\n",
            "Epoch 25 (train)4.1s + (eval)0.2s: train loss = 0.09886 = (mf)0.09384 + (embed)0.00502, recall = 0.17262, ndcg = 0.27300\n",
            "Epoch 26 (train)4.2s: train loss = 0.09281 = (mf)0.08759 + (embed)0.00522\n",
            "Epoch 27 (train)5.1s: train loss = 0.09272 = (mf)0.08731 + (embed)0.00541\n",
            "Epoch 28 (train)4.2s: train loss = 0.09054 = (mf)0.08495 + (embed)0.00559\n",
            "Epoch 29 (train)4.2s: train loss = 0.08694 = (mf)0.08114 + (embed)0.00580\n",
            "Epoch 30 (train)5.1s + (eval)0.2s: train loss = 0.08756 = (mf)0.08159 + (embed)0.00596, recall = 0.17881, ndcg = 0.28154\n",
            "Epoch 31 (train)4.2s: train loss = 0.08461 = (mf)0.07848 + (embed)0.00613\n",
            "Epoch 32 (train)4.3s: train loss = 0.08134 = (mf)0.07502 + (embed)0.00632\n",
            "Epoch 33 (train)4.9s: train loss = 0.08041 = (mf)0.07394 + (embed)0.00647\n",
            "Epoch 34 (train)4.1s: train loss = 0.07784 = (mf)0.07119 + (embed)0.00665\n",
            "Epoch 35 (train)4.6s + (eval)0.3s: train loss = 0.07918 = (mf)0.07238 + (embed)0.00680, recall = 0.18810, ndcg = 0.29567\n",
            "Epoch 36 (train)4.5s: train loss = 0.07673 = (mf)0.06977 + (embed)0.00696\n",
            "Epoch 37 (train)4.3s: train loss = 0.07487 = (mf)0.06776 + (embed)0.00711\n",
            "Epoch 38 (train)5.3s: train loss = 0.07200 = (mf)0.06472 + (embed)0.00728\n",
            "Epoch 39 (train)4.3s: train loss = 0.06938 = (mf)0.06195 + (embed)0.00743\n",
            "Epoch 40 (train)4.2s + (eval)0.6s: train loss = 0.06990 = (mf)0.06234 + (embed)0.00756, recall = 0.19877, ndcg = 0.30867\n",
            "Epoch 41 (train)5.1s: train loss = 0.06881 = (mf)0.06108 + (embed)0.00773\n",
            "Epoch 42 (train)4.1s: train loss = 0.06729 = (mf)0.05944 + (embed)0.00785\n",
            "Epoch 43 (train)4.3s: train loss = 0.06548 = (mf)0.05749 + (embed)0.00799\n",
            "Epoch 44 (train)4.9s: train loss = 0.06533 = (mf)0.05720 + (embed)0.00812\n",
            "Epoch 45 (train)4.1s + (eval)0.2s: train loss = 0.06452 = (mf)0.05630 + (embed)0.00822, recall = 0.20431, ndcg = 0.31885\n",
            "Epoch 46 (train)4.7s: train loss = 0.06456 = (mf)0.05622 + (embed)0.00833\n",
            "Epoch 47 (train)4.4s: train loss = 0.06147 = (mf)0.05301 + (embed)0.00846\n",
            "Epoch 48 (train)4.1s: train loss = 0.06202 = (mf)0.05343 + (embed)0.00859\n",
            "Epoch 49 (train)4.9s: train loss = 0.06266 = (mf)0.05394 + (embed)0.00872\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 0.05963 = (mf)0.05080 + (embed)0.00883, recall = 0.21020, ndcg = 0.32447\n",
            "Epoch 51 (train)4.1s: train loss = 0.06023 = (mf)0.05130 + (embed)0.00893\n",
            "Epoch 52 (train)4.9s: train loss = 0.05948 = (mf)0.05044 + (embed)0.00903\n",
            "Epoch 53 (train)4.1s: train loss = 0.05824 = (mf)0.04907 + (embed)0.00917\n",
            "Epoch 54 (train)4.1s: train loss = 0.05652 = (mf)0.04725 + (embed)0.00927\n",
            "Epoch 55 (train)4.9s + (eval)0.6s: train loss = 0.05637 = (mf)0.04699 + (embed)0.00937, recall = 0.21173, ndcg = 0.32659\n",
            "Epoch 56 (train)4.1s: train loss = 0.05501 = (mf)0.04552 + (embed)0.00948\n",
            "Epoch 57 (train)4.3s: train loss = 0.05429 = (mf)0.04472 + (embed)0.00956\n",
            "Epoch 58 (train)4.8s: train loss = 0.05374 = (mf)0.04408 + (embed)0.00966\n",
            "Epoch 59 (train)4.1s: train loss = 0.05401 = (mf)0.04426 + (embed)0.00974\n",
            "Epoch 60 (train)4.4s + (eval)0.3s: train loss = 0.05482 = (mf)0.04500 + (embed)0.00983, recall = 0.21827, ndcg = 0.33423\n",
            "Epoch 61 (train)4.5s: train loss = 0.05270 = (mf)0.04277 + (embed)0.00993\n",
            "Epoch 62 (train)4.1s: train loss = 0.05193 = (mf)0.04194 + (embed)0.00999\n",
            "Epoch 63 (train)5.0s: train loss = 0.05082 = (mf)0.04071 + (embed)0.01011\n",
            "Epoch 64 (train)4.4s: train loss = 0.05084 = (mf)0.04068 + (embed)0.01016\n",
            "Epoch 65 (train)4.1s + (eval)0.2s: train loss = 0.04948 = (mf)0.03923 + (embed)0.01025, recall = 0.22167, ndcg = 0.33796\n",
            "Epoch 66 (train)4.9s: train loss = 0.04975 = (mf)0.03945 + (embed)0.01030\n",
            "Epoch 67 (train)4.0s: train loss = 0.04913 = (mf)0.03876 + (embed)0.01038\n",
            "Epoch 68 (train)4.1s: train loss = 0.04841 = (mf)0.03796 + (embed)0.01045\n",
            "Epoch 69 (train)4.9s: train loss = 0.04713 = (mf)0.03662 + (embed)0.01051\n",
            "Epoch 70 (train)4.1s + (eval)0.2s: train loss = 0.04819 = (mf)0.03761 + (embed)0.01058, recall = 0.22011, ndcg = 0.33588\n",
            "Epoch 71 (train)4.1s: train loss = 0.04749 = (mf)0.03685 + (embed)0.01065\n",
            "Epoch 72 (train)4.9s: train loss = 0.04679 = (mf)0.03608 + (embed)0.01071\n",
            "Epoch 73 (train)4.2s: train loss = 0.04668 = (mf)0.03589 + (embed)0.01079\n",
            "Epoch 74 (train)4.4s: train loss = 0.04800 = (mf)0.03717 + (embed)0.01084\n",
            "Epoch 75 (train)4.7s + (eval)0.6s: train loss = 0.04508 = (mf)0.03419 + (embed)0.01089, recall = 0.22315, ndcg = 0.33950\n",
            "Epoch 76 (train)4.1s: train loss = 0.04475 = (mf)0.03378 + (embed)0.01096\n",
            "Epoch 77 (train)4.8s: train loss = 0.04502 = (mf)0.03402 + (embed)0.01100\n",
            "Epoch 78 (train)4.2s: train loss = 0.04593 = (mf)0.03485 + (embed)0.01108\n",
            "Epoch 79 (train)4.1s: train loss = 0.04565 = (mf)0.03452 + (embed)0.01113\n",
            "Epoch 80 (train)4.9s + (eval)0.2s: train loss = 0.04422 = (mf)0.03305 + (embed)0.01117, recall = 0.22432, ndcg = 0.34045\n",
            "Epoch 81 (train)4.1s: train loss = 0.04418 = (mf)0.03298 + (embed)0.01120\n",
            "Epoch 82 (train)4.1s: train loss = 0.04311 = (mf)0.03184 + (embed)0.01126\n",
            "Epoch 83 (train)4.9s: train loss = 0.04303 = (mf)0.03172 + (embed)0.01131\n",
            "Epoch 84 (train)4.1s: train loss = 0.04312 = (mf)0.03176 + (embed)0.01137\n",
            "Epoch 85 (train)4.2s + (eval)0.2s: train loss = 0.04404 = (mf)0.03262 + (embed)0.01141, recall = 0.22487, ndcg = 0.34114\n",
            "Epoch 86 (train)5.0s: train loss = 0.04236 = (mf)0.03090 + (embed)0.01147\n",
            "Epoch 87 (train)4.2s: train loss = 0.04202 = (mf)0.03052 + (embed)0.01150\n",
            "Epoch 88 (train)4.2s: train loss = 0.04181 = (mf)0.03027 + (embed)0.01154\n",
            "Epoch 89 (train)4.9s: train loss = 0.04081 = (mf)0.02924 + (embed)0.01157\n",
            "Epoch 90 (train)4.1s + (eval)0.6s: train loss = 0.04113 = (mf)0.02954 + (embed)0.01159, recall = 0.22363, ndcg = 0.34393\n",
            "Epoch 91 (train)4.7s: train loss = 0.04071 = (mf)0.02905 + (embed)0.01166\n",
            "Epoch 92 (train)4.4s: train loss = 0.04171 = (mf)0.03004 + (embed)0.01166\n",
            "Epoch 93 (train)4.1s: train loss = 0.04039 = (mf)0.02866 + (embed)0.01173\n",
            "Epoch 94 (train)4.9s: train loss = 0.03987 = (mf)0.02810 + (embed)0.01177\n",
            "Epoch 95 (train)4.1s + (eval)0.2s: train loss = 0.04010 = (mf)0.02830 + (embed)0.01180, recall = 0.22344, ndcg = 0.34210\n",
            "Epoch 96 (train)4.1s: train loss = 0.04036 = (mf)0.02852 + (embed)0.01184\n",
            "Epoch 97 (train)5.0s: train loss = 0.04025 = (mf)0.02836 + (embed)0.01189\n",
            "Epoch 98 (train)4.0s: train loss = 0.04015 = (mf)0.02824 + (embed)0.01191\n",
            "Epoch 99 (train)4.1s: train loss = 0.03853 = (mf)0.02661 + (embed)0.01192\n",
            "Epoch 100 (train)4.9s + (eval)0.2s: train loss = 0.03826 = (mf)0.02628 + (embed)0.01197, recall = 0.22492, ndcg = 0.34202\n",
            "Training finished in 451.46 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.34202, Recall@20: 0.22492\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.2s: train loss = 0.21756 = (mf)0.21591 + (embed)0.00165\n",
            "Epoch 2 (train)2.9s: train loss = 0.11044 = (mf)0.10690 + (embed)0.00353\n",
            "Epoch 3 (train)2.9s: train loss = 0.08282 = (mf)0.07756 + (embed)0.00526\n",
            "Epoch 4 (train)3.3s: train loss = 0.06394 = (mf)0.05709 + (embed)0.00685\n",
            "Epoch 5 (train)3.4s + (eval)1.3s: train loss = 0.05907 = (mf)0.05084 + (embed)0.00823, recall = 0.17930, ndcg = 0.26778\n",
            "Epoch 6 (train)2.9s: train loss = 0.05497 = (mf)0.04563 + (embed)0.00934\n",
            "Epoch 7 (train)2.9s: train loss = 0.05111 = (mf)0.04070 + (embed)0.01040\n",
            "Epoch 8 (train)3.8s: train loss = 0.05023 = (mf)0.03890 + (embed)0.01133\n",
            "Epoch 9 (train)3.0s: train loss = 0.04802 = (mf)0.03578 + (embed)0.01224\n",
            "Epoch 10 (train)3.0s + (eval)0.2s: train loss = 0.04596 = (mf)0.03301 + (embed)0.01295, recall = 0.16490, ndcg = 0.23479\n",
            "Epoch 11 (train)3.0s: train loss = 0.04491 = (mf)0.03126 + (embed)0.01365\n",
            "Epoch 12 (train)3.8s: train loss = 0.04573 = (mf)0.03149 + (embed)0.01424\n",
            "Epoch 13 (train)2.9s: train loss = 0.04617 = (mf)0.03139 + (embed)0.01478\n",
            "Epoch 14 (train)2.9s: train loss = 0.04645 = (mf)0.03100 + (embed)0.01545\n",
            "Epoch 15 (train)2.9s + (eval)0.2s: train loss = 0.04625 = (mf)0.03029 + (embed)0.01596, recall = 0.15950, ndcg = 0.22466\n",
            "Epoch 16 (train)3.7s: train loss = 0.04567 = (mf)0.02927 + (embed)0.01640\n",
            "Epoch 17 (train)2.9s: train loss = 0.04807 = (mf)0.03113 + (embed)0.01695\n",
            "Epoch 18 (train)2.9s: train loss = 0.04623 = (mf)0.02880 + (embed)0.01743\n",
            "Epoch 19 (train)3.0s: train loss = 0.04435 = (mf)0.02652 + (embed)0.01783\n",
            "Epoch 20 (train)4.4s + (eval)0.6s: train loss = 0.04688 = (mf)0.02877 + (embed)0.01812, recall = 0.16824, ndcg = 0.23675\n",
            "Epoch 21 (train)3.0s: train loss = 0.04533 = (mf)0.02679 + (embed)0.01854\n",
            "Epoch 22 (train)3.0s: train loss = 0.04956 = (mf)0.03065 + (embed)0.01891\n",
            "Epoch 23 (train)3.5s: train loss = 0.04573 = (mf)0.02643 + (embed)0.01931\n",
            "Epoch 24 (train)3.3s: train loss = 0.04602 = (mf)0.02640 + (embed)0.01962\n",
            "Epoch 25 (train)3.0s + (eval)0.2s: train loss = 0.04713 = (mf)0.02722 + (embed)0.01991, recall = 0.15980, ndcg = 0.22886\n",
            "Epoch 26 (train)3.0s: train loss = 0.04765 = (mf)0.02737 + (embed)0.02028\n",
            "Epoch 27 (train)3.6s: train loss = 0.04822 = (mf)0.02766 + (embed)0.02057\n",
            "Epoch 28 (train)3.1s: train loss = 0.04778 = (mf)0.02695 + (embed)0.02083\n",
            "Epoch 29 (train)3.0s: train loss = 0.04688 = (mf)0.02580 + (embed)0.02109\n",
            "Epoch 30 (train)3.0s + (eval)0.2s: train loss = 0.04960 = (mf)0.02817 + (embed)0.02143, recall = 0.16239, ndcg = 0.23244\n",
            "Epoch 31 (train)3.9s: train loss = 0.04702 = (mf)0.02528 + (embed)0.02174\n",
            "Epoch 32 (train)3.0s: train loss = 0.04871 = (mf)0.02679 + (embed)0.02193\n",
            "Epoch 33 (train)3.0s: train loss = 0.05185 = (mf)0.02964 + (embed)0.02221\n",
            "Epoch 34 (train)3.0s: train loss = 0.04915 = (mf)0.02666 + (embed)0.02249\n",
            "Epoch 35 (train)3.8s + (eval)0.2s: train loss = 0.05042 = (mf)0.02774 + (embed)0.02268, recall = 0.15661, ndcg = 0.22520\n",
            "Epoch 36 (train)3.0s: train loss = 0.05215 = (mf)0.02924 + (embed)0.02291\n",
            "Epoch 37 (train)3.0s: train loss = 0.04798 = (mf)0.02483 + (embed)0.02315\n",
            "Epoch 38 (train)3.0s: train loss = 0.04971 = (mf)0.02641 + (embed)0.02330\n",
            "Epoch 39 (train)3.8s: train loss = 0.04952 = (mf)0.02595 + (embed)0.02357\n",
            "Epoch 40 (train)2.9s + (eval)0.6s: train loss = 0.05074 = (mf)0.02699 + (embed)0.02375, recall = 0.15332, ndcg = 0.22259\n",
            "Epoch 41 (train)2.9s: train loss = 0.04979 = (mf)0.02583 + (embed)0.02396\n",
            "Epoch 42 (train)3.0s: train loss = 0.05118 = (mf)0.02699 + (embed)0.02419\n",
            "Epoch 43 (train)3.8s: train loss = 0.05051 = (mf)0.02609 + (embed)0.02442\n",
            "Epoch 44 (train)3.0s: train loss = 0.04896 = (mf)0.02436 + (embed)0.02461\n",
            "Epoch 45 (train)2.9s + (eval)0.2s: train loss = 0.05240 = (mf)0.02763 + (embed)0.02477, recall = 0.15217, ndcg = 0.21707\n",
            "Epoch 46 (train)3.1s: train loss = 0.05252 = (mf)0.02755 + (embed)0.02497\n",
            "Epoch 47 (train)3.6s: train loss = 0.05252 = (mf)0.02728 + (embed)0.02524\n",
            "Epoch 48 (train)2.9s: train loss = 0.05137 = (mf)0.02603 + (embed)0.02534\n",
            "Epoch 49 (train)3.0s: train loss = 0.05225 = (mf)0.02668 + (embed)0.02557\n",
            "Epoch 50 (train)3.1s + (eval)0.3s: train loss = 0.05219 = (mf)0.02649 + (embed)0.02570, recall = 0.14928, ndcg = 0.21257\n",
            "Epoch 51 (train)3.6s: train loss = 0.05238 = (mf)0.02658 + (embed)0.02580\n",
            "Epoch 52 (train)2.9s: train loss = 0.05335 = (mf)0.02740 + (embed)0.02595\n",
            "Epoch 53 (train)2.9s: train loss = 0.05412 = (mf)0.02805 + (embed)0.02608\n",
            "Epoch 54 (train)3.2s: train loss = 0.05026 = (mf)0.02410 + (embed)0.02615\n",
            "Epoch 55 (train)3.5s + (eval)0.6s: train loss = 0.04980 = (mf)0.02357 + (embed)0.02624, recall = 0.15117, ndcg = 0.21739\n",
            "Epoch 56 (train)2.9s: train loss = 0.05237 = (mf)0.02607 + (embed)0.02630\n",
            "Epoch 57 (train)2.9s: train loss = 0.05301 = (mf)0.02653 + (embed)0.02647\n",
            "Epoch 58 (train)3.6s: train loss = 0.05110 = (mf)0.02445 + (embed)0.02665\n",
            "Epoch 59 (train)3.2s: train loss = 0.05183 = (mf)0.02511 + (embed)0.02672\n",
            "Epoch 60 (train)3.0s + (eval)0.2s: train loss = 0.05134 = (mf)0.02448 + (embed)0.02687, recall = 0.15143, ndcg = 0.21717\n",
            "Epoch 61 (train)3.0s: train loss = 0.05304 = (mf)0.02603 + (embed)0.02700\n",
            "Epoch 62 (train)3.7s: train loss = 0.05213 = (mf)0.02507 + (embed)0.02706\n",
            "Epoch 63 (train)3.1s: train loss = 0.05414 = (mf)0.02694 + (embed)0.02721\n",
            "Epoch 64 (train)2.9s: train loss = 0.05350 = (mf)0.02620 + (embed)0.02730\n",
            "Epoch 65 (train)2.9s + (eval)0.2s: train loss = 0.05465 = (mf)0.02725 + (embed)0.02740, recall = 0.14638, ndcg = 0.20948\n",
            "Epoch 66 (train)3.8s: train loss = 0.05693 = (mf)0.02941 + (embed)0.02752\n",
            "Epoch 67 (train)3.0s: train loss = 0.05388 = (mf)0.02625 + (embed)0.02763\n",
            "Epoch 68 (train)2.9s: train loss = 0.05124 = (mf)0.02363 + (embed)0.02762\n",
            "Epoch 69 (train)2.9s: train loss = 0.05299 = (mf)0.02536 + (embed)0.02763\n",
            "Epoch 70 (train)3.8s + (eval)0.2s: train loss = 0.05333 = (mf)0.02552 + (embed)0.02781, recall = 0.14509, ndcg = 0.20361\n",
            "Epoch 71 (train)2.9s: train loss = 0.05468 = (mf)0.02685 + (embed)0.02782\n",
            "Epoch 72 (train)3.2s: train loss = 0.05330 = (mf)0.02539 + (embed)0.02791\n",
            "Epoch 73 (train)2.9s: train loss = 0.05219 = (mf)0.02421 + (embed)0.02798\n",
            "Epoch 74 (train)3.8s: train loss = 0.05262 = (mf)0.02459 + (embed)0.02804\n",
            "Epoch 75 (train)3.0s + (eval)0.6s: train loss = 0.05456 = (mf)0.02646 + (embed)0.02810, recall = 0.14021, ndcg = 0.20029\n",
            "Epoch 76 (train)3.1s: train loss = 0.05627 = (mf)0.02805 + (embed)0.02823\n",
            "Epoch 77 (train)3.1s: train loss = 0.05237 = (mf)0.02410 + (embed)0.02827\n",
            "Epoch 78 (train)3.6s: train loss = 0.05245 = (mf)0.02411 + (embed)0.02834\n",
            "Epoch 79 (train)2.9s: train loss = 0.05175 = (mf)0.02346 + (embed)0.02829\n",
            "Epoch 80 (train)2.9s + (eval)0.2s: train loss = 0.05126 = (mf)0.02290 + (embed)0.02836, recall = 0.15282, ndcg = 0.21144\n",
            "Epoch 81 (train)3.3s: train loss = 0.05365 = (mf)0.02522 + (embed)0.02843\n",
            "Epoch 82 (train)3.5s: train loss = 0.05484 = (mf)0.02632 + (embed)0.02852\n",
            "Epoch 83 (train)3.0s: train loss = 0.05114 = (mf)0.02253 + (embed)0.02861\n",
            "Epoch 84 (train)3.0s: train loss = 0.05211 = (mf)0.02350 + (embed)0.02861\n",
            "Epoch 85 (train)3.5s + (eval)0.3s: train loss = 0.05232 = (mf)0.02379 + (embed)0.02853, recall = 0.15460, ndcg = 0.21695\n",
            "Epoch 86 (train)3.4s: train loss = 0.05498 = (mf)0.02635 + (embed)0.02863\n",
            "Epoch 87 (train)3.0s: train loss = 0.05247 = (mf)0.02385 + (embed)0.02862\n",
            "Epoch 88 (train)3.0s: train loss = 0.05294 = (mf)0.02421 + (embed)0.02873\n",
            "Epoch 89 (train)3.7s: train loss = 0.05444 = (mf)0.02558 + (embed)0.02885\n",
            "Epoch 90 (train)3.2s + (eval)0.2s: train loss = 0.05197 = (mf)0.02298 + (embed)0.02899, recall = 0.14959, ndcg = 0.21180\n",
            "Epoch 91 (train)3.0s: train loss = 0.05372 = (mf)0.02469 + (embed)0.02903\n",
            "Epoch 92 (train)3.0s: train loss = 0.05498 = (mf)0.02591 + (embed)0.02907\n",
            "Epoch 93 (train)3.9s: train loss = 0.05468 = (mf)0.02552 + (embed)0.02916\n",
            "Epoch 94 (train)3.0s: train loss = 0.04826 = (mf)0.01921 + (embed)0.02905\n",
            "Epoch 95 (train)3.0s + (eval)0.6s: train loss = 0.05386 = (mf)0.02488 + (embed)0.02898, recall = 0.14734, ndcg = 0.21211\n",
            "Epoch 96 (train)3.0s: train loss = 0.05266 = (mf)0.02351 + (embed)0.02915\n",
            "Epoch 97 (train)3.8s: train loss = 0.05253 = (mf)0.02334 + (embed)0.02919\n",
            "Epoch 98 (train)3.0s: train loss = 0.05574 = (mf)0.02658 + (embed)0.02916\n",
            "Epoch 99 (train)3.0s: train loss = 0.05160 = (mf)0.02236 + (embed)0.02924\n",
            "Epoch 100 (train)2.9s + (eval)0.2s: train loss = 0.05006 = (mf)0.02078 + (embed)0.02928, recall = 0.14454, ndcg = 0.20429\n",
            "Training finished in 327.86 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.20429, Recall@20: 0.14454\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.8s: train loss = 0.22466 = (mf)0.22305 + (embed)0.00160\n",
            "Epoch 2 (train)3.6s: train loss = 0.12047 = (mf)0.11672 + (embed)0.00375\n",
            "Epoch 3 (train)3.5s: train loss = 0.08651 = (mf)0.08086 + (embed)0.00565\n",
            "Epoch 4 (train)3.7s: train loss = 0.07230 = (mf)0.06489 + (embed)0.00741\n",
            "Epoch 5 (train)4.1s + (eval)1.4s: train loss = 0.06388 = (mf)0.05491 + (embed)0.00897, recall = 0.18853, ndcg = 0.27837\n",
            "Epoch 6 (train)3.5s: train loss = 0.05829 = (mf)0.04796 + (embed)0.01033\n",
            "Epoch 7 (train)3.9s: train loss = 0.05143 = (mf)0.03995 + (embed)0.01149\n",
            "Epoch 8 (train)4.1s: train loss = 0.05054 = (mf)0.03812 + (embed)0.01242\n",
            "Epoch 9 (train)3.5s: train loss = 0.04805 = (mf)0.03478 + (embed)0.01326\n",
            "Epoch 10 (train)3.5s + (eval)0.2s: train loss = 0.04981 = (mf)0.03589 + (embed)0.01392, recall = 0.18643, ndcg = 0.27675\n",
            "Epoch 11 (train)4.3s: train loss = 0.04555 = (mf)0.03091 + (embed)0.01464\n",
            "Epoch 12 (train)3.5s: train loss = 0.04532 = (mf)0.03008 + (embed)0.01524\n",
            "Epoch 13 (train)3.5s: train loss = 0.04479 = (mf)0.02904 + (embed)0.01575\n",
            "Epoch 14 (train)4.3s: train loss = 0.04488 = (mf)0.02866 + (embed)0.01622\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.04433 = (mf)0.02764 + (embed)0.01668, recall = 0.18035, ndcg = 0.25584\n",
            "Epoch 16 (train)3.6s: train loss = 0.04227 = (mf)0.02514 + (embed)0.01713\n",
            "Epoch 17 (train)4.1s: train loss = 0.04322 = (mf)0.02580 + (embed)0.01742\n",
            "Epoch 18 (train)3.9s: train loss = 0.04219 = (mf)0.02442 + (embed)0.01778\n",
            "Epoch 19 (train)3.6s: train loss = 0.04238 = (mf)0.02428 + (embed)0.01810\n",
            "Epoch 20 (train)3.6s + (eval)0.3s: train loss = 0.04102 = (mf)0.02264 + (embed)0.01838, recall = 0.18288, ndcg = 0.26463\n",
            "Epoch 21 (train)4.3s: train loss = 0.04241 = (mf)0.02378 + (embed)0.01863\n",
            "Epoch 22 (train)3.5s: train loss = 0.04221 = (mf)0.02327 + (embed)0.01894\n",
            "Epoch 23 (train)3.6s: train loss = 0.04185 = (mf)0.02264 + (embed)0.01921\n",
            "Epoch 24 (train)4.4s: train loss = 0.04138 = (mf)0.02201 + (embed)0.01937\n",
            "Epoch 25 (train)3.5s + (eval)0.6s: train loss = 0.04122 = (mf)0.02162 + (embed)0.01959, recall = 0.18337, ndcg = 0.25961\n",
            "Epoch 26 (train)3.6s: train loss = 0.04099 = (mf)0.02115 + (embed)0.01985\n",
            "Epoch 27 (train)4.3s: train loss = 0.04294 = (mf)0.02284 + (embed)0.02010\n",
            "Epoch 28 (train)3.5s: train loss = 0.04388 = (mf)0.02359 + (embed)0.02029\n",
            "Epoch 29 (train)3.5s: train loss = 0.04224 = (mf)0.02169 + (embed)0.02055\n",
            "Epoch 30 (train)4.1s + (eval)0.3s: train loss = 0.04125 = (mf)0.02057 + (embed)0.02068, recall = 0.18124, ndcg = 0.25923\n",
            "Epoch 31 (train)3.7s: train loss = 0.04268 = (mf)0.02172 + (embed)0.02096\n",
            "Epoch 32 (train)3.6s: train loss = 0.04222 = (mf)0.02103 + (embed)0.02120\n",
            "Epoch 33 (train)3.8s: train loss = 0.04389 = (mf)0.02255 + (embed)0.02135\n",
            "Epoch 34 (train)4.3s: train loss = 0.04181 = (mf)0.02034 + (embed)0.02147\n",
            "Epoch 35 (train)3.6s + (eval)0.2s: train loss = 0.04056 = (mf)0.01892 + (embed)0.02164, recall = 0.18202, ndcg = 0.25462\n",
            "Epoch 36 (train)3.5s: train loss = 0.04088 = (mf)0.01908 + (embed)0.02180\n",
            "Epoch 37 (train)4.3s: train loss = 0.04075 = (mf)0.01891 + (embed)0.02184\n",
            "Epoch 38 (train)3.6s: train loss = 0.04098 = (mf)0.01907 + (embed)0.02191\n",
            "Epoch 39 (train)3.5s: train loss = 0.04177 = (mf)0.01974 + (embed)0.02203\n",
            "Epoch 40 (train)4.4s + (eval)0.6s: train loss = 0.04132 = (mf)0.01921 + (embed)0.02212, recall = 0.17471, ndcg = 0.24667\n",
            "Epoch 41 (train)3.5s: train loss = 0.04184 = (mf)0.01963 + (embed)0.02221\n",
            "Epoch 42 (train)3.5s: train loss = 0.04223 = (mf)0.01992 + (embed)0.02232\n",
            "Epoch 43 (train)4.3s: train loss = 0.04055 = (mf)0.01812 + (embed)0.02243\n",
            "Epoch 44 (train)3.7s: train loss = 0.04191 = (mf)0.01932 + (embed)0.02259\n",
            "Epoch 45 (train)3.6s + (eval)0.2s: train loss = 0.04238 = (mf)0.01974 + (embed)0.02265, recall = 0.17622, ndcg = 0.24592\n",
            "Epoch 46 (train)4.0s: train loss = 0.04054 = (mf)0.01775 + (embed)0.02279\n",
            "Epoch 47 (train)4.1s: train loss = 0.04254 = (mf)0.01960 + (embed)0.02294\n",
            "Epoch 48 (train)3.6s: train loss = 0.04159 = (mf)0.01859 + (embed)0.02300\n",
            "Epoch 49 (train)3.6s: train loss = 0.04076 = (mf)0.01767 + (embed)0.02309\n",
            "Epoch 50 (train)4.3s + (eval)0.2s: train loss = 0.04174 = (mf)0.01859 + (embed)0.02315, recall = 0.18232, ndcg = 0.25452\n",
            "Epoch 51 (train)3.5s: train loss = 0.04181 = (mf)0.01863 + (embed)0.02318\n",
            "Epoch 52 (train)3.5s: train loss = 0.04193 = (mf)0.01867 + (embed)0.02325\n",
            "Epoch 53 (train)4.3s: train loss = 0.04263 = (mf)0.01923 + (embed)0.02341\n",
            "Epoch 54 (train)3.5s: train loss = 0.03961 = (mf)0.01616 + (embed)0.02344\n",
            "Epoch 55 (train)3.6s + (eval)0.2s: train loss = 0.04118 = (mf)0.01776 + (embed)0.02342, recall = 0.18299, ndcg = 0.25839\n",
            "Epoch 56 (train)4.2s: train loss = 0.04292 = (mf)0.01938 + (embed)0.02354\n",
            "Epoch 57 (train)3.7s: train loss = 0.04178 = (mf)0.01815 + (embed)0.02363\n",
            "Epoch 58 (train)3.6s: train loss = 0.04107 = (mf)0.01736 + (embed)0.02371\n",
            "Epoch 59 (train)3.7s: train loss = 0.04118 = (mf)0.01748 + (embed)0.02370\n",
            "Epoch 60 (train)4.3s + (eval)0.6s: train loss = 0.04074 = (mf)0.01703 + (embed)0.02371, recall = 0.17699, ndcg = 0.25097\n",
            "Epoch 61 (train)3.6s: train loss = 0.04025 = (mf)0.01650 + (embed)0.02375\n",
            "Epoch 62 (train)3.6s: train loss = 0.04209 = (mf)0.01830 + (embed)0.02379\n",
            "Epoch 63 (train)4.5s: train loss = 0.04248 = (mf)0.01866 + (embed)0.02382\n",
            "Epoch 64 (train)3.6s: train loss = 0.04117 = (mf)0.01734 + (embed)0.02383\n",
            "Epoch 65 (train)3.5s + (eval)0.2s: train loss = 0.04039 = (mf)0.01654 + (embed)0.02384, recall = 0.17373, ndcg = 0.24325\n",
            "Epoch 66 (train)4.4s: train loss = 0.04119 = (mf)0.01729 + (embed)0.02389\n",
            "Epoch 67 (train)3.5s: train loss = 0.03980 = (mf)0.01589 + (embed)0.02392\n",
            "Epoch 68 (train)3.5s: train loss = 0.04122 = (mf)0.01724 + (embed)0.02398\n",
            "Epoch 69 (train)4.2s: train loss = 0.04010 = (mf)0.01609 + (embed)0.02401\n",
            "Epoch 70 (train)3.6s + (eval)0.2s: train loss = 0.04125 = (mf)0.01716 + (embed)0.02409, recall = 0.17376, ndcg = 0.23934\n",
            "Epoch 71 (train)3.5s: train loss = 0.04131 = (mf)0.01719 + (embed)0.02412\n",
            "Epoch 72 (train)3.7s: train loss = 0.04034 = (mf)0.01617 + (embed)0.02418\n",
            "Epoch 73 (train)4.1s: train loss = 0.04079 = (mf)0.01662 + (embed)0.02417\n",
            "Epoch 74 (train)3.6s: train loss = 0.04076 = (mf)0.01650 + (embed)0.02426\n",
            "Epoch 75 (train)3.5s + (eval)0.6s: train loss = 0.04062 = (mf)0.01631 + (embed)0.02432, recall = 0.17853, ndcg = 0.24992\n",
            "Epoch 76 (train)4.4s: train loss = 0.04105 = (mf)0.01677 + (embed)0.02429\n",
            "Epoch 77 (train)3.6s: train loss = 0.04101 = (mf)0.01668 + (embed)0.02432\n",
            "Epoch 78 (train)3.5s: train loss = 0.04006 = (mf)0.01579 + (embed)0.02427\n",
            "Epoch 79 (train)4.4s: train loss = 0.04163 = (mf)0.01721 + (embed)0.02443\n",
            "Epoch 80 (train)3.6s + (eval)0.2s: train loss = 0.04019 = (mf)0.01573 + (embed)0.02446, recall = 0.17885, ndcg = 0.25282\n",
            "Epoch 81 (train)3.5s: train loss = 0.04091 = (mf)0.01651 + (embed)0.02440\n",
            "Epoch 82 (train)4.2s: train loss = 0.04070 = (mf)0.01624 + (embed)0.02446\n",
            "Epoch 83 (train)3.6s: train loss = 0.04010 = (mf)0.01568 + (embed)0.02442\n",
            "Epoch 84 (train)3.5s: train loss = 0.04036 = (mf)0.01598 + (embed)0.02439\n",
            "Epoch 85 (train)3.7s + (eval)0.3s: train loss = 0.04219 = (mf)0.01778 + (embed)0.02441, recall = 0.17081, ndcg = 0.24169\n",
            "Epoch 86 (train)4.2s: train loss = 0.04073 = (mf)0.01619 + (embed)0.02454\n",
            "Epoch 87 (train)3.6s: train loss = 0.04276 = (mf)0.01823 + (embed)0.02453\n",
            "Epoch 88 (train)3.6s: train loss = 0.04201 = (mf)0.01746 + (embed)0.02455\n",
            "Epoch 89 (train)4.4s: train loss = 0.04220 = (mf)0.01756 + (embed)0.02463\n",
            "Epoch 90 (train)3.7s + (eval)0.2s: train loss = 0.04177 = (mf)0.01713 + (embed)0.02464, recall = 0.17845, ndcg = 0.24914\n",
            "Epoch 91 (train)3.6s: train loss = 0.04041 = (mf)0.01578 + (embed)0.02463\n",
            "Epoch 92 (train)4.4s: train loss = 0.04200 = (mf)0.01739 + (embed)0.02460\n",
            "Epoch 93 (train)3.6s: train loss = 0.04139 = (mf)0.01670 + (embed)0.02470\n",
            "Epoch 94 (train)3.6s: train loss = 0.04098 = (mf)0.01629 + (embed)0.02469\n",
            "Epoch 95 (train)4.2s + (eval)0.7s: train loss = 0.04102 = (mf)0.01631 + (embed)0.02471, recall = 0.17154, ndcg = 0.24419\n",
            "Epoch 96 (train)3.5s: train loss = 0.04225 = (mf)0.01746 + (embed)0.02479\n",
            "Epoch 97 (train)3.5s: train loss = 0.04138 = (mf)0.01649 + (embed)0.02489\n",
            "Epoch 98 (train)3.9s: train loss = 0.04147 = (mf)0.01653 + (embed)0.02494\n",
            "Epoch 99 (train)4.0s: train loss = 0.04091 = (mf)0.01583 + (embed)0.02509\n",
            "Epoch 100 (train)3.6s + (eval)0.2s: train loss = 0.04166 = (mf)0.01649 + (embed)0.02517, recall = 0.18013, ndcg = 0.24735\n",
            "Training finished in 389.79 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.24735, Recall@20: 0.18013\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.24952 = (mf)0.24798 + (embed)0.00154\n",
            "Epoch 2 (train)4.9s: train loss = 0.13858 = (mf)0.13469 + (embed)0.00389\n",
            "Epoch 3 (train)4.0s: train loss = 0.10901 = (mf)0.10314 + (embed)0.00587\n",
            "Epoch 4 (train)4.1s: train loss = 0.09128 = (mf)0.08347 + (embed)0.00781\n",
            "Epoch 5 (train)5.0s + (eval)1.0s: train loss = 0.07629 = (mf)0.06670 + (embed)0.00959, recall = 0.18790, ndcg = 0.28057\n",
            "Epoch 6 (train)4.1s: train loss = 0.06802 = (mf)0.05696 + (embed)0.01106\n",
            "Epoch 7 (train)4.3s: train loss = 0.06375 = (mf)0.05137 + (embed)0.01239\n",
            "Epoch 8 (train)4.7s: train loss = 0.05963 = (mf)0.04620 + (embed)0.01343\n",
            "Epoch 9 (train)4.1s: train loss = 0.05615 = (mf)0.04182 + (embed)0.01433\n",
            "Epoch 10 (train)4.5s + (eval)0.8s: train loss = 0.05392 = (mf)0.03884 + (embed)0.01508, recall = 0.20065, ndcg = 0.29731\n",
            "Epoch 11 (train)4.2s: train loss = 0.05083 = (mf)0.03504 + (embed)0.01579\n",
            "Epoch 12 (train)4.1s: train loss = 0.04891 = (mf)0.03255 + (embed)0.01636\n",
            "Epoch 13 (train)4.9s: train loss = 0.04971 = (mf)0.03286 + (embed)0.01685\n",
            "Epoch 14 (train)4.1s: train loss = 0.04662 = (mf)0.02939 + (embed)0.01724\n",
            "Epoch 15 (train)4.1s + (eval)0.2s: train loss = 0.04649 = (mf)0.02883 + (embed)0.01765, recall = 0.19594, ndcg = 0.28948\n",
            "Epoch 16 (train)4.9s: train loss = 0.04581 = (mf)0.02778 + (embed)0.01802\n",
            "Epoch 17 (train)4.1s: train loss = 0.04504 = (mf)0.02675 + (embed)0.01829\n",
            "Epoch 18 (train)4.1s: train loss = 0.04516 = (mf)0.02650 + (embed)0.01865\n",
            "Epoch 19 (train)4.9s: train loss = 0.04287 = (mf)0.02394 + (embed)0.01893\n",
            "Epoch 20 (train)4.1s + (eval)0.2s: train loss = 0.04305 = (mf)0.02387 + (embed)0.01919, recall = 0.19980, ndcg = 0.29515\n",
            "Epoch 21 (train)4.1s: train loss = 0.04312 = (mf)0.02361 + (embed)0.01950\n",
            "Epoch 22 (train)4.9s: train loss = 0.04194 = (mf)0.02227 + (embed)0.01968\n",
            "Epoch 23 (train)4.1s: train loss = 0.04162 = (mf)0.02177 + (embed)0.01986\n",
            "Epoch 24 (train)4.4s: train loss = 0.04268 = (mf)0.02255 + (embed)0.02013\n",
            "Epoch 25 (train)4.7s + (eval)0.6s: train loss = 0.04190 = (mf)0.02162 + (embed)0.02028, recall = 0.20305, ndcg = 0.28934\n",
            "Epoch 26 (train)4.1s: train loss = 0.04174 = (mf)0.02127 + (embed)0.02047\n",
            "Epoch 27 (train)4.9s: train loss = 0.04098 = (mf)0.02038 + (embed)0.02060\n",
            "Epoch 28 (train)4.1s: train loss = 0.04240 = (mf)0.02157 + (embed)0.02083\n",
            "Epoch 29 (train)4.1s: train loss = 0.04123 = (mf)0.02017 + (embed)0.02107\n",
            "Epoch 30 (train)5.0s + (eval)0.2s: train loss = 0.04246 = (mf)0.02120 + (embed)0.02126, recall = 0.19512, ndcg = 0.27981\n",
            "Epoch 31 (train)4.1s: train loss = 0.04115 = (mf)0.01973 + (embed)0.02141\n",
            "Epoch 32 (train)4.2s: train loss = 0.03986 = (mf)0.01837 + (embed)0.02149\n",
            "Epoch 33 (train)5.1s: train loss = 0.03978 = (mf)0.01809 + (embed)0.02169\n",
            "Epoch 34 (train)4.2s: train loss = 0.03939 = (mf)0.01765 + (embed)0.02174\n",
            "Epoch 35 (train)4.3s + (eval)0.3s: train loss = 0.03980 = (mf)0.01804 + (embed)0.02176, recall = 0.19300, ndcg = 0.27973\n",
            "Epoch 36 (train)5.0s: train loss = 0.04076 = (mf)0.01894 + (embed)0.02182\n",
            "Epoch 37 (train)4.3s: train loss = 0.04050 = (mf)0.01847 + (embed)0.02202\n",
            "Epoch 38 (train)5.0s: train loss = 0.04141 = (mf)0.01919 + (embed)0.02222\n",
            "Epoch 39 (train)4.3s: train loss = 0.03946 = (mf)0.01711 + (embed)0.02235\n",
            "Epoch 40 (train)4.2s + (eval)0.2s: train loss = 0.04116 = (mf)0.01871 + (embed)0.02245, recall = 0.19285, ndcg = 0.27758\n",
            "Epoch 41 (train)5.0s: train loss = 0.04130 = (mf)0.01874 + (embed)0.02255\n",
            "Epoch 42 (train)4.2s: train loss = 0.03982 = (mf)0.01724 + (embed)0.02259\n",
            "Epoch 43 (train)4.2s: train loss = 0.04060 = (mf)0.01800 + (embed)0.02260\n",
            "Epoch 44 (train)5.1s: train loss = 0.04042 = (mf)0.01764 + (embed)0.02278\n",
            "Epoch 45 (train)4.2s + (eval)0.6s: train loss = 0.04055 = (mf)0.01768 + (embed)0.02287, recall = 0.18692, ndcg = 0.27039\n",
            "Epoch 46 (train)4.7s: train loss = 0.03938 = (mf)0.01639 + (embed)0.02299\n",
            "Epoch 47 (train)4.7s: train loss = 0.03982 = (mf)0.01669 + (embed)0.02313\n",
            "Epoch 48 (train)4.2s: train loss = 0.03976 = (mf)0.01662 + (embed)0.02315\n",
            "Epoch 49 (train)4.8s: train loss = 0.03986 = (mf)0.01677 + (embed)0.02309\n",
            "Epoch 50 (train)4.2s + (eval)0.2s: train loss = 0.03957 = (mf)0.01635 + (embed)0.02322, recall = 0.19686, ndcg = 0.27712\n",
            "Epoch 51 (train)4.1s: train loss = 0.03956 = (mf)0.01630 + (embed)0.02326\n",
            "Epoch 52 (train)4.9s: train loss = 0.04003 = (mf)0.01672 + (embed)0.02331\n",
            "Epoch 53 (train)4.1s: train loss = 0.03945 = (mf)0.01613 + (embed)0.02331\n",
            "Epoch 54 (train)4.0s: train loss = 0.03922 = (mf)0.01582 + (embed)0.02340\n",
            "Epoch 55 (train)4.9s + (eval)0.2s: train loss = 0.04009 = (mf)0.01660 + (embed)0.02349, recall = 0.19738, ndcg = 0.27791\n",
            "Epoch 56 (train)4.1s: train loss = 0.03990 = (mf)0.01639 + (embed)0.02350\n",
            "Epoch 57 (train)4.1s: train loss = 0.03987 = (mf)0.01628 + (embed)0.02359\n",
            "Epoch 58 (train)5.1s: train loss = 0.03975 = (mf)0.01615 + (embed)0.02360\n",
            "Epoch 59 (train)4.2s: train loss = 0.04093 = (mf)0.01728 + (embed)0.02365\n",
            "Epoch 60 (train)4.3s + (eval)0.3s: train loss = 0.03977 = (mf)0.01603 + (embed)0.02374, recall = 0.19877, ndcg = 0.28277\n",
            "Epoch 61 (train)4.7s: train loss = 0.03996 = (mf)0.01627 + (embed)0.02370\n",
            "Epoch 62 (train)4.1s: train loss = 0.03951 = (mf)0.01581 + (embed)0.02370\n",
            "Epoch 63 (train)4.6s: train loss = 0.03987 = (mf)0.01606 + (embed)0.02381\n",
            "Epoch 64 (train)4.5s: train loss = 0.03926 = (mf)0.01536 + (embed)0.02390\n",
            "Epoch 65 (train)4.2s + (eval)0.6s: train loss = 0.03956 = (mf)0.01566 + (embed)0.02390, recall = 0.19789, ndcg = 0.28162\n",
            "Epoch 66 (train)5.0s: train loss = 0.04029 = (mf)0.01638 + (embed)0.02390\n",
            "Epoch 67 (train)4.1s: train loss = 0.03991 = (mf)0.01596 + (embed)0.02395\n",
            "Epoch 68 (train)4.2s: train loss = 0.03914 = (mf)0.01516 + (embed)0.02398\n",
            "Epoch 69 (train)5.0s: train loss = 0.03919 = (mf)0.01509 + (embed)0.02409\n",
            "Epoch 70 (train)4.2s + (eval)0.2s: train loss = 0.04085 = (mf)0.01678 + (embed)0.02407, recall = 0.19298, ndcg = 0.27913\n",
            "Epoch 71 (train)4.2s: train loss = 0.04026 = (mf)0.01610 + (embed)0.02416\n",
            "Epoch 72 (train)5.2s: train loss = 0.03852 = (mf)0.01435 + (embed)0.02418\n",
            "Epoch 73 (train)4.2s: train loss = 0.03943 = (mf)0.01527 + (embed)0.02417\n",
            "Epoch 74 (train)4.5s: train loss = 0.03898 = (mf)0.01477 + (embed)0.02422\n",
            "Epoch 75 (train)4.6s + (eval)0.2s: train loss = 0.03903 = (mf)0.01494 + (embed)0.02410, recall = 0.19039, ndcg = 0.27517\n",
            "Epoch 76 (train)4.2s: train loss = 0.03972 = (mf)0.01552 + (embed)0.02420\n",
            "Epoch 77 (train)4.9s: train loss = 0.03938 = (mf)0.01513 + (embed)0.02426\n",
            "Epoch 78 (train)4.1s: train loss = 0.03918 = (mf)0.01489 + (embed)0.02430\n",
            "Epoch 79 (train)4.2s: train loss = 0.03916 = (mf)0.01494 + (embed)0.02423\n",
            "Epoch 80 (train)5.0s + (eval)0.6s: train loss = 0.04081 = (mf)0.01656 + (embed)0.02426, recall = 0.19645, ndcg = 0.27438\n",
            "Epoch 81 (train)4.2s: train loss = 0.03953 = (mf)0.01520 + (embed)0.02433\n",
            "Epoch 82 (train)4.1s: train loss = 0.03883 = (mf)0.01444 + (embed)0.02439\n",
            "Epoch 83 (train)4.9s: train loss = 0.04053 = (mf)0.01619 + (embed)0.02434\n",
            "Epoch 84 (train)4.1s: train loss = 0.03839 = (mf)0.01398 + (embed)0.02441\n",
            "Epoch 85 (train)4.1s + (eval)0.3s: train loss = 0.03981 = (mf)0.01546 + (embed)0.02436, recall = 0.19918, ndcg = 0.28267\n",
            "Epoch 86 (train)4.8s: train loss = 0.03871 = (mf)0.01434 + (embed)0.02436\n",
            "Epoch 87 (train)4.1s: train loss = 0.03979 = (mf)0.01540 + (embed)0.02440\n",
            "Epoch 88 (train)4.3s: train loss = 0.03901 = (mf)0.01464 + (embed)0.02437\n",
            "Epoch 89 (train)4.6s: train loss = 0.03946 = (mf)0.01513 + (embed)0.02433\n",
            "Epoch 90 (train)4.1s + (eval)0.2s: train loss = 0.03930 = (mf)0.01484 + (embed)0.02446, recall = 0.20250, ndcg = 0.28920\n",
            "Epoch 91 (train)4.8s: train loss = 0.04068 = (mf)0.01625 + (embed)0.02443\n",
            "Epoch 92 (train)4.3s: train loss = 0.03949 = (mf)0.01496 + (embed)0.02453\n",
            "Epoch 93 (train)4.2s: train loss = 0.04012 = (mf)0.01549 + (embed)0.02463\n",
            "Epoch 94 (train)4.9s: train loss = 0.04013 = (mf)0.01548 + (embed)0.02465\n",
            "Epoch 95 (train)4.0s + (eval)0.2s: train loss = 0.04044 = (mf)0.01566 + (embed)0.02478, recall = 0.19543, ndcg = 0.27295\n",
            "Epoch 96 (train)4.1s: train loss = 0.03932 = (mf)0.01448 + (embed)0.02485\n",
            "Epoch 97 (train)4.9s: train loss = 0.04018 = (mf)0.01542 + (embed)0.02476\n",
            "Epoch 98 (train)4.1s: train loss = 0.04018 = (mf)0.01545 + (embed)0.02473\n",
            "Epoch 99 (train)4.1s: train loss = 0.03993 = (mf)0.01514 + (embed)0.02479\n",
            "Epoch 100 (train)5.0s + (eval)0.6s: train loss = 0.04054 = (mf)0.01569 + (embed)0.02485, recall = 0.18836, ndcg = 0.26070\n",
            "Training finished in 452.34 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.26070, Recall@20: 0.18836\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.6s: train loss = 0.60131 = (mf)0.53822 + (embed)0.06309\n",
            "Epoch 2 (train)3.0s: train loss = 1.29255 = (mf)1.09241 + (embed)0.20014\n",
            "Epoch 3 (train)3.0s: train loss = 1.82411 = (mf)1.48617 + (embed)0.33794\n",
            "Epoch 4 (train)3.8s: train loss = 2.30937 = (mf)1.83616 + (embed)0.47321\n",
            "Epoch 5 (train)2.9s + (eval)1.0s: train loss = 2.54525 = (mf)1.95273 + (embed)0.59252, recall = 0.10062, ndcg = 0.16491\n",
            "Epoch 6 (train)3.0s: train loss = 2.67578 = (mf)1.97715 + (embed)0.69863\n",
            "Epoch 7 (train)3.0s: train loss = 2.86053 = (mf)2.07111 + (embed)0.78942\n",
            "Epoch 8 (train)3.8s: train loss = 3.06180 = (mf)2.19365 + (embed)0.86815\n",
            "Epoch 9 (train)2.9s: train loss = 3.25335 = (mf)2.30894 + (embed)0.94441\n",
            "Epoch 10 (train)2.9s + (eval)0.6s: train loss = 3.35560 = (mf)2.33899 + (embed)1.01660, recall = 0.10808, ndcg = 0.17152\n",
            "Epoch 11 (train)3.2s: train loss = 3.55736 = (mf)2.46638 + (embed)1.09098\n",
            "Epoch 12 (train)3.5s: train loss = 3.77120 = (mf)2.61306 + (embed)1.15814\n",
            "Epoch 13 (train)3.0s: train loss = 3.95329 = (mf)2.72921 + (embed)1.22408\n",
            "Epoch 14 (train)3.0s: train loss = 4.06958 = (mf)2.78914 + (embed)1.28044\n",
            "Epoch 15 (train)3.4s + (eval)0.3s: train loss = 4.22046 = (mf)2.87756 + (embed)1.34290, recall = 0.10884, ndcg = 0.17500\n",
            "Epoch 16 (train)3.4s: train loss = 4.30839 = (mf)2.90079 + (embed)1.40761\n",
            "Epoch 17 (train)3.1s: train loss = 4.24987 = (mf)2.78625 + (embed)1.46362\n",
            "Epoch 18 (train)3.0s: train loss = 4.45978 = (mf)2.94618 + (embed)1.51360\n",
            "Epoch 19 (train)3.6s: train loss = 4.23579 = (mf)2.67891 + (embed)1.55688\n",
            "Epoch 20 (train)3.3s + (eval)0.2s: train loss = 4.62871 = (mf)3.02886 + (embed)1.59985, recall = 0.10716, ndcg = 0.16953\n",
            "Epoch 21 (train)3.0s: train loss = 4.61218 = (mf)2.96212 + (embed)1.65006\n",
            "Epoch 22 (train)3.0s: train loss = 4.54977 = (mf)2.85852 + (embed)1.69125\n",
            "Epoch 23 (train)3.9s: train loss = 4.85532 = (mf)3.12093 + (embed)1.73440\n",
            "Epoch 24 (train)3.0s: train loss = 4.87202 = (mf)3.09869 + (embed)1.77333\n",
            "Epoch 25 (train)3.0s + (eval)0.2s: train loss = 4.85175 = (mf)3.04273 + (embed)1.80903, recall = 0.10930, ndcg = 0.16785\n",
            "Epoch 26 (train)3.0s: train loss = 4.71064 = (mf)2.87217 + (embed)1.83847\n",
            "Epoch 27 (train)3.9s: train loss = 4.86403 = (mf)2.98783 + (embed)1.87620\n",
            "Epoch 28 (train)3.0s: train loss = 4.86405 = (mf)2.95299 + (embed)1.91105\n",
            "Epoch 29 (train)3.0s: train loss = 4.84181 = (mf)2.90445 + (embed)1.93736\n",
            "Epoch 30 (train)2.9s + (eval)0.6s: train loss = 4.97367 = (mf)3.00150 + (embed)1.97217, recall = 0.11787, ndcg = 0.18801\n",
            "Epoch 31 (train)3.8s: train loss = 5.14071 = (mf)3.14026 + (embed)2.00045\n",
            "Epoch 32 (train)3.0s: train loss = 5.14206 = (mf)3.11797 + (embed)2.02410\n",
            "Epoch 33 (train)3.0s: train loss = 5.10995 = (mf)3.06053 + (embed)2.04942\n",
            "Epoch 34 (train)3.1s: train loss = 4.99368 = (mf)2.91233 + (embed)2.08136\n",
            "Epoch 35 (train)3.7s + (eval)0.2s: train loss = 5.02561 = (mf)2.92315 + (embed)2.10246, recall = 0.11506, ndcg = 0.17612\n",
            "Epoch 36 (train)3.0s: train loss = 5.20260 = (mf)3.07295 + (embed)2.12965\n",
            "Epoch 37 (train)3.0s: train loss = 4.82954 = (mf)2.68345 + (embed)2.14609\n",
            "Epoch 38 (train)3.3s: train loss = 5.20102 = (mf)3.02904 + (embed)2.17198\n",
            "Epoch 39 (train)3.6s: train loss = 5.10185 = (mf)2.90674 + (embed)2.19510\n",
            "Epoch 40 (train)3.0s + (eval)0.2s: train loss = 4.91609 = (mf)2.70318 + (embed)2.21291, recall = 0.11835, ndcg = 0.18056\n",
            "Epoch 41 (train)3.0s: train loss = 5.19784 = (mf)2.96351 + (embed)2.23433\n",
            "Epoch 42 (train)3.5s: train loss = 5.37116 = (mf)3.12070 + (embed)2.25046\n",
            "Epoch 43 (train)3.3s: train loss = 5.36325 = (mf)3.09098 + (embed)2.27227\n",
            "Epoch 44 (train)3.0s: train loss = 5.26328 = (mf)2.97484 + (embed)2.28844\n",
            "Epoch 45 (train)3.0s + (eval)0.2s: train loss = 4.81694 = (mf)2.50987 + (embed)2.30707, recall = 0.09832, ndcg = 0.15070\n",
            "Epoch 46 (train)3.7s: train loss = 5.31749 = (mf)2.99675 + (embed)2.32074\n",
            "Epoch 47 (train)3.2s: train loss = 5.04732 = (mf)2.71227 + (embed)2.33504\n",
            "Epoch 48 (train)2.9s: train loss = 5.26738 = (mf)2.90860 + (embed)2.35878\n",
            "Epoch 49 (train)2.9s: train loss = 5.02752 = (mf)2.65493 + (embed)2.37259\n",
            "Epoch 50 (train)3.6s + (eval)0.8s: train loss = 5.06717 = (mf)2.66887 + (embed)2.39830, recall = 0.11022, ndcg = 0.16373\n",
            "Epoch 51 (train)3.0s: train loss = 5.11702 = (mf)2.71096 + (embed)2.40606\n",
            "Epoch 52 (train)3.0s: train loss = 5.33164 = (mf)2.90553 + (embed)2.42612\n",
            "Epoch 53 (train)3.0s: train loss = 5.34534 = (mf)2.90442 + (embed)2.44092\n",
            "Epoch 54 (train)3.9s: train loss = 4.98553 = (mf)2.53907 + (embed)2.44646\n",
            "Epoch 55 (train)3.0s + (eval)0.2s: train loss = 5.12091 = (mf)2.65978 + (embed)2.46112, recall = 0.11065, ndcg = 0.16367\n",
            "Epoch 56 (train)3.0s: train loss = 5.52448 = (mf)3.05168 + (embed)2.47280\n",
            "Epoch 57 (train)2.9s: train loss = 5.36935 = (mf)2.88143 + (embed)2.48792\n",
            "Epoch 58 (train)3.8s: train loss = 5.23400 = (mf)2.74074 + (embed)2.49327\n",
            "Epoch 59 (train)2.9s: train loss = 5.19328 = (mf)2.68796 + (embed)2.50532\n",
            "Epoch 60 (train)3.0s + (eval)0.2s: train loss = 5.73449 = (mf)3.21533 + (embed)2.51916, recall = 0.11086, ndcg = 0.16548\n",
            "Epoch 61 (train)2.9s: train loss = 5.45679 = (mf)2.93025 + (embed)2.52654\n",
            "Epoch 62 (train)3.8s: train loss = 5.49508 = (mf)2.95254 + (embed)2.54254\n",
            "Epoch 63 (train)2.9s: train loss = 5.85626 = (mf)3.28976 + (embed)2.56650\n",
            "Epoch 64 (train)2.9s: train loss = 5.53939 = (mf)2.96211 + (embed)2.57729\n",
            "Epoch 65 (train)2.9s + (eval)0.8s: train loss = 5.58561 = (mf)2.98549 + (embed)2.60012, recall = 0.10235, ndcg = 0.15116\n",
            "Epoch 66 (train)3.5s: train loss = 5.45729 = (mf)2.83891 + (embed)2.61839\n",
            "Epoch 67 (train)2.9s: train loss = 5.58124 = (mf)2.94546 + (embed)2.63578\n",
            "Epoch 68 (train)3.0s: train loss = 5.68285 = (mf)3.03830 + (embed)2.64455\n",
            "Epoch 69 (train)3.3s: train loss = 5.30929 = (mf)2.65039 + (embed)2.65890\n",
            "Epoch 70 (train)3.5s + (eval)0.2s: train loss = 5.20231 = (mf)2.53059 + (embed)2.67173, recall = 0.10732, ndcg = 0.16109\n",
            "Epoch 71 (train)3.0s: train loss = 5.45259 = (mf)2.76754 + (embed)2.68506\n",
            "Epoch 72 (train)3.0s: train loss = 5.37424 = (mf)2.68654 + (embed)2.68770\n",
            "Epoch 73 (train)3.4s: train loss = 5.24937 = (mf)2.55678 + (embed)2.69259\n",
            "Epoch 74 (train)3.3s: train loss = 5.70665 = (mf)2.99441 + (embed)2.71224\n",
            "Epoch 75 (train)3.0s + (eval)0.2s: train loss = 5.73501 = (mf)3.00888 + (embed)2.72613, recall = 0.10056, ndcg = 0.15359\n",
            "Epoch 76 (train)3.0s: train loss = 5.65895 = (mf)2.92552 + (embed)2.73343\n",
            "Epoch 77 (train)3.7s: train loss = 5.65105 = (mf)2.90870 + (embed)2.74235\n",
            "Epoch 78 (train)3.1s: train loss = 5.17344 = (mf)2.41891 + (embed)2.75453\n",
            "Epoch 79 (train)3.0s: train loss = 5.70844 = (mf)2.94679 + (embed)2.76165\n",
            "Epoch 80 (train)3.0s + (eval)0.2s: train loss = 5.19865 = (mf)2.43380 + (embed)2.76484, recall = 0.12394, ndcg = 0.18656\n",
            "Epoch 81 (train)3.9s: train loss = 5.38424 = (mf)2.62354 + (embed)2.76070\n",
            "Epoch 82 (train)3.0s: train loss = 5.67591 = (mf)2.90761 + (embed)2.76830\n",
            "Epoch 83 (train)3.0s: train loss = 5.62204 = (mf)2.84886 + (embed)2.77318\n",
            "Epoch 84 (train)3.0s: train loss = 5.72961 = (mf)2.94967 + (embed)2.77994\n",
            "Epoch 85 (train)3.8s + (eval)0.6s: train loss = 5.33249 = (mf)2.55159 + (embed)2.78090, recall = 0.10846, ndcg = 0.15739\n",
            "Epoch 86 (train)3.0s: train loss = 5.49915 = (mf)2.71657 + (embed)2.78258\n",
            "Epoch 87 (train)3.0s: train loss = 5.75824 = (mf)2.96371 + (embed)2.79453\n",
            "Epoch 88 (train)3.0s: train loss = 5.55607 = (mf)2.75167 + (embed)2.80440\n",
            "Epoch 89 (train)3.8s: train loss = 5.48389 = (mf)2.67416 + (embed)2.80973\n",
            "Epoch 90 (train)2.9s + (eval)0.2s: train loss = 5.45797 = (mf)2.64403 + (embed)2.81394, recall = 0.10495, ndcg = 0.16023\n",
            "Epoch 91 (train)2.9s: train loss = 5.68955 = (mf)2.86445 + (embed)2.82510\n",
            "Epoch 92 (train)3.0s: train loss = 5.72357 = (mf)2.89636 + (embed)2.82721\n",
            "Epoch 93 (train)3.7s: train loss = 5.73244 = (mf)2.88358 + (embed)2.84886\n",
            "Epoch 94 (train)2.9s: train loss = 5.76222 = (mf)2.91297 + (embed)2.84926\n",
            "Epoch 95 (train)2.9s + (eval)0.2s: train loss = 5.49916 = (mf)2.64717 + (embed)2.85199, recall = 0.11644, ndcg = 0.17041\n",
            "Epoch 96 (train)3.1s: train loss = 5.52785 = (mf)2.66607 + (embed)2.86178\n",
            "Epoch 97 (train)3.6s: train loss = 5.49563 = (mf)2.63076 + (embed)2.86487\n",
            "Epoch 98 (train)2.9s: train loss = 5.35433 = (mf)2.48608 + (embed)2.86824\n",
            "Epoch 99 (train)2.9s: train loss = 5.92242 = (mf)3.03643 + (embed)2.88599\n",
            "Epoch 100 (train)3.2s + (eval)0.3s: train loss = 5.75628 = (mf)2.86310 + (embed)2.89318, recall = 0.10382, ndcg = 0.15253\n",
            "Training finished in 327.37 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.15253, Recall@20: 0.10382\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.0s: train loss = 0.43923 = (mf)0.38633 + (embed)0.05289\n",
            "Epoch 2 (train)3.5s: train loss = 0.79468 = (mf)0.62278 + (embed)0.17190\n",
            "Epoch 3 (train)3.6s: train loss = 1.18250 = (mf)0.88581 + (embed)0.29670\n",
            "Epoch 4 (train)4.4s: train loss = 1.42158 = (mf)1.00958 + (embed)0.41200\n",
            "Epoch 5 (train)3.5s + (eval)1.0s: train loss = 1.69298 = (mf)1.18335 + (embed)0.50964, recall = 0.10749, ndcg = 0.16617\n",
            "Epoch 6 (train)3.5s: train loss = 1.86608 = (mf)1.26772 + (embed)0.59836\n",
            "Epoch 7 (train)4.4s: train loss = 1.96491 = (mf)1.28117 + (embed)0.68374\n",
            "Epoch 8 (train)3.5s: train loss = 2.14954 = (mf)1.39332 + (embed)0.75622\n",
            "Epoch 9 (train)3.6s: train loss = 2.17340 = (mf)1.35396 + (embed)0.81944\n",
            "Epoch 10 (train)3.9s + (eval)0.3s: train loss = 2.28830 = (mf)1.41104 + (embed)0.87726, recall = 0.09556, ndcg = 0.15304\n",
            "Epoch 11 (train)3.9s: train loss = 2.46990 = (mf)1.53498 + (embed)0.93493\n",
            "Epoch 12 (train)3.6s: train loss = 2.50174 = (mf)1.51248 + (embed)0.98926\n",
            "Epoch 13 (train)3.5s: train loss = 2.49151 = (mf)1.45349 + (embed)1.03803\n",
            "Epoch 14 (train)4.4s: train loss = 2.66805 = (mf)1.59369 + (embed)1.07435\n",
            "Epoch 15 (train)3.5s + (eval)0.6s: train loss = 2.71784 = (mf)1.59560 + (embed)1.12224, recall = 0.10337, ndcg = 0.16456\n",
            "Epoch 16 (train)3.5s: train loss = 2.64288 = (mf)1.48163 + (embed)1.16124\n",
            "Epoch 17 (train)4.4s: train loss = 2.77461 = (mf)1.57838 + (embed)1.19623\n",
            "Epoch 18 (train)3.6s: train loss = 2.85395 = (mf)1.62780 + (embed)1.22614\n",
            "Epoch 19 (train)3.6s: train loss = 2.89161 = (mf)1.62797 + (embed)1.26365\n",
            "Epoch 20 (train)4.4s + (eval)0.2s: train loss = 2.98071 = (mf)1.67505 + (embed)1.30565, recall = 0.10260, ndcg = 0.16042\n",
            "Epoch 21 (train)3.6s: train loss = 2.97299 = (mf)1.63495 + (embed)1.33804\n",
            "Epoch 22 (train)3.6s: train loss = 3.02199 = (mf)1.66229 + (embed)1.35970\n",
            "Epoch 23 (train)4.1s: train loss = 3.09554 = (mf)1.71239 + (embed)1.38315\n",
            "Epoch 24 (train)3.9s: train loss = 2.90571 = (mf)1.50034 + (embed)1.40538\n",
            "Epoch 25 (train)3.6s + (eval)0.2s: train loss = 2.91745 = (mf)1.48605 + (embed)1.43139, recall = 0.11539, ndcg = 0.17757\n",
            "Epoch 26 (train)3.9s: train loss = 3.06782 = (mf)1.61659 + (embed)1.45123\n",
            "Epoch 27 (train)4.1s: train loss = 3.13921 = (mf)1.65980 + (embed)1.47942\n",
            "Epoch 28 (train)3.6s: train loss = 3.18000 = (mf)1.68584 + (embed)1.49417\n",
            "Epoch 29 (train)3.6s: train loss = 3.12663 = (mf)1.61255 + (embed)1.51408\n",
            "Epoch 30 (train)4.4s + (eval)0.2s: train loss = 3.23565 = (mf)1.69810 + (embed)1.53756, recall = 0.10505, ndcg = 0.16491\n",
            "Epoch 31 (train)3.5s: train loss = 3.28121 = (mf)1.71779 + (embed)1.56343\n",
            "Epoch 32 (train)3.5s: train loss = 3.19471 = (mf)1.61327 + (embed)1.58144\n",
            "Epoch 33 (train)4.3s: train loss = 3.25130 = (mf)1.66122 + (embed)1.59008\n",
            "Epoch 34 (train)3.6s: train loss = 3.21034 = (mf)1.60608 + (embed)1.60425\n",
            "Epoch 35 (train)3.5s + (eval)0.6s: train loss = 3.33292 = (mf)1.70542 + (embed)1.62750, recall = 0.09973, ndcg = 0.15303\n",
            "Epoch 36 (train)4.2s: train loss = 3.24357 = (mf)1.60198 + (embed)1.64159\n",
            "Epoch 37 (train)3.6s: train loss = 3.31776 = (mf)1.66299 + (embed)1.65477\n",
            "Epoch 38 (train)3.5s: train loss = 3.31545 = (mf)1.64781 + (embed)1.66764\n",
            "Epoch 39 (train)3.7s: train loss = 3.48269 = (mf)1.80704 + (embed)1.67564\n",
            "Epoch 40 (train)4.3s + (eval)0.2s: train loss = 3.43661 = (mf)1.73991 + (embed)1.69671, recall = 0.11516, ndcg = 0.17484\n",
            "Epoch 41 (train)3.6s: train loss = 3.24003 = (mf)1.53373 + (embed)1.70630\n",
            "Epoch 42 (train)3.6s: train loss = 3.12294 = (mf)1.41000 + (embed)1.71294\n",
            "Epoch 43 (train)4.3s: train loss = 3.31499 = (mf)1.59406 + (embed)1.72092\n",
            "Epoch 44 (train)3.5s: train loss = 3.27350 = (mf)1.53762 + (embed)1.73588\n",
            "Epoch 45 (train)3.4s + (eval)0.2s: train loss = 3.43031 = (mf)1.68280 + (embed)1.74752, recall = 0.09670, ndcg = 0.14654\n",
            "Epoch 46 (train)4.3s: train loss = 3.35801 = (mf)1.59753 + (embed)1.76048\n",
            "Epoch 47 (train)3.4s: train loss = 3.39116 = (mf)1.62648 + (embed)1.76469\n",
            "Epoch 48 (train)3.4s: train loss = 3.44401 = (mf)1.66602 + (embed)1.77799\n",
            "Epoch 49 (train)3.7s: train loss = 3.11579 = (mf)1.34078 + (embed)1.77502\n",
            "Epoch 50 (train)4.1s + (eval)0.6s: train loss = 3.26988 = (mf)1.49469 + (embed)1.77519, recall = 0.09716, ndcg = 0.14812\n",
            "Epoch 51 (train)3.5s: train loss = 3.26946 = (mf)1.48650 + (embed)1.78296\n",
            "Epoch 52 (train)3.5s: train loss = 3.41571 = (mf)1.62058 + (embed)1.79512\n",
            "Epoch 53 (train)4.3s: train loss = 3.37526 = (mf)1.56759 + (embed)1.80766\n",
            "Epoch 54 (train)3.5s: train loss = 3.19396 = (mf)1.38356 + (embed)1.81041\n",
            "Epoch 55 (train)3.5s + (eval)0.2s: train loss = 3.36630 = (mf)1.54924 + (embed)1.81705, recall = 0.09818, ndcg = 0.15544\n",
            "Epoch 56 (train)4.3s: train loss = 3.35029 = (mf)1.52687 + (embed)1.82342\n",
            "Epoch 57 (train)3.6s: train loss = 3.30071 = (mf)1.46779 + (embed)1.83293\n",
            "Epoch 58 (train)3.5s: train loss = 3.37730 = (mf)1.53883 + (embed)1.83848\n",
            "Epoch 59 (train)4.0s: train loss = 3.34614 = (mf)1.50387 + (embed)1.84227\n",
            "Epoch 60 (train)3.7s + (eval)0.2s: train loss = 3.42543 = (mf)1.58603 + (embed)1.83940, recall = 0.11265, ndcg = 0.16979\n",
            "Epoch 61 (train)3.5s: train loss = 3.50882 = (mf)1.66316 + (embed)1.84566\n",
            "Epoch 62 (train)3.5s: train loss = 3.35549 = (mf)1.51064 + (embed)1.84484\n",
            "Epoch 63 (train)4.3s: train loss = 3.19683 = (mf)1.35176 + (embed)1.84507\n",
            "Epoch 64 (train)3.5s: train loss = 3.41380 = (mf)1.56151 + (embed)1.85229\n",
            "Epoch 65 (train)3.5s + (eval)0.2s: train loss = 3.39468 = (mf)1.54262 + (embed)1.85206, recall = 0.11124, ndcg = 0.15639\n",
            "Epoch 66 (train)4.4s: train loss = 3.23190 = (mf)1.37069 + (embed)1.86121\n",
            "Epoch 67 (train)3.5s: train loss = 3.35534 = (mf)1.48984 + (embed)1.86550\n",
            "Epoch 68 (train)3.5s: train loss = 3.28980 = (mf)1.42749 + (embed)1.86231\n",
            "Epoch 69 (train)4.2s: train loss = 3.27273 = (mf)1.39292 + (embed)1.87982\n",
            "Epoch 70 (train)3.7s + (eval)0.6s: train loss = 3.30436 = (mf)1.42837 + (embed)1.87599, recall = 0.10335, ndcg = 0.15609\n",
            "Epoch 71 (train)3.6s: train loss = 3.25928 = (mf)1.36904 + (embed)1.89024\n",
            "Epoch 72 (train)3.9s: train loss = 3.28201 = (mf)1.38962 + (embed)1.89240\n",
            "Epoch 73 (train)4.0s: train loss = 3.33111 = (mf)1.43605 + (embed)1.89505\n",
            "Epoch 74 (train)3.5s: train loss = 3.25795 = (mf)1.36055 + (embed)1.89740\n",
            "Epoch 75 (train)3.5s + (eval)0.2s: train loss = 3.24510 = (mf)1.34628 + (embed)1.89881, recall = 0.10514, ndcg = 0.16604\n",
            "Epoch 76 (train)4.3s: train loss = 3.21275 = (mf)1.32099 + (embed)1.89176\n",
            "Epoch 77 (train)3.6s: train loss = 3.17767 = (mf)1.27723 + (embed)1.90044\n",
            "Epoch 78 (train)3.5s: train loss = 3.29589 = (mf)1.38866 + (embed)1.90723\n",
            "Epoch 79 (train)4.4s: train loss = 3.22500 = (mf)1.30852 + (embed)1.91649\n",
            "Epoch 80 (train)3.6s + (eval)0.2s: train loss = 3.38910 = (mf)1.47330 + (embed)1.91580, recall = 0.11030, ndcg = 0.16853\n",
            "Epoch 81 (train)3.6s: train loss = 3.39140 = (mf)1.47497 + (embed)1.91642\n",
            "Epoch 82 (train)4.4s: train loss = 3.46976 = (mf)1.54408 + (embed)1.92568\n",
            "Epoch 83 (train)3.6s: train loss = 3.51000 = (mf)1.57824 + (embed)1.93176\n",
            "Epoch 84 (train)3.6s: train loss = 3.48635 = (mf)1.54582 + (embed)1.94054\n",
            "Epoch 85 (train)4.0s + (eval)0.3s: train loss = 3.33554 = (mf)1.39411 + (embed)1.94143, recall = 0.10989, ndcg = 0.16607\n",
            "Epoch 86 (train)3.9s: train loss = 3.35021 = (mf)1.40812 + (embed)1.94209\n",
            "Epoch 87 (train)3.5s: train loss = 3.43063 = (mf)1.48520 + (embed)1.94543\n",
            "Epoch 88 (train)3.5s: train loss = 3.27967 = (mf)1.33446 + (embed)1.94521\n",
            "Epoch 89 (train)4.4s: train loss = 3.49465 = (mf)1.55018 + (embed)1.94448\n",
            "Epoch 90 (train)3.5s + (eval)0.6s: train loss = 3.39493 = (mf)1.44601 + (embed)1.94892, recall = 0.11434, ndcg = 0.16785\n",
            "Epoch 91 (train)3.5s: train loss = 3.28417 = (mf)1.33596 + (embed)1.94821\n",
            "Epoch 92 (train)4.3s: train loss = 3.41104 = (mf)1.46486 + (embed)1.94617\n",
            "Epoch 93 (train)3.5s: train loss = 3.38393 = (mf)1.43477 + (embed)1.94916\n",
            "Epoch 94 (train)3.6s: train loss = 3.54150 = (mf)1.58984 + (embed)1.95165\n",
            "Epoch 95 (train)4.4s + (eval)0.2s: train loss = 3.45958 = (mf)1.49642 + (embed)1.96316, recall = 0.11331, ndcg = 0.16592\n",
            "Epoch 96 (train)3.6s: train loss = 3.34225 = (mf)1.37593 + (embed)1.96632\n",
            "Epoch 97 (train)3.5s: train loss = 3.47755 = (mf)1.51622 + (embed)1.96133\n",
            "Epoch 98 (train)4.0s: train loss = 3.61406 = (mf)1.64259 + (embed)1.97147\n",
            "Epoch 99 (train)3.9s: train loss = 3.47368 = (mf)1.49893 + (embed)1.97475\n",
            "Epoch 100 (train)3.5s + (eval)0.2s: train loss = 3.41926 = (mf)1.43439 + (embed)1.98487, recall = 0.11254, ndcg = 0.16058\n",
            "Training finished in 387.97 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.16058, Recall@20: 0.11254\n",
            "--- Trying combination: Epochs=100, TopK=20, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.34179 = (mf)0.29376 + (embed)0.04803\n",
            "Epoch 2 (train)4.9s: train loss = 0.53844 = (mf)0.38747 + (embed)0.15097\n",
            "Epoch 3 (train)4.0s: train loss = 0.75517 = (mf)0.50180 + (embed)0.25338\n",
            "Epoch 4 (train)4.1s: train loss = 1.01905 = (mf)0.66165 + (embed)0.35740\n",
            "Epoch 5 (train)5.0s + (eval)1.1s: train loss = 1.18916 = (mf)0.74033 + (embed)0.44883, recall = 0.09952, ndcg = 0.15940\n",
            "Epoch 6 (train)4.2s: train loss = 1.30407 = (mf)0.77823 + (embed)0.52584\n",
            "Epoch 7 (train)4.9s: train loss = 1.44647 = (mf)0.84983 + (embed)0.59664\n",
            "Epoch 8 (train)4.2s: train loss = 1.51384 = (mf)0.85229 + (embed)0.66155\n",
            "Epoch 9 (train)4.1s: train loss = 1.60923 = (mf)0.89610 + (embed)0.71313\n",
            "Epoch 10 (train)4.9s + (eval)0.2s: train loss = 1.67092 = (mf)0.90334 + (embed)0.76758, recall = 0.11984, ndcg = 0.17806\n",
            "Epoch 11 (train)4.1s: train loss = 1.80121 = (mf)0.98517 + (embed)0.81604\n",
            "Epoch 12 (train)4.1s: train loss = 1.91332 = (mf)1.04983 + (embed)0.86350\n",
            "Epoch 13 (train)4.9s: train loss = 2.02356 = (mf)1.11657 + (embed)0.90698\n",
            "Epoch 14 (train)4.1s: train loss = 2.05845 = (mf)1.11869 + (embed)0.93976\n",
            "Epoch 15 (train)4.1s + (eval)0.2s: train loss = 2.03905 = (mf)1.06536 + (embed)0.97369, recall = 0.09932, ndcg = 0.15523\n",
            "Epoch 16 (train)4.9s: train loss = 2.08708 = (mf)1.08569 + (embed)1.00139\n",
            "Epoch 17 (train)4.1s: train loss = 2.11146 = (mf)1.08739 + (embed)1.02407\n",
            "Epoch 18 (train)4.3s: train loss = 2.19449 = (mf)1.14724 + (embed)1.04725\n",
            "Epoch 19 (train)4.7s: train loss = 2.07055 = (mf)0.99912 + (embed)1.07143\n",
            "Epoch 20 (train)4.1s + (eval)0.6s: train loss = 2.22407 = (mf)1.12929 + (embed)1.09478, recall = 0.09486, ndcg = 0.14627\n",
            "Epoch 21 (train)4.8s: train loss = 2.10495 = (mf)0.98987 + (embed)1.11509\n",
            "Epoch 22 (train)4.2s: train loss = 2.23391 = (mf)1.09629 + (embed)1.13762\n",
            "Epoch 23 (train)4.1s: train loss = 2.25914 = (mf)1.10555 + (embed)1.15358\n",
            "Epoch 24 (train)4.9s: train loss = 2.29657 = (mf)1.11843 + (embed)1.17814\n",
            "Epoch 25 (train)4.1s + (eval)0.2s: train loss = 2.19215 = (mf)0.99432 + (embed)1.19783, recall = 0.10266, ndcg = 0.15743\n",
            "Epoch 26 (train)4.1s: train loss = 2.33803 = (mf)1.12155 + (embed)1.21648\n",
            "Epoch 27 (train)5.0s: train loss = 2.31337 = (mf)1.08071 + (embed)1.23265\n",
            "Epoch 28 (train)4.2s: train loss = 2.27016 = (mf)1.03098 + (embed)1.23918\n",
            "Epoch 29 (train)4.1s: train loss = 2.36973 = (mf)1.11630 + (embed)1.25343\n",
            "Epoch 30 (train)5.0s + (eval)0.2s: train loss = 2.21460 = (mf)0.96043 + (embed)1.25417, recall = 0.10141, ndcg = 0.15242\n",
            "Epoch 31 (train)4.1s: train loss = 2.31610 = (mf)1.04838 + (embed)1.26772\n",
            "Epoch 32 (train)4.2s: train loss = 2.28800 = (mf)1.00967 + (embed)1.27833\n",
            "Epoch 33 (train)4.8s: train loss = 2.38529 = (mf)1.09535 + (embed)1.28994\n",
            "Epoch 34 (train)4.1s: train loss = 2.38602 = (mf)1.08301 + (embed)1.30302\n",
            "Epoch 35 (train)4.5s + (eval)0.3s: train loss = 2.34462 = (mf)1.03233 + (embed)1.31229, recall = 0.11140, ndcg = 0.16193\n",
            "Epoch 36 (train)4.4s: train loss = 2.29522 = (mf)0.98017 + (embed)1.31505\n",
            "Epoch 37 (train)4.1s: train loss = 2.37261 = (mf)1.05262 + (embed)1.31999\n",
            "Epoch 38 (train)4.8s: train loss = 2.38097 = (mf)1.04497 + (embed)1.33600\n",
            "Epoch 39 (train)4.2s: train loss = 2.40544 = (mf)1.05389 + (embed)1.35155\n",
            "Epoch 40 (train)4.1s + (eval)0.6s: train loss = 2.38802 = (mf)1.03004 + (embed)1.35798, recall = 0.09739, ndcg = 0.14199\n",
            "Epoch 41 (train)5.0s: train loss = 2.33825 = (mf)0.97799 + (embed)1.36026\n",
            "Epoch 42 (train)4.1s: train loss = 2.42796 = (mf)1.05322 + (embed)1.37474\n",
            "Epoch 43 (train)4.1s: train loss = 2.37673 = (mf)0.99760 + (embed)1.37913\n",
            "Epoch 44 (train)4.9s: train loss = 2.39375 = (mf)1.00012 + (embed)1.39363\n",
            "Epoch 45 (train)4.0s + (eval)0.2s: train loss = 2.44601 = (mf)1.05259 + (embed)1.39342, recall = 0.09810, ndcg = 0.14222\n",
            "Epoch 46 (train)4.1s: train loss = 2.39640 = (mf)0.99737 + (embed)1.39902\n",
            "Epoch 47 (train)4.8s: train loss = 2.38997 = (mf)0.98985 + (embed)1.40013\n",
            "Epoch 48 (train)4.1s: train loss = 2.51406 = (mf)1.11565 + (embed)1.39842\n",
            "Epoch 49 (train)4.3s: train loss = 2.50293 = (mf)1.09775 + (embed)1.40518\n",
            "Epoch 50 (train)4.7s + (eval)0.2s: train loss = 2.43050 = (mf)1.02199 + (embed)1.40851, recall = 0.11003, ndcg = 0.15581\n",
            "Epoch 51 (train)4.1s: train loss = 2.42562 = (mf)1.01219 + (embed)1.41343\n",
            "Epoch 52 (train)4.6s: train loss = 2.34371 = (mf)0.92821 + (embed)1.41550\n",
            "Epoch 53 (train)4.5s: train loss = 2.37031 = (mf)0.96364 + (embed)1.40666\n",
            "Epoch 54 (train)4.1s: train loss = 2.41451 = (mf)1.00644 + (embed)1.40807\n",
            "Epoch 55 (train)4.8s + (eval)0.7s: train loss = 2.34506 = (mf)0.92706 + (embed)1.41800, recall = 0.11631, ndcg = 0.16966\n",
            "Epoch 56 (train)4.1s: train loss = 2.42524 = (mf)1.00684 + (embed)1.41840\n",
            "Epoch 57 (train)4.1s: train loss = 2.38870 = (mf)0.96675 + (embed)1.42195\n",
            "Epoch 58 (train)4.9s: train loss = 2.39224 = (mf)0.96830 + (embed)1.42394\n",
            "Epoch 59 (train)4.1s: train loss = 2.34104 = (mf)0.91765 + (embed)1.42340\n",
            "Epoch 60 (train)4.1s + (eval)0.2s: train loss = 2.42523 = (mf)0.99234 + (embed)1.43289, recall = 0.09396, ndcg = 0.14081\n",
            "Epoch 61 (train)4.9s: train loss = 2.34138 = (mf)0.90685 + (embed)1.43453\n",
            "Epoch 62 (train)4.1s: train loss = 2.28674 = (mf)0.85366 + (embed)1.43308\n",
            "Epoch 63 (train)4.1s: train loss = 2.40053 = (mf)0.96860 + (embed)1.43193\n",
            "Epoch 64 (train)5.0s: train loss = 2.32908 = (mf)0.89053 + (embed)1.43855\n",
            "Epoch 65 (train)4.2s + (eval)0.2s: train loss = 2.42235 = (mf)0.97613 + (embed)1.44623, recall = 0.10501, ndcg = 0.14802\n",
            "Epoch 66 (train)4.6s: train loss = 2.41360 = (mf)0.96551 + (embed)1.44809\n",
            "Epoch 67 (train)4.5s: train loss = 2.35453 = (mf)0.90932 + (embed)1.44521\n",
            "Epoch 68 (train)4.2s: train loss = 2.36929 = (mf)0.92208 + (embed)1.44722\n",
            "Epoch 69 (train)4.9s: train loss = 2.29730 = (mf)0.85480 + (embed)1.44250\n",
            "Epoch 70 (train)4.1s + (eval)0.2s: train loss = 2.50399 = (mf)1.05555 + (embed)1.44844, recall = 0.10264, ndcg = 0.15327\n",
            "Epoch 71 (train)4.1s: train loss = 2.40548 = (mf)0.95899 + (embed)1.44649\n",
            "Epoch 72 (train)4.9s: train loss = 2.33382 = (mf)0.88868 + (embed)1.44514\n",
            "Epoch 73 (train)4.1s: train loss = 2.34376 = (mf)0.90676 + (embed)1.43700\n",
            "Epoch 74 (train)4.1s: train loss = 2.29456 = (mf)0.86244 + (embed)1.43212\n",
            "Epoch 75 (train)5.0s + (eval)0.6s: train loss = 2.39344 = (mf)0.95874 + (embed)1.43470, recall = 0.10045, ndcg = 0.14185\n",
            "Epoch 76 (train)4.2s: train loss = 2.41200 = (mf)0.97783 + (embed)1.43417\n",
            "Epoch 77 (train)4.4s: train loss = 2.36343 = (mf)0.92011 + (embed)1.44333\n",
            "Epoch 78 (train)4.8s: train loss = 2.33799 = (mf)0.89084 + (embed)1.44715\n",
            "Epoch 79 (train)4.1s: train loss = 2.34388 = (mf)0.89859 + (embed)1.44529\n",
            "Epoch 80 (train)4.7s + (eval)0.3s: train loss = 2.43561 = (mf)0.98488 + (embed)1.45073, recall = 0.10443, ndcg = 0.15128\n",
            "Epoch 81 (train)4.3s: train loss = 2.25743 = (mf)0.80585 + (embed)1.45158\n",
            "Epoch 82 (train)4.1s: train loss = 2.41270 = (mf)0.96513 + (embed)1.44757\n",
            "Epoch 83 (train)5.0s: train loss = 2.33160 = (mf)0.88256 + (embed)1.44905\n",
            "Epoch 84 (train)4.2s: train loss = 2.32113 = (mf)0.87508 + (embed)1.44605\n",
            "Epoch 85 (train)4.2s + (eval)0.2s: train loss = 2.32671 = (mf)0.87214 + (embed)1.45457, recall = 0.12522, ndcg = 0.17525\n",
            "Epoch 86 (train)5.0s: train loss = 2.42108 = (mf)0.96481 + (embed)1.45627\n",
            "Epoch 87 (train)4.1s: train loss = 2.44935 = (mf)0.98791 + (embed)1.46144\n",
            "Epoch 88 (train)4.1s: train loss = 2.41758 = (mf)0.94907 + (embed)1.46851\n",
            "Epoch 89 (train)5.1s: train loss = 2.35946 = (mf)0.88706 + (embed)1.47240\n",
            "Epoch 90 (train)4.3s + (eval)0.2s: train loss = 2.45515 = (mf)0.97268 + (embed)1.48247, recall = 0.11828, ndcg = 0.17374\n",
            "Epoch 91 (train)4.5s: train loss = 2.31110 = (mf)0.83245 + (embed)1.47865\n",
            "Epoch 92 (train)4.7s: train loss = 2.45896 = (mf)0.97219 + (embed)1.48677\n",
            "Epoch 93 (train)4.2s: train loss = 2.33634 = (mf)0.85119 + (embed)1.48515\n",
            "Epoch 94 (train)4.7s: train loss = 2.47746 = (mf)0.99527 + (embed)1.48219\n",
            "Epoch 95 (train)4.3s + (eval)0.6s: train loss = 2.43967 = (mf)0.95073 + (embed)1.48894, recall = 0.10562, ndcg = 0.16074\n",
            "Epoch 96 (train)4.1s: train loss = 2.44459 = (mf)0.95533 + (embed)1.48926\n",
            "Epoch 97 (train)4.9s: train loss = 2.46612 = (mf)0.97330 + (embed)1.49282\n",
            "Epoch 98 (train)4.1s: train loss = 2.45950 = (mf)0.96159 + (embed)1.49790\n",
            "Epoch 99 (train)4.2s: train loss = 2.39485 = (mf)0.90001 + (embed)1.49484\n",
            "Epoch 100 (train)4.9s + (eval)0.2s: train loss = 2.53155 = (mf)1.04102 + (embed)1.49052, recall = 0.09561, ndcg = 0.13977\n",
            "Training finished in 449.80 seconds.\n",
            "Evaluation results for this combination: NDCG@20: 0.13977, Recall@20: 0.09561\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.005, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.9s: train loss = 0.42296 = (mf)0.42272 + (embed)0.00024\n",
            "Epoch 2 (train)3.0s: train loss = 0.22621 = (mf)0.22557 + (embed)0.00064\n",
            "Epoch 3 (train)3.0s: train loss = 0.19941 = (mf)0.19866 + (embed)0.00075\n",
            "Epoch 4 (train)3.8s: train loss = 0.18514 = (mf)0.18426 + (embed)0.00087\n",
            "Epoch 5 (train)3.0s + (eval)1.1s: train loss = 0.16529 = (mf)0.16427 + (embed)0.00102, recall = 0.23609, ndcg = 0.23645\n",
            "Epoch 6 (train)3.0s: train loss = 0.15541 = (mf)0.15426 + (embed)0.00115\n",
            "Epoch 7 (train)3.2s: train loss = 0.14449 = (mf)0.14318 + (embed)0.00131\n",
            "Epoch 8 (train)3.7s: train loss = 0.13715 = (mf)0.13570 + (embed)0.00145\n",
            "Epoch 9 (train)3.0s: train loss = 0.13014 = (mf)0.12855 + (embed)0.00160\n",
            "Epoch 10 (train)3.0s + (eval)0.7s: train loss = 0.12261 = (mf)0.12087 + (embed)0.00174, recall = 0.27313, ndcg = 0.26922\n",
            "Epoch 11 (train)3.6s: train loss = 0.11655 = (mf)0.11465 + (embed)0.00190\n",
            "Epoch 12 (train)3.3s: train loss = 0.11151 = (mf)0.10946 + (embed)0.00205\n",
            "Epoch 13 (train)3.0s: train loss = 0.10608 = (mf)0.10388 + (embed)0.00220\n",
            "Epoch 14 (train)3.1s: train loss = 0.09853 = (mf)0.09616 + (embed)0.00236\n",
            "Epoch 15 (train)3.9s + (eval)0.3s: train loss = 0.09852 = (mf)0.09599 + (embed)0.00252, recall = 0.29086, ndcg = 0.28610\n",
            "Epoch 16 (train)3.0s: train loss = 0.09129 = (mf)0.08861 + (embed)0.00268\n",
            "Epoch 17 (train)3.1s: train loss = 0.08684 = (mf)0.08400 + (embed)0.00284\n",
            "Epoch 18 (train)3.1s: train loss = 0.08287 = (mf)0.07986 + (embed)0.00302\n",
            "Epoch 19 (train)3.8s: train loss = 0.07885 = (mf)0.07569 + (embed)0.00316\n",
            "Epoch 20 (train)2.9s + (eval)0.2s: train loss = 0.07461 = (mf)0.07127 + (embed)0.00334, recall = 0.30830, ndcg = 0.30478\n",
            "Epoch 21 (train)2.9s: train loss = 0.07440 = (mf)0.07092 + (embed)0.00348\n",
            "Epoch 22 (train)3.0s: train loss = 0.07051 = (mf)0.06689 + (embed)0.00362\n",
            "Epoch 23 (train)3.8s: train loss = 0.06742 = (mf)0.06364 + (embed)0.00378\n",
            "Epoch 24 (train)3.0s: train loss = 0.06335 = (mf)0.05943 + (embed)0.00392\n",
            "Epoch 25 (train)3.0s + (eval)0.2s: train loss = 0.06335 = (mf)0.05928 + (embed)0.00407, recall = 0.32756, ndcg = 0.32611\n",
            "Epoch 26 (train)3.1s: train loss = 0.06128 = (mf)0.05707 + (embed)0.00421\n",
            "Epoch 27 (train)3.7s: train loss = 0.05705 = (mf)0.05270 + (embed)0.00435\n",
            "Epoch 28 (train)3.0s: train loss = 0.05846 = (mf)0.05397 + (embed)0.00449\n",
            "Epoch 29 (train)3.0s: train loss = 0.05453 = (mf)0.04991 + (embed)0.00463\n",
            "Epoch 30 (train)3.1s + (eval)0.8s: train loss = 0.05413 = (mf)0.04939 + (embed)0.00475, recall = 0.33769, ndcg = 0.33241\n",
            "Epoch 31 (train)3.4s: train loss = 0.05490 = (mf)0.05005 + (embed)0.00485\n",
            "Epoch 32 (train)3.0s: train loss = 0.05168 = (mf)0.04670 + (embed)0.00498\n",
            "Epoch 33 (train)3.0s: train loss = 0.05145 = (mf)0.04636 + (embed)0.00510\n",
            "Epoch 34 (train)3.6s: train loss = 0.04835 = (mf)0.04313 + (embed)0.00522\n",
            "Epoch 35 (train)3.4s + (eval)0.2s: train loss = 0.04982 = (mf)0.04449 + (embed)0.00533, recall = 0.33942, ndcg = 0.33760\n",
            "Epoch 36 (train)3.0s: train loss = 0.04806 = (mf)0.04265 + (embed)0.00542\n",
            "Epoch 37 (train)3.0s: train loss = 0.04573 = (mf)0.04020 + (embed)0.00553\n",
            "Epoch 38 (train)3.7s: train loss = 0.04528 = (mf)0.03964 + (embed)0.00564\n",
            "Epoch 39 (train)3.1s: train loss = 0.04520 = (mf)0.03947 + (embed)0.00574\n",
            "Epoch 40 (train)3.0s + (eval)0.2s: train loss = 0.04335 = (mf)0.03752 + (embed)0.00583, recall = 0.34520, ndcg = 0.34169\n",
            "Epoch 41 (train)3.0s: train loss = 0.04318 = (mf)0.03726 + (embed)0.00592\n",
            "Epoch 42 (train)3.8s: train loss = 0.04366 = (mf)0.03766 + (embed)0.00600\n",
            "Epoch 43 (train)3.0s: train loss = 0.04227 = (mf)0.03617 + (embed)0.00610\n",
            "Epoch 44 (train)3.0s: train loss = 0.04044 = (mf)0.03424 + (embed)0.00620\n",
            "Epoch 45 (train)3.0s + (eval)0.2s: train loss = 0.03976 = (mf)0.03347 + (embed)0.00630, recall = 0.34920, ndcg = 0.34816\n",
            "Epoch 46 (train)3.8s: train loss = 0.03984 = (mf)0.03345 + (embed)0.00639\n",
            "Epoch 47 (train)3.0s: train loss = 0.03868 = (mf)0.03221 + (embed)0.00647\n",
            "Epoch 48 (train)3.0s: train loss = 0.03790 = (mf)0.03137 + (embed)0.00653\n",
            "Epoch 49 (train)3.0s: train loss = 0.03745 = (mf)0.03084 + (embed)0.00662\n",
            "Epoch 50 (train)3.8s + (eval)0.6s: train loss = 0.03691 = (mf)0.03023 + (embed)0.00668, recall = 0.35196, ndcg = 0.34970\n",
            "Epoch 51 (train)3.0s: train loss = 0.03531 = (mf)0.02855 + (embed)0.00676\n",
            "Epoch 52 (train)2.9s: train loss = 0.03540 = (mf)0.02859 + (embed)0.00681\n",
            "Epoch 53 (train)3.2s: train loss = 0.03447 = (mf)0.02757 + (embed)0.00690\n",
            "Epoch 54 (train)3.6s: train loss = 0.03470 = (mf)0.02775 + (embed)0.00695\n",
            "Epoch 55 (train)3.0s + (eval)0.2s: train loss = 0.03458 = (mf)0.02757 + (embed)0.00700, recall = 0.35689, ndcg = 0.35072\n",
            "Epoch 56 (train)3.0s: train loss = 0.03469 = (mf)0.02763 + (embed)0.00706\n",
            "Epoch 57 (train)3.3s: train loss = 0.03426 = (mf)0.02714 + (embed)0.00711\n",
            "Epoch 58 (train)3.5s: train loss = 0.03194 = (mf)0.02474 + (embed)0.00719\n",
            "Epoch 59 (train)3.1s: train loss = 0.03224 = (mf)0.02498 + (embed)0.00726\n",
            "Epoch 60 (train)3.0s + (eval)0.2s: train loss = 0.03405 = (mf)0.02675 + (embed)0.00730, recall = 0.35376, ndcg = 0.35030\n",
            "Epoch 61 (train)3.7s: train loss = 0.03262 = (mf)0.02527 + (embed)0.00734\n",
            "Epoch 62 (train)3.2s: train loss = 0.03252 = (mf)0.02512 + (embed)0.00740\n",
            "Epoch 63 (train)3.0s: train loss = 0.03076 = (mf)0.02330 + (embed)0.00746\n",
            "Epoch 64 (train)3.0s: train loss = 0.03214 = (mf)0.02462 + (embed)0.00751\n",
            "Epoch 65 (train)3.9s + (eval)0.6s: train loss = 0.02990 = (mf)0.02236 + (embed)0.00755, recall = 0.35340, ndcg = 0.34906\n",
            "Epoch 66 (train)3.0s: train loss = 0.03120 = (mf)0.02361 + (embed)0.00759\n",
            "Epoch 67 (train)2.9s: train loss = 0.03029 = (mf)0.02264 + (embed)0.00764\n",
            "Epoch 68 (train)3.0s: train loss = 0.02959 = (mf)0.02189 + (embed)0.00769\n",
            "Epoch 69 (train)3.8s: train loss = 0.02932 = (mf)0.02162 + (embed)0.00770\n",
            "Epoch 70 (train)3.0s + (eval)0.2s: train loss = 0.02932 = (mf)0.02153 + (embed)0.00779, recall = 0.35513, ndcg = 0.35244\n",
            "Epoch 71 (train)3.0s: train loss = 0.02941 = (mf)0.02160 + (embed)0.00781\n",
            "Epoch 72 (train)3.1s: train loss = 0.02872 = (mf)0.02088 + (embed)0.00784\n",
            "Epoch 73 (train)3.9s: train loss = 0.02811 = (mf)0.02023 + (embed)0.00788\n",
            "Epoch 74 (train)3.0s: train loss = 0.02920 = (mf)0.02127 + (embed)0.00793\n",
            "Epoch 75 (train)3.2s + (eval)0.2s: train loss = 0.02752 = (mf)0.01957 + (embed)0.00795, recall = 0.35053, ndcg = 0.34883\n",
            "Epoch 76 (train)3.5s: train loss = 0.02739 = (mf)0.01939 + (embed)0.00801\n",
            "Epoch 77 (train)3.5s: train loss = 0.02707 = (mf)0.01903 + (embed)0.00804\n",
            "Epoch 78 (train)3.2s: train loss = 0.02611 = (mf)0.01803 + (embed)0.00808\n",
            "Epoch 79 (train)3.1s: train loss = 0.02690 = (mf)0.01879 + (embed)0.00811\n",
            "Epoch 80 (train)3.9s + (eval)0.3s: train loss = 0.02627 = (mf)0.01814 + (embed)0.00812, recall = 0.35439, ndcg = 0.34907\n",
            "Epoch 81 (train)3.1s: train loss = 0.02725 = (mf)0.01907 + (embed)0.00818\n",
            "Epoch 82 (train)3.1s: train loss = 0.02564 = (mf)0.01744 + (embed)0.00820\n",
            "Epoch 83 (train)3.1s: train loss = 0.02532 = (mf)0.01709 + (embed)0.00823\n",
            "Epoch 84 (train)3.8s: train loss = 0.02603 = (mf)0.01778 + (embed)0.00825\n",
            "Epoch 85 (train)3.0s + (eval)0.6s: train loss = 0.02540 = (mf)0.01713 + (embed)0.00826, recall = 0.35453, ndcg = 0.34922\n",
            "Epoch 86 (train)3.0s: train loss = 0.02634 = (mf)0.01805 + (embed)0.00829\n",
            "Epoch 87 (train)3.1s: train loss = 0.02521 = (mf)0.01689 + (embed)0.00833\n",
            "Epoch 88 (train)3.7s: train loss = 0.02552 = (mf)0.01718 + (embed)0.00834\n",
            "Epoch 89 (train)3.1s: train loss = 0.02504 = (mf)0.01667 + (embed)0.00837\n",
            "Epoch 90 (train)3.0s + (eval)0.2s: train loss = 0.02541 = (mf)0.01702 + (embed)0.00839, recall = 0.35208, ndcg = 0.34742\n",
            "Epoch 91 (train)3.4s: train loss = 0.02456 = (mf)0.01617 + (embed)0.00839\n",
            "Epoch 92 (train)3.5s: train loss = 0.02515 = (mf)0.01675 + (embed)0.00841\n",
            "Epoch 93 (train)3.0s: train loss = 0.02500 = (mf)0.01657 + (embed)0.00843\n",
            "Epoch 94 (train)3.0s: train loss = 0.02415 = (mf)0.01569 + (embed)0.00846\n",
            "Epoch 95 (train)3.5s + (eval)0.4s: train loss = 0.02480 = (mf)0.01631 + (embed)0.00849, recall = 0.35582, ndcg = 0.34977\n",
            "Epoch 96 (train)3.3s: train loss = 0.02458 = (mf)0.01606 + (embed)0.00852\n",
            "Epoch 97 (train)3.0s: train loss = 0.02468 = (mf)0.01615 + (embed)0.00852\n",
            "Epoch 98 (train)3.0s: train loss = 0.02433 = (mf)0.01578 + (embed)0.00855\n",
            "Epoch 99 (train)3.7s: train loss = 0.02412 = (mf)0.01555 + (embed)0.00857\n",
            "Epoch 100 (train)3.1s + (eval)0.2s: train loss = 0.02312 = (mf)0.01456 + (embed)0.00857, recall = 0.35445, ndcg = 0.34929\n",
            "Training finished in 332.16 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.34929, Recall@50: 0.35445\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.005, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.3s: train loss = 0.41169 = (mf)0.41144 + (embed)0.00025\n",
            "Epoch 2 (train)3.6s: train loss = 0.23011 = (mf)0.22946 + (embed)0.00065\n",
            "Epoch 3 (train)4.4s: train loss = 0.21502 = (mf)0.21427 + (embed)0.00075\n",
            "Epoch 4 (train)3.6s: train loss = 0.19738 = (mf)0.19650 + (embed)0.00088\n",
            "Epoch 5 (train)3.5s + (eval)1.1s: train loss = 0.18502 = (mf)0.18398 + (embed)0.00103, recall = 0.22058, ndcg = 0.22320\n",
            "Epoch 6 (train)4.4s: train loss = 0.17158 = (mf)0.17037 + (embed)0.00121\n",
            "Epoch 7 (train)3.6s: train loss = 0.15894 = (mf)0.15757 + (embed)0.00137\n",
            "Epoch 8 (train)3.6s: train loss = 0.15319 = (mf)0.15166 + (embed)0.00153\n",
            "Epoch 9 (train)4.4s: train loss = 0.14538 = (mf)0.14370 + (embed)0.00168\n",
            "Epoch 10 (train)3.5s + (eval)0.2s: train loss = 0.14089 = (mf)0.13905 + (embed)0.00184, recall = 0.25986, ndcg = 0.25843\n",
            "Epoch 11 (train)3.5s: train loss = 0.13536 = (mf)0.13336 + (embed)0.00200\n",
            "Epoch 12 (train)3.9s: train loss = 0.12943 = (mf)0.12726 + (embed)0.00216\n",
            "Epoch 13 (train)3.9s: train loss = 0.12460 = (mf)0.12226 + (embed)0.00233\n",
            "Epoch 14 (train)3.5s: train loss = 0.12101 = (mf)0.11850 + (embed)0.00251\n",
            "Epoch 15 (train)3.5s + (eval)0.2s: train loss = 0.11791 = (mf)0.11523 + (embed)0.00269, recall = 0.27766, ndcg = 0.27300\n",
            "Epoch 16 (train)4.3s: train loss = 0.11296 = (mf)0.11010 + (embed)0.00287\n",
            "Epoch 17 (train)3.5s: train loss = 0.10583 = (mf)0.10278 + (embed)0.00306\n",
            "Epoch 18 (train)3.5s: train loss = 0.10023 = (mf)0.09697 + (embed)0.00326\n",
            "Epoch 19 (train)4.4s: train loss = 0.09582 = (mf)0.09237 + (embed)0.00346\n",
            "Epoch 20 (train)3.6s + (eval)0.7s: train loss = 0.09451 = (mf)0.09087 + (embed)0.00365, recall = 0.29150, ndcg = 0.29005\n",
            "Epoch 21 (train)3.6s: train loss = 0.08921 = (mf)0.08537 + (embed)0.00384\n",
            "Epoch 22 (train)4.4s: train loss = 0.08711 = (mf)0.08309 + (embed)0.00402\n",
            "Epoch 23 (train)3.6s: train loss = 0.08513 = (mf)0.08095 + (embed)0.00419\n",
            "Epoch 24 (train)3.6s: train loss = 0.08438 = (mf)0.08002 + (embed)0.00436\n",
            "Epoch 25 (train)4.1s + (eval)0.4s: train loss = 0.07849 = (mf)0.07393 + (embed)0.00456, recall = 0.30630, ndcg = 0.30560\n",
            "Epoch 26 (train)3.8s: train loss = 0.07646 = (mf)0.07174 + (embed)0.00472\n",
            "Epoch 27 (train)3.6s: train loss = 0.07551 = (mf)0.07062 + (embed)0.00489\n",
            "Epoch 28 (train)3.6s: train loss = 0.07190 = (mf)0.06682 + (embed)0.00508\n",
            "Epoch 29 (train)4.3s: train loss = 0.06938 = (mf)0.06412 + (embed)0.00525\n",
            "Epoch 30 (train)3.6s + (eval)0.2s: train loss = 0.06668 = (mf)0.06126 + (embed)0.00542, recall = 0.31736, ndcg = 0.32107\n",
            "Epoch 31 (train)3.6s: train loss = 0.06569 = (mf)0.06013 + (embed)0.00557\n",
            "Epoch 32 (train)4.4s: train loss = 0.06429 = (mf)0.05860 + (embed)0.00569\n",
            "Epoch 33 (train)3.6s: train loss = 0.06273 = (mf)0.05688 + (embed)0.00585\n",
            "Epoch 34 (train)3.6s: train loss = 0.06179 = (mf)0.05578 + (embed)0.00600\n",
            "Epoch 35 (train)4.4s + (eval)0.2s: train loss = 0.06079 = (mf)0.05464 + (embed)0.00614, recall = 0.32765, ndcg = 0.32930\n",
            "Epoch 36 (train)3.5s: train loss = 0.06004 = (mf)0.05376 + (embed)0.00628\n",
            "Epoch 37 (train)3.5s: train loss = 0.05994 = (mf)0.05353 + (embed)0.00641\n",
            "Epoch 38 (train)4.0s: train loss = 0.05784 = (mf)0.05130 + (embed)0.00653\n",
            "Epoch 39 (train)3.8s: train loss = 0.05535 = (mf)0.04868 + (embed)0.00667\n",
            "Epoch 40 (train)3.5s + (eval)0.6s: train loss = 0.05468 = (mf)0.04789 + (embed)0.00678, recall = 0.33870, ndcg = 0.34001\n",
            "Epoch 41 (train)3.6s: train loss = 0.05355 = (mf)0.04665 + (embed)0.00690\n",
            "Epoch 42 (train)4.1s: train loss = 0.05396 = (mf)0.04695 + (embed)0.00702\n",
            "Epoch 43 (train)3.5s: train loss = 0.05179 = (mf)0.04465 + (embed)0.00714\n",
            "Epoch 44 (train)3.4s: train loss = 0.05166 = (mf)0.04443 + (embed)0.00724\n",
            "Epoch 45 (train)4.3s + (eval)0.2s: train loss = 0.05092 = (mf)0.04359 + (embed)0.00733, recall = 0.33969, ndcg = 0.34084\n",
            "Epoch 46 (train)3.5s: train loss = 0.05058 = (mf)0.04315 + (embed)0.00743\n",
            "Epoch 47 (train)3.5s: train loss = 0.04927 = (mf)0.04174 + (embed)0.00753\n",
            "Epoch 48 (train)4.3s: train loss = 0.04770 = (mf)0.04006 + (embed)0.00764\n",
            "Epoch 49 (train)3.7s: train loss = 0.04783 = (mf)0.04009 + (embed)0.00774\n",
            "Epoch 50 (train)3.5s + (eval)0.2s: train loss = 0.04707 = (mf)0.03923 + (embed)0.00784, recall = 0.34330, ndcg = 0.34459\n",
            "Epoch 51 (train)3.8s: train loss = 0.04738 = (mf)0.03947 + (embed)0.00792\n",
            "Epoch 52 (train)4.0s: train loss = 0.04639 = (mf)0.03839 + (embed)0.00800\n",
            "Epoch 53 (train)3.5s: train loss = 0.04623 = (mf)0.03817 + (embed)0.00806\n",
            "Epoch 54 (train)3.5s: train loss = 0.04499 = (mf)0.03684 + (embed)0.00815\n",
            "Epoch 55 (train)4.3s + (eval)0.2s: train loss = 0.04438 = (mf)0.03615 + (embed)0.00823, recall = 0.34760, ndcg = 0.34820\n",
            "Epoch 56 (train)3.5s: train loss = 0.04341 = (mf)0.03509 + (embed)0.00832\n",
            "Epoch 57 (train)3.6s: train loss = 0.04262 = (mf)0.03422 + (embed)0.00840\n",
            "Epoch 58 (train)4.3s: train loss = 0.04236 = (mf)0.03391 + (embed)0.00845\n",
            "Epoch 59 (train)3.5s: train loss = 0.04229 = (mf)0.03374 + (embed)0.00855\n",
            "Epoch 60 (train)3.5s + (eval)0.7s: train loss = 0.04089 = (mf)0.03227 + (embed)0.00862, recall = 0.35028, ndcg = 0.35008\n",
            "Epoch 61 (train)4.3s: train loss = 0.04180 = (mf)0.03310 + (embed)0.00870\n",
            "Epoch 62 (train)3.8s: train loss = 0.04138 = (mf)0.03264 + (embed)0.00875\n",
            "Epoch 63 (train)3.7s: train loss = 0.03963 = (mf)0.03080 + (embed)0.00883\n",
            "Epoch 64 (train)4.1s: train loss = 0.03997 = (mf)0.03111 + (embed)0.00887\n",
            "Epoch 65 (train)4.1s + (eval)0.2s: train loss = 0.03919 = (mf)0.03026 + (embed)0.00893, recall = 0.35389, ndcg = 0.35229\n",
            "Epoch 66 (train)3.6s: train loss = 0.03903 = (mf)0.03002 + (embed)0.00901\n",
            "Epoch 67 (train)3.7s: train loss = 0.03858 = (mf)0.02954 + (embed)0.00904\n",
            "Epoch 68 (train)4.3s: train loss = 0.03754 = (mf)0.02842 + (embed)0.00912\n",
            "Epoch 69 (train)3.6s: train loss = 0.03811 = (mf)0.02895 + (embed)0.00915\n",
            "Epoch 70 (train)3.6s + (eval)0.2s: train loss = 0.03911 = (mf)0.02991 + (embed)0.00920, recall = 0.35487, ndcg = 0.35503\n",
            "Epoch 71 (train)4.5s: train loss = 0.03675 = (mf)0.02750 + (embed)0.00925\n",
            "Epoch 72 (train)3.7s: train loss = 0.03640 = (mf)0.02710 + (embed)0.00930\n",
            "Epoch 73 (train)3.7s: train loss = 0.03690 = (mf)0.02754 + (embed)0.00935\n",
            "Epoch 74 (train)4.5s: train loss = 0.03691 = (mf)0.02750 + (embed)0.00941\n",
            "Epoch 75 (train)3.7s + (eval)0.3s: train loss = 0.03597 = (mf)0.02651 + (embed)0.00946, recall = 0.35459, ndcg = 0.35623\n",
            "Epoch 76 (train)3.9s: train loss = 0.03557 = (mf)0.02607 + (embed)0.00950\n",
            "Epoch 77 (train)4.7s: train loss = 0.03591 = (mf)0.02636 + (embed)0.00955\n",
            "Epoch 78 (train)3.8s: train loss = 0.03434 = (mf)0.02475 + (embed)0.00958\n",
            "Epoch 79 (train)3.6s: train loss = 0.03484 = (mf)0.02521 + (embed)0.00962\n",
            "Epoch 80 (train)4.3s + (eval)0.8s: train loss = 0.03346 = (mf)0.02380 + (embed)0.00966, recall = 0.35675, ndcg = 0.35675\n",
            "Epoch 81 (train)3.6s: train loss = 0.03388 = (mf)0.02417 + (embed)0.00971\n",
            "Epoch 82 (train)3.6s: train loss = 0.03341 = (mf)0.02368 + (embed)0.00973\n",
            "Epoch 83 (train)4.1s: train loss = 0.03316 = (mf)0.02339 + (embed)0.00977\n",
            "Epoch 84 (train)3.9s: train loss = 0.03449 = (mf)0.02469 + (embed)0.00980\n",
            "Epoch 85 (train)3.6s + (eval)0.2s: train loss = 0.03306 = (mf)0.02323 + (embed)0.00984, recall = 0.35975, ndcg = 0.35657\n",
            "Epoch 86 (train)3.6s: train loss = 0.03264 = (mf)0.02277 + (embed)0.00987\n",
            "Epoch 87 (train)4.4s: train loss = 0.03329 = (mf)0.02337 + (embed)0.00992\n",
            "Epoch 88 (train)3.6s: train loss = 0.03334 = (mf)0.02339 + (embed)0.00995\n",
            "Epoch 89 (train)3.6s: train loss = 0.03340 = (mf)0.02344 + (embed)0.00996\n",
            "Epoch 90 (train)4.4s + (eval)0.2s: train loss = 0.03311 = (mf)0.02312 + (embed)0.00999, recall = 0.35912, ndcg = 0.35642\n",
            "Epoch 91 (train)3.6s: train loss = 0.03348 = (mf)0.02344 + (embed)0.01004\n",
            "Epoch 92 (train)3.6s: train loss = 0.03162 = (mf)0.02155 + (embed)0.01007\n",
            "Epoch 93 (train)4.4s: train loss = 0.03135 = (mf)0.02126 + (embed)0.01010\n",
            "Epoch 94 (train)3.5s: train loss = 0.03107 = (mf)0.02093 + (embed)0.01013\n",
            "Epoch 95 (train)3.5s + (eval)0.2s: train loss = 0.03056 = (mf)0.02041 + (embed)0.01015, recall = 0.36044, ndcg = 0.35977\n",
            "Epoch 96 (train)4.0s: train loss = 0.03040 = (mf)0.02021 + (embed)0.01019\n",
            "Epoch 97 (train)3.9s: train loss = 0.03133 = (mf)0.02112 + (embed)0.01021\n",
            "Epoch 98 (train)3.5s: train loss = 0.03140 = (mf)0.02116 + (embed)0.01024\n",
            "Epoch 99 (train)3.5s: train loss = 0.03052 = (mf)0.02026 + (embed)0.01027\n",
            "Epoch 100 (train)4.3s + (eval)0.7s: train loss = 0.03037 = (mf)0.02009 + (embed)0.01028, recall = 0.35932, ndcg = 0.35889\n",
            "Training finished in 392.09 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.35889, Recall@50: 0.35932\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.005, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.8s: train loss = 0.41515 = (mf)0.41490 + (embed)0.00025\n",
            "Epoch 2 (train)4.2s: train loss = 0.23946 = (mf)0.23883 + (embed)0.00063\n",
            "Epoch 3 (train)4.9s: train loss = 0.22670 = (mf)0.22596 + (embed)0.00074\n",
            "Epoch 4 (train)4.0s: train loss = 0.21897 = (mf)0.21809 + (embed)0.00088\n",
            "Epoch 5 (train)4.0s + (eval)1.3s: train loss = 0.21336 = (mf)0.21234 + (embed)0.00102, recall = 0.20264, ndcg = 0.20168\n",
            "Epoch 6 (train)4.7s: train loss = 0.20581 = (mf)0.20463 + (embed)0.00118\n",
            "Epoch 7 (train)4.2s: train loss = 0.19677 = (mf)0.19540 + (embed)0.00137\n",
            "Epoch 8 (train)4.6s: train loss = 0.18300 = (mf)0.18143 + (embed)0.00157\n",
            "Epoch 9 (train)4.5s: train loss = 0.17037 = (mf)0.16857 + (embed)0.00180\n",
            "Epoch 10 (train)4.1s + (eval)0.2s: train loss = 0.16055 = (mf)0.15849 + (embed)0.00206, recall = 0.24901, ndcg = 0.24975\n",
            "Epoch 11 (train)4.9s: train loss = 0.15082 = (mf)0.14852 + (embed)0.00230\n",
            "Epoch 12 (train)4.1s: train loss = 0.14194 = (mf)0.13941 + (embed)0.00253\n",
            "Epoch 13 (train)4.1s: train loss = 0.14257 = (mf)0.13985 + (embed)0.00272\n",
            "Epoch 14 (train)5.0s: train loss = 0.13468 = (mf)0.13179 + (embed)0.00289\n",
            "Epoch 15 (train)4.2s + (eval)0.7s: train loss = 0.13007 = (mf)0.12698 + (embed)0.00309, recall = 0.26513, ndcg = 0.26578\n",
            "Epoch 16 (train)4.1s: train loss = 0.12733 = (mf)0.12407 + (embed)0.00326\n",
            "Epoch 17 (train)4.9s: train loss = 0.12628 = (mf)0.12285 + (embed)0.00344\n",
            "Epoch 18 (train)4.1s: train loss = 0.11750 = (mf)0.11385 + (embed)0.00365\n",
            "Epoch 19 (train)4.2s: train loss = 0.11407 = (mf)0.11023 + (embed)0.00384\n",
            "Epoch 20 (train)4.8s + (eval)0.3s: train loss = 0.11440 = (mf)0.11038 + (embed)0.00402, recall = 0.28172, ndcg = 0.27999\n",
            "Epoch 21 (train)4.1s: train loss = 0.10919 = (mf)0.10498 + (embed)0.00421\n",
            "Epoch 22 (train)4.5s: train loss = 0.10480 = (mf)0.10039 + (embed)0.00441\n",
            "Epoch 23 (train)4.5s: train loss = 0.10261 = (mf)0.09801 + (embed)0.00460\n",
            "Epoch 24 (train)4.1s: train loss = 0.09784 = (mf)0.09306 + (embed)0.00478\n",
            "Epoch 25 (train)4.7s + (eval)0.4s: train loss = 0.09961 = (mf)0.09466 + (embed)0.00495, recall = 0.29070, ndcg = 0.28743\n",
            "Epoch 26 (train)4.2s: train loss = 0.09621 = (mf)0.09107 + (embed)0.00514\n",
            "Epoch 27 (train)4.1s: train loss = 0.09217 = (mf)0.08684 + (embed)0.00534\n",
            "Epoch 28 (train)4.9s: train loss = 0.08849 = (mf)0.08297 + (embed)0.00552\n",
            "Epoch 29 (train)4.0s: train loss = 0.08622 = (mf)0.08051 + (embed)0.00570\n",
            "Epoch 30 (train)4.1s + (eval)0.2s: train loss = 0.08591 = (mf)0.08001 + (embed)0.00590, recall = 0.30116, ndcg = 0.30111\n",
            "Epoch 31 (train)4.9s: train loss = 0.08446 = (mf)0.07840 + (embed)0.00607\n",
            "Epoch 32 (train)4.1s: train loss = 0.08032 = (mf)0.07406 + (embed)0.00626\n",
            "Epoch 33 (train)4.0s: train loss = 0.07820 = (mf)0.07177 + (embed)0.00644\n",
            "Epoch 34 (train)4.9s: train loss = 0.07975 = (mf)0.07313 + (embed)0.00662\n",
            "Epoch 35 (train)4.1s + (eval)0.7s: train loss = 0.07633 = (mf)0.06956 + (embed)0.00678, recall = 0.31336, ndcg = 0.31360\n",
            "Epoch 36 (train)4.3s: train loss = 0.07448 = (mf)0.06756 + (embed)0.00692\n",
            "Epoch 37 (train)4.7s: train loss = 0.07374 = (mf)0.06666 + (embed)0.00708\n",
            "Epoch 38 (train)4.1s: train loss = 0.07233 = (mf)0.06508 + (embed)0.00724\n",
            "Epoch 39 (train)4.7s: train loss = 0.07186 = (mf)0.06447 + (embed)0.00739\n",
            "Epoch 40 (train)4.4s + (eval)0.2s: train loss = 0.06924 = (mf)0.06170 + (embed)0.00754, recall = 0.32072, ndcg = 0.32304\n",
            "Epoch 41 (train)4.1s: train loss = 0.06909 = (mf)0.06143 + (embed)0.00766\n",
            "Epoch 42 (train)4.9s: train loss = 0.06834 = (mf)0.06055 + (embed)0.00779\n",
            "Epoch 43 (train)4.1s: train loss = 0.06879 = (mf)0.06087 + (embed)0.00792\n",
            "Epoch 44 (train)4.1s: train loss = 0.06594 = (mf)0.05786 + (embed)0.00807\n",
            "Epoch 45 (train)4.9s + (eval)0.2s: train loss = 0.06544 = (mf)0.05721 + (embed)0.00823, recall = 0.33027, ndcg = 0.33354\n",
            "Epoch 46 (train)4.1s: train loss = 0.06345 = (mf)0.05512 + (embed)0.00833\n",
            "Epoch 47 (train)4.1s: train loss = 0.06460 = (mf)0.05616 + (embed)0.00844\n",
            "Epoch 48 (train)4.9s: train loss = 0.06163 = (mf)0.05308 + (embed)0.00855\n",
            "Epoch 49 (train)4.2s: train loss = 0.05913 = (mf)0.05042 + (embed)0.00871\n",
            "Epoch 50 (train)4.2s + (eval)0.9s: train loss = 0.06044 = (mf)0.05164 + (embed)0.00880, recall = 0.33332, ndcg = 0.33557\n",
            "Epoch 51 (train)4.8s: train loss = 0.05925 = (mf)0.05033 + (embed)0.00892\n",
            "Epoch 52 (train)4.3s: train loss = 0.05905 = (mf)0.05002 + (embed)0.00903\n",
            "Epoch 53 (train)4.6s: train loss = 0.06022 = (mf)0.05111 + (embed)0.00911\n",
            "Epoch 54 (train)4.3s: train loss = 0.05568 = (mf)0.04648 + (embed)0.00921\n",
            "Epoch 55 (train)4.1s + (eval)0.2s: train loss = 0.05758 = (mf)0.04828 + (embed)0.00931, recall = 0.34039, ndcg = 0.34085\n",
            "Epoch 56 (train)4.9s: train loss = 0.05722 = (mf)0.04783 + (embed)0.00939\n",
            "Epoch 57 (train)4.1s: train loss = 0.05686 = (mf)0.04738 + (embed)0.00948\n",
            "Epoch 58 (train)4.1s: train loss = 0.05477 = (mf)0.04519 + (embed)0.00958\n",
            "Epoch 59 (train)5.0s: train loss = 0.05481 = (mf)0.04515 + (embed)0.00966\n",
            "Epoch 60 (train)4.2s + (eval)0.3s: train loss = 0.05449 = (mf)0.04472 + (embed)0.00977, recall = 0.34466, ndcg = 0.34476\n",
            "Epoch 61 (train)4.2s: train loss = 0.05261 = (mf)0.04278 + (embed)0.00983\n",
            "Epoch 62 (train)5.0s: train loss = 0.05144 = (mf)0.04152 + (embed)0.00992\n",
            "Epoch 63 (train)4.2s: train loss = 0.05147 = (mf)0.04143 + (embed)0.01004\n",
            "Epoch 64 (train)4.4s: train loss = 0.05143 = (mf)0.04134 + (embed)0.01009\n",
            "Epoch 65 (train)4.8s + (eval)0.2s: train loss = 0.05081 = (mf)0.04064 + (embed)0.01016, recall = 0.34971, ndcg = 0.34830\n",
            "Epoch 66 (train)4.2s: train loss = 0.05064 = (mf)0.04040 + (embed)0.01024\n",
            "Epoch 67 (train)4.7s: train loss = 0.04882 = (mf)0.03848 + (embed)0.01034\n",
            "Epoch 68 (train)4.4s: train loss = 0.04891 = (mf)0.03853 + (embed)0.01039\n",
            "Epoch 69 (train)4.1s: train loss = 0.04818 = (mf)0.03772 + (embed)0.01046\n",
            "Epoch 70 (train)4.9s + (eval)0.7s: train loss = 0.04907 = (mf)0.03852 + (embed)0.01055, recall = 0.34858, ndcg = 0.34817\n",
            "Epoch 71 (train)4.1s: train loss = 0.04851 = (mf)0.03791 + (embed)0.01060\n",
            "Epoch 72 (train)4.2s: train loss = 0.04988 = (mf)0.03924 + (embed)0.01064\n",
            "Epoch 73 (train)5.0s: train loss = 0.04784 = (mf)0.03713 + (embed)0.01070\n",
            "Epoch 74 (train)4.2s: train loss = 0.04719 = (mf)0.03644 + (embed)0.01074\n",
            "Epoch 75 (train)4.1s + (eval)0.2s: train loss = 0.04603 = (mf)0.03521 + (embed)0.01082, recall = 0.35134, ndcg = 0.35050\n",
            "Epoch 76 (train)4.9s: train loss = 0.04548 = (mf)0.03459 + (embed)0.01089\n",
            "Epoch 77 (train)4.1s: train loss = 0.04622 = (mf)0.03528 + (embed)0.01094\n",
            "Epoch 78 (train)4.3s: train loss = 0.04571 = (mf)0.03472 + (embed)0.01098\n",
            "Epoch 79 (train)4.8s: train loss = 0.04408 = (mf)0.03303 + (embed)0.01104\n",
            "Epoch 80 (train)4.2s + (eval)0.3s: train loss = 0.04478 = (mf)0.03366 + (embed)0.01112, recall = 0.35308, ndcg = 0.35332\n",
            "Epoch 81 (train)4.8s: train loss = 0.04537 = (mf)0.03423 + (embed)0.01114\n",
            "Epoch 82 (train)4.3s: train loss = 0.04443 = (mf)0.03323 + (embed)0.01120\n",
            "Epoch 83 (train)4.1s: train loss = 0.04393 = (mf)0.03268 + (embed)0.01126\n",
            "Epoch 84 (train)4.9s: train loss = 0.04298 = (mf)0.03167 + (embed)0.01131\n",
            "Epoch 85 (train)4.1s + (eval)0.2s: train loss = 0.04222 = (mf)0.03089 + (embed)0.01133, recall = 0.35287, ndcg = 0.35033\n",
            "Epoch 86 (train)4.1s: train loss = 0.04271 = (mf)0.03133 + (embed)0.01138\n",
            "Epoch 87 (train)4.9s: train loss = 0.04233 = (mf)0.03090 + (embed)0.01143\n",
            "Epoch 88 (train)4.2s: train loss = 0.04355 = (mf)0.03210 + (embed)0.01145\n",
            "Epoch 89 (train)4.2s: train loss = 0.04197 = (mf)0.03045 + (embed)0.01151\n",
            "Epoch 90 (train)4.9s + (eval)0.7s: train loss = 0.04266 = (mf)0.03110 + (embed)0.01156, recall = 0.35576, ndcg = 0.35465\n",
            "Epoch 91 (train)4.1s: train loss = 0.04172 = (mf)0.03013 + (embed)0.01159\n",
            "Epoch 92 (train)4.4s: train loss = 0.04133 = (mf)0.02968 + (embed)0.01166\n",
            "Epoch 93 (train)4.6s: train loss = 0.04207 = (mf)0.03040 + (embed)0.01168\n",
            "Epoch 94 (train)4.1s: train loss = 0.03985 = (mf)0.02815 + (embed)0.01170\n",
            "Epoch 95 (train)4.6s + (eval)0.4s: train loss = 0.03930 = (mf)0.02757 + (embed)0.01173, recall = 0.35920, ndcg = 0.35723\n",
            "Epoch 96 (train)4.4s: train loss = 0.04114 = (mf)0.02936 + (embed)0.01178\n",
            "Epoch 97 (train)4.1s: train loss = 0.03948 = (mf)0.02764 + (embed)0.01184\n",
            "Epoch 98 (train)4.9s: train loss = 0.04061 = (mf)0.02877 + (embed)0.01184\n",
            "Epoch 99 (train)4.1s: train loss = 0.03919 = (mf)0.02730 + (embed)0.01189\n",
            "Epoch 100 (train)4.1s + (eval)0.2s: train loss = 0.03943 = (mf)0.02751 + (embed)0.01192, recall = 0.35957, ndcg = 0.35971\n",
            "Training finished in 449.98 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.35971, Recall@50: 0.35957\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.05, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)4.8s: train loss = 0.22573 = (mf)0.22417 + (embed)0.00155\n",
            "Epoch 2 (train)3.6s: train loss = 0.11133 = (mf)0.10786 + (embed)0.00347\n",
            "Epoch 3 (train)3.0s: train loss = 0.08088 = (mf)0.07563 + (embed)0.00524\n",
            "Epoch 4 (train)3.0s: train loss = 0.06727 = (mf)0.06039 + (embed)0.00688\n",
            "Epoch 5 (train)3.3s + (eval)2.1s: train loss = 0.05746 = (mf)0.04918 + (embed)0.00828, recall = 0.29699, ndcg = 0.28214\n",
            "Epoch 6 (train)3.0s: train loss = 0.04985 = (mf)0.04030 + (embed)0.00954\n",
            "Epoch 7 (train)3.0s: train loss = 0.04972 = (mf)0.03911 + (embed)0.01061\n",
            "Epoch 8 (train)3.0s: train loss = 0.04714 = (mf)0.03562 + (embed)0.01152\n",
            "Epoch 9 (train)3.9s: train loss = 0.04741 = (mf)0.03514 + (embed)0.01227\n",
            "Epoch 10 (train)3.1s + (eval)0.2s: train loss = 0.04503 = (mf)0.03201 + (embed)0.01302, recall = 0.28515, ndcg = 0.26923\n",
            "Epoch 11 (train)3.1s: train loss = 0.04564 = (mf)0.03188 + (embed)0.01376\n",
            "Epoch 12 (train)3.1s: train loss = 0.04481 = (mf)0.03038 + (embed)0.01442\n",
            "Epoch 13 (train)3.7s: train loss = 0.04406 = (mf)0.02902 + (embed)0.01505\n",
            "Epoch 14 (train)3.0s: train loss = 0.04643 = (mf)0.03082 + (embed)0.01561\n",
            "Epoch 15 (train)3.0s + (eval)0.2s: train loss = 0.04601 = (mf)0.02991 + (embed)0.01610, recall = 0.28237, ndcg = 0.26125\n",
            "Epoch 16 (train)3.3s: train loss = 0.04779 = (mf)0.03117 + (embed)0.01661\n",
            "Epoch 17 (train)3.6s: train loss = 0.04596 = (mf)0.02880 + (embed)0.01716\n",
            "Epoch 18 (train)2.9s: train loss = 0.04794 = (mf)0.03031 + (embed)0.01763\n",
            "Epoch 19 (train)3.0s: train loss = 0.04655 = (mf)0.02851 + (embed)0.01804\n",
            "Epoch 20 (train)3.4s + (eval)0.3s: train loss = 0.04749 = (mf)0.02899 + (embed)0.01851, recall = 0.27163, ndcg = 0.24857\n",
            "Epoch 21 (train)3.3s: train loss = 0.04714 = (mf)0.02823 + (embed)0.01891\n",
            "Epoch 22 (train)3.0s: train loss = 0.04780 = (mf)0.02849 + (embed)0.01931\n",
            "Epoch 23 (train)3.0s: train loss = 0.04737 = (mf)0.02769 + (embed)0.01968\n",
            "Epoch 24 (train)3.5s: train loss = 0.04779 = (mf)0.02769 + (embed)0.02010\n",
            "Epoch 25 (train)3.3s + (eval)0.2s: train loss = 0.04721 = (mf)0.02683 + (embed)0.02037, recall = 0.27091, ndcg = 0.25281\n",
            "Epoch 26 (train)3.0s: train loss = 0.04923 = (mf)0.02845 + (embed)0.02078\n",
            "Epoch 27 (train)3.0s: train loss = 0.04955 = (mf)0.02853 + (embed)0.02102\n",
            "Epoch 28 (train)3.9s: train loss = 0.04864 = (mf)0.02727 + (embed)0.02137\n",
            "Epoch 29 (train)3.0s: train loss = 0.05075 = (mf)0.02915 + (embed)0.02161\n",
            "Epoch 30 (train)3.0s + (eval)0.7s: train loss = 0.05039 = (mf)0.02841 + (embed)0.02198, recall = 0.26750, ndcg = 0.24970\n",
            "Epoch 31 (train)3.0s: train loss = 0.05152 = (mf)0.02925 + (embed)0.02227\n",
            "Epoch 32 (train)3.8s: train loss = 0.05238 = (mf)0.02983 + (embed)0.02254\n",
            "Epoch 33 (train)3.0s: train loss = 0.05361 = (mf)0.03063 + (embed)0.02297\n",
            "Epoch 34 (train)3.0s: train loss = 0.04961 = (mf)0.02634 + (embed)0.02326\n",
            "Epoch 35 (train)3.0s + (eval)0.3s: train loss = 0.05255 = (mf)0.02905 + (embed)0.02350, recall = 0.25075, ndcg = 0.22441\n",
            "Epoch 36 (train)3.7s: train loss = 0.05272 = (mf)0.02898 + (embed)0.02374\n",
            "Epoch 37 (train)3.0s: train loss = 0.05291 = (mf)0.02892 + (embed)0.02399\n",
            "Epoch 38 (train)3.0s: train loss = 0.05370 = (mf)0.02943 + (embed)0.02427\n",
            "Epoch 39 (train)3.1s: train loss = 0.05331 = (mf)0.02883 + (embed)0.02448\n",
            "Epoch 40 (train)3.7s + (eval)0.2s: train loss = 0.05147 = (mf)0.02686 + (embed)0.02462, recall = 0.25947, ndcg = 0.24016\n",
            "Epoch 41 (train)3.0s: train loss = 0.05140 = (mf)0.02661 + (embed)0.02480\n",
            "Epoch 42 (train)3.1s: train loss = 0.05304 = (mf)0.02808 + (embed)0.02496\n",
            "Epoch 43 (train)3.5s: train loss = 0.05492 = (mf)0.02982 + (embed)0.02509\n",
            "Epoch 44 (train)3.4s: train loss = 0.05286 = (mf)0.02749 + (embed)0.02537\n",
            "Epoch 45 (train)3.0s + (eval)0.2s: train loss = 0.05437 = (mf)0.02878 + (embed)0.02560, recall = 0.25460, ndcg = 0.23400\n",
            "Epoch 46 (train)3.0s: train loss = 0.05221 = (mf)0.02640 + (embed)0.02581\n",
            "Epoch 47 (train)3.7s: train loss = 0.05169 = (mf)0.02588 + (embed)0.02581\n",
            "Epoch 48 (train)3.1s: train loss = 0.05424 = (mf)0.02825 + (embed)0.02599\n",
            "Epoch 49 (train)3.0s: train loss = 0.05440 = (mf)0.02831 + (embed)0.02609\n",
            "Epoch 50 (train)3.0s + (eval)0.7s: train loss = 0.05044 = (mf)0.02424 + (embed)0.02620, recall = 0.25128, ndcg = 0.22706\n",
            "Epoch 51 (train)3.8s: train loss = 0.05183 = (mf)0.02543 + (embed)0.02640\n",
            "Epoch 52 (train)3.0s: train loss = 0.05147 = (mf)0.02502 + (embed)0.02645\n",
            "Epoch 53 (train)3.0s: train loss = 0.05212 = (mf)0.02561 + (embed)0.02651\n",
            "Epoch 54 (train)3.0s: train loss = 0.05402 = (mf)0.02742 + (embed)0.02660\n",
            "Epoch 55 (train)3.9s + (eval)0.3s: train loss = 0.05219 = (mf)0.02549 + (embed)0.02670, recall = 0.25287, ndcg = 0.22863\n",
            "Epoch 56 (train)3.1s: train loss = 0.05428 = (mf)0.02742 + (embed)0.02686\n",
            "Epoch 57 (train)3.1s: train loss = 0.05630 = (mf)0.02936 + (embed)0.02693\n",
            "Epoch 58 (train)3.2s: train loss = 0.05674 = (mf)0.02962 + (embed)0.02712\n",
            "Epoch 59 (train)3.7s: train loss = 0.05254 = (mf)0.02534 + (embed)0.02720\n",
            "Epoch 60 (train)3.0s + (eval)0.2s: train loss = 0.05314 = (mf)0.02574 + (embed)0.02740, recall = 0.24702, ndcg = 0.22584\n",
            "Epoch 61 (train)3.0s: train loss = 0.05247 = (mf)0.02501 + (embed)0.02745\n",
            "Epoch 62 (train)3.5s: train loss = 0.05398 = (mf)0.02631 + (embed)0.02767\n",
            "Epoch 63 (train)3.5s: train loss = 0.05090 = (mf)0.02314 + (embed)0.02775\n",
            "Epoch 64 (train)3.1s: train loss = 0.05322 = (mf)0.02543 + (embed)0.02779\n",
            "Epoch 65 (train)3.1s + (eval)0.2s: train loss = 0.05380 = (mf)0.02591 + (embed)0.02789, recall = 0.25525, ndcg = 0.23240\n",
            "Epoch 66 (train)3.9s: train loss = 0.05657 = (mf)0.02862 + (embed)0.02795\n",
            "Epoch 67 (train)3.1s: train loss = 0.05201 = (mf)0.02401 + (embed)0.02800\n",
            "Epoch 68 (train)3.0s: train loss = 0.05285 = (mf)0.02486 + (embed)0.02799\n",
            "Epoch 69 (train)3.0s: train loss = 0.05577 = (mf)0.02770 + (embed)0.02807\n",
            "Epoch 70 (train)3.9s + (eval)0.7s: train loss = 0.05150 = (mf)0.02340 + (embed)0.02811, recall = 0.24898, ndcg = 0.22786\n",
            "Epoch 71 (train)3.1s: train loss = 0.05340 = (mf)0.02525 + (embed)0.02815\n",
            "Epoch 72 (train)3.1s: train loss = 0.05228 = (mf)0.02406 + (embed)0.02822\n",
            "Epoch 73 (train)3.2s: train loss = 0.05381 = (mf)0.02563 + (embed)0.02818\n",
            "Epoch 74 (train)3.8s: train loss = 0.05322 = (mf)0.02495 + (embed)0.02827\n",
            "Epoch 75 (train)3.0s + (eval)0.2s: train loss = 0.05644 = (mf)0.02805 + (embed)0.02839, recall = 0.25240, ndcg = 0.22215\n",
            "Epoch 76 (train)3.0s: train loss = 0.05215 = (mf)0.02370 + (embed)0.02845\n",
            "Epoch 77 (train)3.3s: train loss = 0.05299 = (mf)0.02456 + (embed)0.02843\n",
            "Epoch 78 (train)3.5s: train loss = 0.05181 = (mf)0.02338 + (embed)0.02843\n",
            "Epoch 79 (train)3.0s: train loss = 0.05222 = (mf)0.02369 + (embed)0.02853\n",
            "Epoch 80 (train)3.0s + (eval)0.2s: train loss = 0.05279 = (mf)0.02428 + (embed)0.02851, recall = 0.25926, ndcg = 0.23145\n",
            "Epoch 81 (train)3.5s: train loss = 0.05213 = (mf)0.02347 + (embed)0.02866\n",
            "Epoch 82 (train)3.3s: train loss = 0.05092 = (mf)0.02219 + (embed)0.02874\n",
            "Epoch 83 (train)2.9s: train loss = 0.05354 = (mf)0.02473 + (embed)0.02880\n",
            "Epoch 84 (train)3.0s: train loss = 0.05692 = (mf)0.02810 + (embed)0.02882\n",
            "Epoch 85 (train)3.5s + (eval)0.9s: train loss = 0.05446 = (mf)0.02553 + (embed)0.02893, recall = 0.25404, ndcg = 0.23432\n",
            "Epoch 86 (train)3.0s: train loss = 0.05277 = (mf)0.02380 + (embed)0.02897\n",
            "Epoch 87 (train)3.0s: train loss = 0.05229 = (mf)0.02320 + (embed)0.02909\n",
            "Epoch 88 (train)3.0s: train loss = 0.05207 = (mf)0.02299 + (embed)0.02909\n",
            "Epoch 89 (train)3.9s: train loss = 0.05457 = (mf)0.02540 + (embed)0.02917\n",
            "Epoch 90 (train)3.0s + (eval)0.2s: train loss = 0.05372 = (mf)0.02448 + (embed)0.02924, recall = 0.25951, ndcg = 0.23855\n",
            "Epoch 91 (train)3.0s: train loss = 0.05716 = (mf)0.02788 + (embed)0.02928\n",
            "Epoch 92 (train)3.0s: train loss = 0.05493 = (mf)0.02551 + (embed)0.02942\n",
            "Epoch 93 (train)3.8s: train loss = 0.05181 = (mf)0.02238 + (embed)0.02943\n",
            "Epoch 94 (train)3.0s: train loss = 0.05260 = (mf)0.02313 + (embed)0.02946\n",
            "Epoch 95 (train)2.9s + (eval)0.2s: train loss = 0.05611 = (mf)0.02662 + (embed)0.02948, recall = 0.24741, ndcg = 0.23037\n",
            "Epoch 96 (train)3.0s: train loss = 0.05414 = (mf)0.02471 + (embed)0.02943\n",
            "Epoch 97 (train)3.8s: train loss = 0.05340 = (mf)0.02381 + (embed)0.02959\n",
            "Epoch 98 (train)3.0s: train loss = 0.05335 = (mf)0.02364 + (embed)0.02970\n",
            "Epoch 99 (train)2.9s: train loss = 0.05376 = (mf)0.02400 + (embed)0.02976\n",
            "Epoch 100 (train)3.0s + (eval)0.3s: train loss = 0.05221 = (mf)0.02241 + (embed)0.02980, recall = 0.25045, ndcg = 0.22850\n",
            "Training finished in 331.77 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.22850, Recall@50: 0.25045\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.05, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.2s: train loss = 0.22699 = (mf)0.22534 + (embed)0.00165\n",
            "Epoch 2 (train)3.6s: train loss = 0.12225 = (mf)0.11848 + (embed)0.00377\n",
            "Epoch 3 (train)3.6s: train loss = 0.09131 = (mf)0.08579 + (embed)0.00552\n",
            "Epoch 4 (train)4.4s: train loss = 0.07329 = (mf)0.06602 + (embed)0.00727\n",
            "Epoch 5 (train)3.5s + (eval)1.2s: train loss = 0.06515 = (mf)0.05630 + (embed)0.00885, recall = 0.30984, ndcg = 0.30434\n",
            "Epoch 6 (train)3.5s: train loss = 0.05923 = (mf)0.04907 + (embed)0.01016\n",
            "Epoch 7 (train)4.3s: train loss = 0.05442 = (mf)0.04313 + (embed)0.01129\n",
            "Epoch 8 (train)3.5s: train loss = 0.05043 = (mf)0.03815 + (embed)0.01228\n",
            "Epoch 9 (train)3.5s: train loss = 0.04756 = (mf)0.03445 + (embed)0.01311\n",
            "Epoch 10 (train)4.2s + (eval)0.4s: train loss = 0.04683 = (mf)0.03298 + (embed)0.01385, recall = 0.31567, ndcg = 0.30391\n",
            "Epoch 11 (train)3.7s: train loss = 0.04611 = (mf)0.03165 + (embed)0.01446\n",
            "Epoch 12 (train)3.6s: train loss = 0.04422 = (mf)0.02912 + (embed)0.01510\n",
            "Epoch 13 (train)3.8s: train loss = 0.04317 = (mf)0.02759 + (embed)0.01558\n",
            "Epoch 14 (train)4.2s: train loss = 0.04286 = (mf)0.02680 + (embed)0.01606\n",
            "Epoch 15 (train)3.6s + (eval)0.3s: train loss = 0.04335 = (mf)0.02690 + (embed)0.01645, recall = 0.30217, ndcg = 0.29101\n",
            "Epoch 16 (train)3.7s: train loss = 0.04249 = (mf)0.02565 + (embed)0.01684\n",
            "Epoch 17 (train)4.6s: train loss = 0.04234 = (mf)0.02512 + (embed)0.01722\n",
            "Epoch 18 (train)3.6s: train loss = 0.04219 = (mf)0.02460 + (embed)0.01759\n",
            "Epoch 19 (train)3.6s: train loss = 0.04255 = (mf)0.02454 + (embed)0.01801\n",
            "Epoch 20 (train)4.4s + (eval)0.7s: train loss = 0.04176 = (mf)0.02341 + (embed)0.01835, recall = 0.30714, ndcg = 0.28792\n",
            "Epoch 21 (train)3.6s: train loss = 0.03988 = (mf)0.02128 + (embed)0.01860\n",
            "Epoch 22 (train)3.5s: train loss = 0.04252 = (mf)0.02366 + (embed)0.01886\n",
            "Epoch 23 (train)4.4s: train loss = 0.04175 = (mf)0.02258 + (embed)0.01917\n",
            "Epoch 24 (train)3.5s: train loss = 0.04249 = (mf)0.02305 + (embed)0.01944\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 0.04068 = (mf)0.02104 + (embed)0.01964, recall = 0.29755, ndcg = 0.28301\n",
            "Epoch 26 (train)4.3s: train loss = 0.04171 = (mf)0.02180 + (embed)0.01990\n",
            "Epoch 27 (train)3.8s: train loss = 0.04142 = (mf)0.02126 + (embed)0.02017\n",
            "Epoch 28 (train)3.6s: train loss = 0.04159 = (mf)0.02122 + (embed)0.02037\n",
            "Epoch 29 (train)3.8s: train loss = 0.04132 = (mf)0.02070 + (embed)0.02062\n",
            "Epoch 30 (train)4.2s + (eval)0.3s: train loss = 0.04178 = (mf)0.02098 + (embed)0.02080, recall = 0.30059, ndcg = 0.28367\n",
            "Epoch 31 (train)3.6s: train loss = 0.04083 = (mf)0.01983 + (embed)0.02100\n",
            "Epoch 32 (train)3.5s: train loss = 0.04180 = (mf)0.02073 + (embed)0.02108\n",
            "Epoch 33 (train)4.4s: train loss = 0.04169 = (mf)0.02050 + (embed)0.02119\n",
            "Epoch 34 (train)3.5s: train loss = 0.04379 = (mf)0.02236 + (embed)0.02143\n",
            "Epoch 35 (train)3.5s + (eval)0.2s: train loss = 0.04020 = (mf)0.01861 + (embed)0.02160, recall = 0.29819, ndcg = 0.28319\n",
            "Epoch 36 (train)4.3s: train loss = 0.04098 = (mf)0.01926 + (embed)0.02171\n",
            "Epoch 37 (train)3.5s: train loss = 0.04240 = (mf)0.02059 + (embed)0.02181\n",
            "Epoch 38 (train)3.5s: train loss = 0.03947 = (mf)0.01753 + (embed)0.02194\n",
            "Epoch 39 (train)4.0s: train loss = 0.04027 = (mf)0.01815 + (embed)0.02211\n",
            "Epoch 40 (train)3.8s + (eval)0.7s: train loss = 0.04160 = (mf)0.01935 + (embed)0.02225, recall = 0.29230, ndcg = 0.27913\n",
            "Epoch 41 (train)3.5s: train loss = 0.04181 = (mf)0.01930 + (embed)0.02251\n",
            "Epoch 42 (train)3.8s: train loss = 0.04252 = (mf)0.01993 + (embed)0.02259\n",
            "Epoch 43 (train)4.1s: train loss = 0.04116 = (mf)0.01837 + (embed)0.02279\n",
            "Epoch 44 (train)3.5s: train loss = 0.04121 = (mf)0.01830 + (embed)0.02291\n",
            "Epoch 45 (train)3.5s + (eval)0.3s: train loss = 0.04295 = (mf)0.01995 + (embed)0.02300, recall = 0.29508, ndcg = 0.27497\n",
            "Epoch 46 (train)4.4s: train loss = 0.04129 = (mf)0.01815 + (embed)0.02314\n",
            "Epoch 47 (train)3.5s: train loss = 0.04185 = (mf)0.01868 + (embed)0.02317\n",
            "Epoch 48 (train)3.5s: train loss = 0.04140 = (mf)0.01811 + (embed)0.02329\n",
            "Epoch 49 (train)4.4s: train loss = 0.04289 = (mf)0.01953 + (embed)0.02336\n",
            "Epoch 50 (train)3.5s + (eval)0.3s: train loss = 0.04151 = (mf)0.01807 + (embed)0.02344, recall = 0.29974, ndcg = 0.28321\n",
            "Epoch 51 (train)3.6s: train loss = 0.04266 = (mf)0.01917 + (embed)0.02350\n",
            "Epoch 52 (train)4.1s: train loss = 0.04256 = (mf)0.01895 + (embed)0.02361\n",
            "Epoch 53 (train)3.8s: train loss = 0.04116 = (mf)0.01744 + (embed)0.02373\n",
            "Epoch 54 (train)3.5s: train loss = 0.04178 = (mf)0.01803 + (embed)0.02375\n",
            "Epoch 55 (train)3.5s + (eval)0.3s: train loss = 0.04206 = (mf)0.01832 + (embed)0.02374, recall = 0.29919, ndcg = 0.28135\n",
            "Epoch 56 (train)4.3s: train loss = 0.04156 = (mf)0.01775 + (embed)0.02381\n",
            "Epoch 57 (train)3.5s: train loss = 0.04083 = (mf)0.01691 + (embed)0.02393\n",
            "Epoch 58 (train)3.6s: train loss = 0.04234 = (mf)0.01832 + (embed)0.02403\n",
            "Epoch 59 (train)4.4s: train loss = 0.04250 = (mf)0.01835 + (embed)0.02415\n",
            "Epoch 60 (train)3.6s + (eval)0.7s: train loss = 0.04060 = (mf)0.01641 + (embed)0.02419, recall = 0.29297, ndcg = 0.27783\n",
            "Epoch 61 (train)3.6s: train loss = 0.04167 = (mf)0.01756 + (embed)0.02412\n",
            "Epoch 62 (train)4.4s: train loss = 0.04091 = (mf)0.01669 + (embed)0.02422\n",
            "Epoch 63 (train)3.6s: train loss = 0.04319 = (mf)0.01892 + (embed)0.02427\n",
            "Epoch 64 (train)3.6s: train loss = 0.04201 = (mf)0.01773 + (embed)0.02428\n",
            "Epoch 65 (train)4.2s + (eval)0.4s: train loss = 0.04009 = (mf)0.01578 + (embed)0.02431, recall = 0.30019, ndcg = 0.27867\n",
            "Epoch 66 (train)3.6s: train loss = 0.04033 = (mf)0.01605 + (embed)0.02428\n",
            "Epoch 67 (train)3.5s: train loss = 0.04268 = (mf)0.01840 + (embed)0.02429\n",
            "Epoch 68 (train)3.8s: train loss = 0.04197 = (mf)0.01750 + (embed)0.02447\n",
            "Epoch 69 (train)4.2s: train loss = 0.04305 = (mf)0.01845 + (embed)0.02460\n",
            "Epoch 70 (train)3.6s + (eval)0.3s: train loss = 0.04221 = (mf)0.01756 + (embed)0.02465, recall = 0.29583, ndcg = 0.27761\n",
            "Epoch 71 (train)3.6s: train loss = 0.04228 = (mf)0.01768 + (embed)0.02460\n",
            "Epoch 72 (train)4.4s: train loss = 0.04425 = (mf)0.01956 + (embed)0.02468\n",
            "Epoch 73 (train)3.5s: train loss = 0.04225 = (mf)0.01749 + (embed)0.02476\n",
            "Epoch 74 (train)3.6s: train loss = 0.04221 = (mf)0.01745 + (embed)0.02477\n",
            "Epoch 75 (train)4.4s + (eval)0.2s: train loss = 0.04132 = (mf)0.01651 + (embed)0.02482, recall = 0.28963, ndcg = 0.26791\n",
            "Epoch 76 (train)3.5s: train loss = 0.04287 = (mf)0.01800 + (embed)0.02487\n",
            "Epoch 77 (train)3.5s: train loss = 0.04174 = (mf)0.01691 + (embed)0.02482\n",
            "Epoch 78 (train)4.1s: train loss = 0.04219 = (mf)0.01732 + (embed)0.02488\n",
            "Epoch 79 (train)3.8s: train loss = 0.04056 = (mf)0.01569 + (embed)0.02487\n",
            "Epoch 80 (train)3.5s + (eval)0.7s: train loss = 0.04187 = (mf)0.01710 + (embed)0.02476, recall = 0.29525, ndcg = 0.27750\n",
            "Epoch 81 (train)3.9s: train loss = 0.04234 = (mf)0.01748 + (embed)0.02485\n",
            "Epoch 82 (train)4.0s: train loss = 0.04209 = (mf)0.01715 + (embed)0.02493\n",
            "Epoch 83 (train)3.5s: train loss = 0.04248 = (mf)0.01756 + (embed)0.02493\n",
            "Epoch 84 (train)3.6s: train loss = 0.04057 = (mf)0.01568 + (embed)0.02489\n",
            "Epoch 85 (train)4.4s + (eval)0.2s: train loss = 0.04054 = (mf)0.01568 + (embed)0.02486, recall = 0.29755, ndcg = 0.27978\n",
            "Epoch 86 (train)3.5s: train loss = 0.03987 = (mf)0.01505 + (embed)0.02482\n",
            "Epoch 87 (train)3.5s: train loss = 0.04094 = (mf)0.01624 + (embed)0.02470\n",
            "Epoch 88 (train)4.3s: train loss = 0.04152 = (mf)0.01681 + (embed)0.02471\n",
            "Epoch 89 (train)3.5s: train loss = 0.04167 = (mf)0.01693 + (embed)0.02475\n",
            "Epoch 90 (train)3.5s + (eval)0.3s: train loss = 0.04289 = (mf)0.01810 + (embed)0.02479, recall = 0.29077, ndcg = 0.26705\n",
            "Epoch 91 (train)4.2s: train loss = 0.04103 = (mf)0.01623 + (embed)0.02480\n",
            "Epoch 92 (train)3.7s: train loss = 0.04100 = (mf)0.01623 + (embed)0.02477\n",
            "Epoch 93 (train)3.5s: train loss = 0.04248 = (mf)0.01769 + (embed)0.02479\n",
            "Epoch 94 (train)3.7s: train loss = 0.04139 = (mf)0.01637 + (embed)0.02502\n",
            "Epoch 95 (train)4.3s + (eval)0.3s: train loss = 0.04244 = (mf)0.01752 + (embed)0.02492, recall = 0.28987, ndcg = 0.27080\n",
            "Epoch 96 (train)3.5s: train loss = 0.04056 = (mf)0.01555 + (embed)0.02501\n",
            "Epoch 97 (train)3.6s: train loss = 0.04169 = (mf)0.01669 + (embed)0.02500\n",
            "Epoch 98 (train)4.4s: train loss = 0.04213 = (mf)0.01707 + (embed)0.02506\n",
            "Epoch 99 (train)3.5s: train loss = 0.04146 = (mf)0.01640 + (embed)0.02506\n",
            "Epoch 100 (train)3.5s + (eval)0.7s: train loss = 0.04217 = (mf)0.01700 + (embed)0.02517, recall = 0.29327, ndcg = 0.27478\n",
            "Training finished in 391.04 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.27478, Recall@50: 0.29327\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.05, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.7s: train loss = 0.24657 = (mf)0.24503 + (embed)0.00154\n",
            "Epoch 2 (train)4.1s: train loss = 0.13835 = (mf)0.13446 + (embed)0.00390\n",
            "Epoch 3 (train)4.1s: train loss = 0.10591 = (mf)0.10000 + (embed)0.00592\n",
            "Epoch 4 (train)4.9s: train loss = 0.09062 = (mf)0.08286 + (embed)0.00775\n",
            "Epoch 5 (train)4.2s + (eval)1.2s: train loss = 0.07802 = (mf)0.06855 + (embed)0.00947, recall = 0.29727, ndcg = 0.29068\n",
            "Epoch 6 (train)4.6s: train loss = 0.06687 = (mf)0.05584 + (embed)0.01104\n",
            "Epoch 7 (train)4.9s: train loss = 0.06280 = (mf)0.05052 + (embed)0.01228\n",
            "Epoch 8 (train)4.3s: train loss = 0.05646 = (mf)0.04305 + (embed)0.01341\n",
            "Epoch 9 (train)5.0s: train loss = 0.05472 = (mf)0.04039 + (embed)0.01433\n",
            "Epoch 10 (train)4.2s + (eval)0.2s: train loss = 0.05413 = (mf)0.03904 + (embed)0.01509, recall = 0.32058, ndcg = 0.30682\n",
            "Epoch 11 (train)4.2s: train loss = 0.05074 = (mf)0.03495 + (embed)0.01579\n",
            "Epoch 12 (train)5.0s: train loss = 0.05138 = (mf)0.03499 + (embed)0.01639\n",
            "Epoch 13 (train)4.2s: train loss = 0.04645 = (mf)0.02949 + (embed)0.01696\n",
            "Epoch 14 (train)4.2s: train loss = 0.04861 = (mf)0.03130 + (embed)0.01732\n",
            "Epoch 15 (train)5.0s + (eval)0.3s: train loss = 0.04571 = (mf)0.02807 + (embed)0.01764, recall = 0.32286, ndcg = 0.30974\n",
            "Epoch 16 (train)4.2s: train loss = 0.04305 = (mf)0.02506 + (embed)0.01799\n",
            "Epoch 17 (train)4.3s: train loss = 0.04518 = (mf)0.02684 + (embed)0.01835\n",
            "Epoch 18 (train)4.9s: train loss = 0.04442 = (mf)0.02575 + (embed)0.01867\n",
            "Epoch 19 (train)4.3s: train loss = 0.04340 = (mf)0.02448 + (embed)0.01892\n",
            "Epoch 20 (train)4.9s + (eval)0.8s: train loss = 0.04352 = (mf)0.02433 + (embed)0.01919, recall = 0.32182, ndcg = 0.30814\n",
            "Epoch 21 (train)4.2s: train loss = 0.04284 = (mf)0.02339 + (embed)0.01945\n",
            "Epoch 22 (train)4.1s: train loss = 0.04218 = (mf)0.02250 + (embed)0.01969\n",
            "Epoch 23 (train)4.9s: train loss = 0.04157 = (mf)0.02161 + (embed)0.01996\n",
            "Epoch 24 (train)4.1s: train loss = 0.04114 = (mf)0.02104 + (embed)0.02011\n",
            "Epoch 25 (train)4.1s + (eval)0.2s: train loss = 0.04339 = (mf)0.02305 + (embed)0.02034, recall = 0.31573, ndcg = 0.29855\n",
            "Epoch 26 (train)4.9s: train loss = 0.04083 = (mf)0.02025 + (embed)0.02058\n",
            "Epoch 27 (train)4.2s: train loss = 0.04069 = (mf)0.02003 + (embed)0.02066\n",
            "Epoch 28 (train)4.3s: train loss = 0.03959 = (mf)0.01873 + (embed)0.02086\n",
            "Epoch 29 (train)4.9s: train loss = 0.04010 = (mf)0.01916 + (embed)0.02095\n",
            "Epoch 30 (train)4.1s + (eval)0.3s: train loss = 0.04127 = (mf)0.02025 + (embed)0.02102, recall = 0.32140, ndcg = 0.30726\n",
            "Epoch 31 (train)4.7s: train loss = 0.04009 = (mf)0.01903 + (embed)0.02106\n",
            "Epoch 32 (train)4.5s: train loss = 0.04010 = (mf)0.01887 + (embed)0.02123\n",
            "Epoch 33 (train)4.1s: train loss = 0.04031 = (mf)0.01898 + (embed)0.02133\n",
            "Epoch 34 (train)4.9s: train loss = 0.04075 = (mf)0.01924 + (embed)0.02151\n",
            "Epoch 35 (train)4.2s + (eval)0.2s: train loss = 0.04064 = (mf)0.01907 + (embed)0.02157, recall = 0.32147, ndcg = 0.30624\n",
            "Epoch 36 (train)4.1s: train loss = 0.04069 = (mf)0.01893 + (embed)0.02176\n",
            "Epoch 37 (train)4.9s: train loss = 0.04108 = (mf)0.01906 + (embed)0.02202\n",
            "Epoch 38 (train)4.1s: train loss = 0.03898 = (mf)0.01685 + (embed)0.02213\n",
            "Epoch 39 (train)4.1s: train loss = 0.04013 = (mf)0.01796 + (embed)0.02217\n",
            "Epoch 40 (train)4.9s + (eval)0.7s: train loss = 0.04012 = (mf)0.01790 + (embed)0.02222, recall = 0.31707, ndcg = 0.30308\n",
            "Epoch 41 (train)4.2s: train loss = 0.03963 = (mf)0.01734 + (embed)0.02229\n",
            "Epoch 42 (train)4.4s: train loss = 0.03837 = (mf)0.01600 + (embed)0.02237\n",
            "Epoch 43 (train)4.7s: train loss = 0.04022 = (mf)0.01785 + (embed)0.02237\n",
            "Epoch 44 (train)4.1s: train loss = 0.03898 = (mf)0.01647 + (embed)0.02251\n",
            "Epoch 45 (train)4.6s + (eval)0.4s: train loss = 0.04093 = (mf)0.01831 + (embed)0.02262, recall = 0.32020, ndcg = 0.30191\n",
            "Epoch 46 (train)4.4s: train loss = 0.03992 = (mf)0.01713 + (embed)0.02279\n",
            "Epoch 47 (train)4.1s: train loss = 0.03999 = (mf)0.01710 + (embed)0.02289\n",
            "Epoch 48 (train)5.0s: train loss = 0.03950 = (mf)0.01652 + (embed)0.02298\n",
            "Epoch 49 (train)4.1s: train loss = 0.03973 = (mf)0.01675 + (embed)0.02297\n",
            "Epoch 50 (train)4.1s + (eval)0.2s: train loss = 0.04001 = (mf)0.01692 + (embed)0.02309, recall = 0.31254, ndcg = 0.28883\n",
            "Epoch 51 (train)5.0s: train loss = 0.03923 = (mf)0.01604 + (embed)0.02319\n",
            "Epoch 52 (train)4.1s: train loss = 0.03909 = (mf)0.01596 + (embed)0.02313\n",
            "Epoch 53 (train)4.2s: train loss = 0.03704 = (mf)0.01389 + (embed)0.02315\n",
            "Epoch 54 (train)5.1s: train loss = 0.03962 = (mf)0.01643 + (embed)0.02319\n",
            "Epoch 55 (train)4.2s + (eval)0.3s: train loss = 0.03835 = (mf)0.01512 + (embed)0.02323, recall = 0.31670, ndcg = 0.29892\n",
            "Epoch 56 (train)4.4s: train loss = 0.03830 = (mf)0.01505 + (embed)0.02325\n",
            "Epoch 57 (train)4.8s: train loss = 0.03949 = (mf)0.01616 + (embed)0.02333\n",
            "Epoch 58 (train)4.2s: train loss = 0.03897 = (mf)0.01554 + (embed)0.02343\n",
            "Epoch 59 (train)4.8s: train loss = 0.03812 = (mf)0.01480 + (embed)0.02332\n",
            "Epoch 60 (train)4.3s + (eval)0.7s: train loss = 0.03872 = (mf)0.01532 + (embed)0.02340, recall = 0.31951, ndcg = 0.30108\n",
            "Epoch 61 (train)4.2s: train loss = 0.03956 = (mf)0.01613 + (embed)0.02343\n",
            "Epoch 62 (train)4.9s: train loss = 0.03914 = (mf)0.01576 + (embed)0.02337\n",
            "Epoch 63 (train)4.1s: train loss = 0.03926 = (mf)0.01579 + (embed)0.02347\n",
            "Epoch 64 (train)4.1s: train loss = 0.03883 = (mf)0.01530 + (embed)0.02353\n",
            "Epoch 65 (train)5.0s + (eval)0.2s: train loss = 0.04012 = (mf)0.01646 + (embed)0.02366, recall = 0.31894, ndcg = 0.30394\n",
            "Epoch 66 (train)4.3s: train loss = 0.03872 = (mf)0.01497 + (embed)0.02375\n",
            "Epoch 67 (train)4.3s: train loss = 0.03837 = (mf)0.01453 + (embed)0.02385\n",
            "Epoch 68 (train)4.8s: train loss = 0.03911 = (mf)0.01520 + (embed)0.02391\n",
            "Epoch 69 (train)4.1s: train loss = 0.03963 = (mf)0.01573 + (embed)0.02390\n",
            "Epoch 70 (train)4.5s + (eval)0.4s: train loss = 0.03923 = (mf)0.01536 + (embed)0.02388, recall = 0.31446, ndcg = 0.29500\n",
            "Epoch 71 (train)4.5s: train loss = 0.03913 = (mf)0.01515 + (embed)0.02398\n",
            "Epoch 72 (train)4.1s: train loss = 0.03898 = (mf)0.01500 + (embed)0.02398\n",
            "Epoch 73 (train)4.9s: train loss = 0.03916 = (mf)0.01512 + (embed)0.02403\n",
            "Epoch 74 (train)4.2s: train loss = 0.03862 = (mf)0.01464 + (embed)0.02398\n",
            "Epoch 75 (train)4.2s + (eval)0.2s: train loss = 0.03914 = (mf)0.01513 + (embed)0.02401, recall = 0.31710, ndcg = 0.29799\n",
            "Epoch 76 (train)4.9s: train loss = 0.04023 = (mf)0.01617 + (embed)0.02406\n",
            "Epoch 77 (train)4.2s: train loss = 0.03847 = (mf)0.01435 + (embed)0.02412\n",
            "Epoch 78 (train)4.2s: train loss = 0.03960 = (mf)0.01553 + (embed)0.02406\n",
            "Epoch 79 (train)4.9s: train loss = 0.03913 = (mf)0.01502 + (embed)0.02410\n",
            "Epoch 80 (train)4.2s + (eval)0.7s: train loss = 0.03898 = (mf)0.01486 + (embed)0.02412, recall = 0.31207, ndcg = 0.29261\n",
            "Epoch 81 (train)4.6s: train loss = 0.03967 = (mf)0.01557 + (embed)0.02410\n",
            "Epoch 82 (train)4.7s: train loss = 0.03984 = (mf)0.01566 + (embed)0.02418\n",
            "Epoch 83 (train)4.1s: train loss = 0.03928 = (mf)0.01506 + (embed)0.02422\n",
            "Epoch 84 (train)4.7s: train loss = 0.03876 = (mf)0.01452 + (embed)0.02424\n",
            "Epoch 85 (train)4.4s + (eval)0.2s: train loss = 0.03968 = (mf)0.01544 + (embed)0.02424, recall = 0.31640, ndcg = 0.29393\n",
            "Epoch 86 (train)4.1s: train loss = 0.03939 = (mf)0.01502 + (embed)0.02437\n",
            "Epoch 87 (train)5.0s: train loss = 0.03912 = (mf)0.01475 + (embed)0.02437\n",
            "Epoch 88 (train)4.1s: train loss = 0.03935 = (mf)0.01491 + (embed)0.02444\n",
            "Epoch 89 (train)4.2s: train loss = 0.04040 = (mf)0.01601 + (embed)0.02439\n",
            "Epoch 90 (train)4.9s + (eval)0.2s: train loss = 0.03939 = (mf)0.01490 + (embed)0.02448, recall = 0.32035, ndcg = 0.30351\n",
            "Epoch 91 (train)4.2s: train loss = 0.03943 = (mf)0.01492 + (embed)0.02451\n",
            "Epoch 92 (train)4.2s: train loss = 0.03890 = (mf)0.01439 + (embed)0.02451\n",
            "Epoch 93 (train)5.0s: train loss = 0.03943 = (mf)0.01488 + (embed)0.02454\n",
            "Epoch 94 (train)4.1s: train loss = 0.03845 = (mf)0.01397 + (embed)0.02448\n",
            "Epoch 95 (train)4.4s + (eval)0.4s: train loss = 0.04032 = (mf)0.01580 + (embed)0.02452, recall = 0.31313, ndcg = 0.29940\n",
            "Epoch 96 (train)4.6s: train loss = 0.04019 = (mf)0.01558 + (embed)0.02461\n",
            "Epoch 97 (train)4.2s: train loss = 0.03997 = (mf)0.01527 + (embed)0.02471\n",
            "Epoch 98 (train)4.9s: train loss = 0.03883 = (mf)0.01412 + (embed)0.02471\n",
            "Epoch 99 (train)4.3s: train loss = 0.03912 = (mf)0.01443 + (embed)0.02469\n",
            "Epoch 100 (train)4.2s + (eval)0.7s: train loss = 0.04039 = (mf)0.01576 + (embed)0.02463, recall = 0.30995, ndcg = 0.29452\n",
            "Training finished in 455.51 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.29452, Recall@50: 0.30995\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.5, Layers=3 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)5.6s: train loss = 0.61815 = (mf)0.55544 + (embed)0.06271\n",
            "Epoch 2 (train)3.1s: train loss = 1.27097 = (mf)1.07089 + (embed)0.20008\n",
            "Epoch 3 (train)3.0s: train loss = 1.71635 = (mf)1.38105 + (embed)0.33529\n",
            "Epoch 4 (train)3.0s: train loss = 2.08994 = (mf)1.63062 + (embed)0.45933\n",
            "Epoch 5 (train)3.9s + (eval)1.2s: train loss = 2.48358 = (mf)1.90470 + (embed)0.57888, recall = 0.19464, ndcg = 0.18865\n",
            "Epoch 6 (train)3.0s: train loss = 2.75855 = (mf)2.07319 + (embed)0.68535\n",
            "Epoch 7 (train)3.0s: train loss = 2.95096 = (mf)2.16832 + (embed)0.78263\n",
            "Epoch 8 (train)3.2s: train loss = 3.12599 = (mf)2.24821 + (embed)0.87778\n",
            "Epoch 9 (train)3.6s: train loss = 3.43500 = (mf)2.47837 + (embed)0.95663\n",
            "Epoch 10 (train)3.0s + (eval)0.3s: train loss = 3.60559 = (mf)2.56806 + (embed)1.03753, recall = 0.18229, ndcg = 0.17405\n",
            "Epoch 11 (train)3.0s: train loss = 3.85322 = (mf)2.73803 + (embed)1.11519\n",
            "Epoch 12 (train)3.4s: train loss = 3.84009 = (mf)2.65170 + (embed)1.18838\n",
            "Epoch 13 (train)3.5s: train loss = 4.17050 = (mf)2.91637 + (embed)1.25413\n",
            "Epoch 14 (train)3.1s: train loss = 4.12238 = (mf)2.80493 + (embed)1.31745\n",
            "Epoch 15 (train)3.0s + (eval)0.7s: train loss = 4.25358 = (mf)2.88334 + (embed)1.37024, recall = 0.18727, ndcg = 0.17286\n",
            "Epoch 16 (train)4.0s: train loss = 4.19657 = (mf)2.77752 + (embed)1.41905\n",
            "Epoch 17 (train)3.1s: train loss = 4.45937 = (mf)2.99575 + (embed)1.46361\n",
            "Epoch 18 (train)3.0s: train loss = 4.07810 = (mf)2.56570 + (embed)1.51240\n",
            "Epoch 19 (train)3.0s: train loss = 4.20086 = (mf)2.64013 + (embed)1.56073\n",
            "Epoch 20 (train)3.8s + (eval)0.2s: train loss = 4.61019 = (mf)2.99849 + (embed)1.61170, recall = 0.21447, ndcg = 0.20113\n",
            "Epoch 21 (train)3.0s: train loss = 4.43416 = (mf)2.78184 + (embed)1.65232\n",
            "Epoch 22 (train)3.0s: train loss = 4.77277 = (mf)3.07729 + (embed)1.69549\n",
            "Epoch 23 (train)3.0s: train loss = 4.74161 = (mf)3.01501 + (embed)1.72660\n",
            "Epoch 24 (train)3.8s: train loss = 4.80276 = (mf)3.03464 + (embed)1.76812\n",
            "Epoch 25 (train)3.0s + (eval)0.3s: train loss = 4.66634 = (mf)2.85592 + (embed)1.81042, recall = 0.21280, ndcg = 0.19871\n",
            "Epoch 26 (train)2.9s: train loss = 4.79626 = (mf)2.95725 + (embed)1.83900\n",
            "Epoch 27 (train)3.0s: train loss = 4.59486 = (mf)2.72972 + (embed)1.86514\n",
            "Epoch 28 (train)3.8s: train loss = 4.73742 = (mf)2.83915 + (embed)1.89827\n",
            "Epoch 29 (train)3.0s: train loss = 4.79251 = (mf)2.86252 + (embed)1.92999\n",
            "Epoch 30 (train)3.0s + (eval)0.2s: train loss = 5.06578 = (mf)3.11509 + (embed)1.95069, recall = 0.21299, ndcg = 0.20483\n",
            "Epoch 31 (train)3.3s: train loss = 4.78132 = (mf)2.80290 + (embed)1.97841\n",
            "Epoch 32 (train)3.6s: train loss = 4.85727 = (mf)2.85503 + (embed)2.00224\n",
            "Epoch 33 (train)3.1s: train loss = 4.75872 = (mf)2.73209 + (embed)2.02663\n",
            "Epoch 34 (train)3.1s: train loss = 4.83742 = (mf)2.78328 + (embed)2.05414\n",
            "Epoch 35 (train)3.4s + (eval)0.9s: train loss = 4.78891 = (mf)2.70843 + (embed)2.08048, recall = 0.21345, ndcg = 0.20298\n",
            "Epoch 36 (train)3.2s: train loss = 4.72371 = (mf)2.61552 + (embed)2.10819\n",
            "Epoch 37 (train)3.0s: train loss = 5.26109 = (mf)3.13610 + (embed)2.12499\n",
            "Epoch 38 (train)3.0s: train loss = 4.96403 = (mf)2.83078 + (embed)2.13325\n",
            "Epoch 39 (train)3.8s: train loss = 4.84233 = (mf)2.69552 + (embed)2.14681\n",
            "Epoch 40 (train)3.0s + (eval)0.2s: train loss = 4.73792 = (mf)2.56424 + (embed)2.17368, recall = 0.20396, ndcg = 0.18840\n",
            "Epoch 41 (train)3.0s: train loss = 4.90594 = (mf)2.71151 + (embed)2.19444\n",
            "Epoch 42 (train)3.0s: train loss = 5.07945 = (mf)2.86820 + (embed)2.21125\n",
            "Epoch 43 (train)3.8s: train loss = 5.19232 = (mf)2.95774 + (embed)2.23459\n",
            "Epoch 44 (train)3.0s: train loss = 4.93920 = (mf)2.68448 + (embed)2.25473\n",
            "Epoch 45 (train)3.0s + (eval)0.3s: train loss = 4.87367 = (mf)2.59710 + (embed)2.27656, recall = 0.20755, ndcg = 0.19236\n",
            "Epoch 46 (train)3.1s: train loss = 5.08742 = (mf)2.80012 + (embed)2.28731\n",
            "Epoch 47 (train)3.9s: train loss = 4.92974 = (mf)2.62216 + (embed)2.30759\n",
            "Epoch 48 (train)3.1s: train loss = 5.26493 = (mf)2.94682 + (embed)2.31811\n",
            "Epoch 49 (train)3.1s: train loss = 5.14987 = (mf)2.81264 + (embed)2.33723\n",
            "Epoch 50 (train)3.1s + (eval)0.4s: train loss = 4.94484 = (mf)2.58847 + (embed)2.35637, recall = 0.22318, ndcg = 0.20639\n",
            "Epoch 51 (train)3.7s: train loss = 5.00171 = (mf)2.63489 + (embed)2.36682\n",
            "Epoch 52 (train)3.0s: train loss = 5.28512 = (mf)2.89667 + (embed)2.38844\n",
            "Epoch 53 (train)3.0s: train loss = 5.09991 = (mf)2.69467 + (embed)2.40524\n",
            "Epoch 54 (train)3.2s: train loss = 5.07684 = (mf)2.65755 + (embed)2.41930\n",
            "Epoch 55 (train)3.6s + (eval)0.7s: train loss = 5.52107 = (mf)3.08750 + (embed)2.43357, recall = 0.19814, ndcg = 0.18149\n",
            "Epoch 56 (train)3.0s: train loss = 5.10880 = (mf)2.65777 + (embed)2.45102\n",
            "Epoch 57 (train)3.0s: train loss = 5.30235 = (mf)2.84352 + (embed)2.45884\n",
            "Epoch 58 (train)3.7s: train loss = 5.13308 = (mf)2.65221 + (embed)2.48087\n",
            "Epoch 59 (train)3.3s: train loss = 5.49400 = (mf)2.99692 + (embed)2.49708\n",
            "Epoch 60 (train)3.0s + (eval)0.2s: train loss = 5.33434 = (mf)2.81917 + (embed)2.51517, recall = 0.21009, ndcg = 0.19311\n",
            "Epoch 61 (train)3.1s: train loss = 5.16990 = (mf)2.64891 + (embed)2.52099\n",
            "Epoch 62 (train)4.0s: train loss = 5.34928 = (mf)2.81124 + (embed)2.53804\n",
            "Epoch 63 (train)3.1s: train loss = 5.60799 = (mf)3.05655 + (embed)2.55144\n",
            "Epoch 64 (train)3.0s: train loss = 5.32764 = (mf)2.76672 + (embed)2.56092\n",
            "Epoch 65 (train)3.0s + (eval)0.3s: train loss = 5.26520 = (mf)2.69170 + (embed)2.57350, recall = 0.20410, ndcg = 0.19005\n",
            "Epoch 66 (train)3.9s: train loss = 5.45538 = (mf)2.86944 + (embed)2.58594\n",
            "Epoch 67 (train)3.0s: train loss = 5.58894 = (mf)2.99661 + (embed)2.59233\n",
            "Epoch 68 (train)3.0s: train loss = 5.53232 = (mf)2.92939 + (embed)2.60293\n",
            "Epoch 69 (train)3.1s: train loss = 5.44969 = (mf)2.82357 + (embed)2.62612\n",
            "Epoch 70 (train)3.8s + (eval)0.2s: train loss = 5.44831 = (mf)2.81244 + (embed)2.63586, recall = 0.21523, ndcg = 0.19172\n",
            "Epoch 71 (train)3.0s: train loss = 5.25234 = (mf)2.61909 + (embed)2.63325\n",
            "Epoch 72 (train)2.9s: train loss = 5.45638 = (mf)2.81324 + (embed)2.64314\n",
            "Epoch 73 (train)3.2s: train loss = 5.36322 = (mf)2.71036 + (embed)2.65286\n",
            "Epoch 74 (train)3.6s: train loss = 5.42815 = (mf)2.76953 + (embed)2.65863\n",
            "Epoch 75 (train)3.0s + (eval)0.7s: train loss = 5.69949 = (mf)3.03176 + (embed)2.66772, recall = 0.21770, ndcg = 0.19532\n",
            "Epoch 76 (train)3.0s: train loss = 5.52873 = (mf)2.85306 + (embed)2.67567\n",
            "Epoch 77 (train)3.7s: train loss = 5.47668 = (mf)2.78381 + (embed)2.69286\n",
            "Epoch 78 (train)3.2s: train loss = 5.50953 = (mf)2.80308 + (embed)2.70644\n",
            "Epoch 79 (train)2.9s: train loss = 5.31694 = (mf)2.59781 + (embed)2.71913\n",
            "Epoch 80 (train)3.0s + (eval)0.2s: train loss = 5.47793 = (mf)2.75595 + (embed)2.72198, recall = 0.21710, ndcg = 0.19413\n",
            "Epoch 81 (train)3.9s: train loss = 5.38946 = (mf)2.66544 + (embed)2.72402\n",
            "Epoch 82 (train)3.0s: train loss = 5.65838 = (mf)2.92366 + (embed)2.73472\n",
            "Epoch 83 (train)3.0s: train loss = 5.46472 = (mf)2.71519 + (embed)2.74953\n",
            "Epoch 84 (train)3.0s: train loss = 5.46572 = (mf)2.70219 + (embed)2.76354\n",
            "Epoch 85 (train)3.8s + (eval)0.2s: train loss = 5.35829 = (mf)2.58546 + (embed)2.77283, recall = 0.20917, ndcg = 0.19016\n",
            "Epoch 86 (train)2.9s: train loss = 5.40223 = (mf)2.62401 + (embed)2.77822\n",
            "Epoch 87 (train)3.0s: train loss = 5.58188 = (mf)2.80102 + (embed)2.78086\n",
            "Epoch 88 (train)3.0s: train loss = 5.33217 = (mf)2.54466 + (embed)2.78752\n",
            "Epoch 89 (train)3.8s: train loss = 5.58998 = (mf)2.79774 + (embed)2.79224\n",
            "Epoch 90 (train)2.9s + (eval)0.2s: train loss = 5.40067 = (mf)2.59635 + (embed)2.80432, recall = 0.20068, ndcg = 0.18426\n",
            "Epoch 91 (train)2.9s: train loss = 5.40921 = (mf)2.60296 + (embed)2.80625\n",
            "Epoch 92 (train)3.0s: train loss = 5.75122 = (mf)2.94285 + (embed)2.80837\n",
            "Epoch 93 (train)3.9s: train loss = 5.37340 = (mf)2.55678 + (embed)2.81663\n",
            "Epoch 94 (train)3.0s: train loss = 5.62902 = (mf)2.80989 + (embed)2.81913\n",
            "Epoch 95 (train)3.0s + (eval)0.7s: train loss = 5.33673 = (mf)2.51326 + (embed)2.82346, recall = 0.19897, ndcg = 0.17607\n",
            "Epoch 96 (train)3.1s: train loss = 5.33053 = (mf)2.49612 + (embed)2.83441\n",
            "Epoch 97 (train)3.6s: train loss = 5.15226 = (mf)2.31918 + (embed)2.83308\n",
            "Epoch 98 (train)3.0s: train loss = 5.42611 = (mf)2.57539 + (embed)2.85072\n",
            "Epoch 99 (train)3.0s: train loss = 5.69236 = (mf)2.83166 + (embed)2.86070\n",
            "Epoch 100 (train)3.3s + (eval)0.4s: train loss = 5.69078 = (mf)2.82376 + (embed)2.86702, recall = 0.20925, ndcg = 0.19274\n",
            "Training finished in 332.38 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19274, Recall@50: 0.20925\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.5, Layers=4 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.1s: train loss = 0.41780 = (mf)0.36514 + (embed)0.05267\n",
            "Epoch 2 (train)3.6s: train loss = 0.83182 = (mf)0.65855 + (embed)0.17327\n",
            "Epoch 3 (train)3.6s: train loss = 1.25522 = (mf)0.95037 + (embed)0.30485\n",
            "Epoch 4 (train)4.4s: train loss = 1.49422 = (mf)1.07397 + (embed)0.42025\n",
            "Epoch 5 (train)3.6s + (eval)1.2s: train loss = 1.74088 = (mf)1.22217 + (embed)0.51870, recall = 0.19291, ndcg = 0.18936\n",
            "Epoch 6 (train)3.6s: train loss = 1.80695 = (mf)1.20176 + (embed)0.60519\n",
            "Epoch 7 (train)4.4s: train loss = 1.92434 = (mf)1.24564 + (embed)0.67870\n",
            "Epoch 8 (train)3.6s: train loss = 2.13647 = (mf)1.38962 + (embed)0.74685\n",
            "Epoch 9 (train)3.6s: train loss = 2.21596 = (mf)1.39903 + (embed)0.81693\n",
            "Epoch 10 (train)4.5s + (eval)0.7s: train loss = 2.31611 = (mf)1.43535 + (embed)0.88076, recall = 0.18837, ndcg = 0.17625\n",
            "Epoch 11 (train)3.6s: train loss = 2.41335 = (mf)1.47782 + (embed)0.93553\n",
            "Epoch 12 (train)3.6s: train loss = 2.44463 = (mf)1.45641 + (embed)0.98822\n",
            "Epoch 13 (train)4.4s: train loss = 2.55852 = (mf)1.52388 + (embed)1.03464\n",
            "Epoch 14 (train)3.7s: train loss = 2.62202 = (mf)1.55190 + (embed)1.07012\n",
            "Epoch 15 (train)3.6s + (eval)0.3s: train loss = 2.67708 = (mf)1.56659 + (embed)1.11049, recall = 0.20791, ndcg = 0.19655\n",
            "Epoch 16 (train)4.3s: train loss = 2.63934 = (mf)1.48871 + (embed)1.15063\n",
            "Epoch 17 (train)3.7s: train loss = 2.89006 = (mf)1.70330 + (embed)1.18676\n",
            "Epoch 18 (train)3.7s: train loss = 2.90318 = (mf)1.68204 + (embed)1.22114\n",
            "Epoch 19 (train)3.9s: train loss = 2.78278 = (mf)1.52711 + (embed)1.25567\n",
            "Epoch 20 (train)4.2s + (eval)0.3s: train loss = 2.90726 = (mf)1.61879 + (embed)1.28847, recall = 0.19771, ndcg = 0.18302\n",
            "Epoch 21 (train)3.7s: train loss = 3.04056 = (mf)1.72014 + (embed)1.32042\n",
            "Epoch 22 (train)3.9s: train loss = 2.87595 = (mf)1.52272 + (embed)1.35323\n",
            "Epoch 23 (train)4.3s: train loss = 3.11838 = (mf)1.73270 + (embed)1.38568\n",
            "Epoch 24 (train)3.6s: train loss = 3.12538 = (mf)1.71547 + (embed)1.40991\n",
            "Epoch 25 (train)3.5s + (eval)0.2s: train loss = 3.01496 = (mf)1.57797 + (embed)1.43699, recall = 0.18619, ndcg = 0.17235\n",
            "Epoch 26 (train)4.5s: train loss = 3.12908 = (mf)1.66665 + (embed)1.46243\n",
            "Epoch 27 (train)3.6s: train loss = 3.19409 = (mf)1.70843 + (embed)1.48565\n",
            "Epoch 28 (train)3.6s: train loss = 3.04360 = (mf)1.53496 + (embed)1.50864\n",
            "Epoch 29 (train)4.4s: train loss = 3.20095 = (mf)1.66924 + (embed)1.53171\n",
            "Epoch 30 (train)3.6s + (eval)0.7s: train loss = 3.23164 = (mf)1.67987 + (embed)1.55177, recall = 0.20873, ndcg = 0.19534\n",
            "Epoch 31 (train)3.6s: train loss = 3.22673 = (mf)1.65818 + (embed)1.56855\n",
            "Epoch 32 (train)4.5s: train loss = 3.12476 = (mf)1.54518 + (embed)1.57958\n",
            "Epoch 33 (train)3.6s: train loss = 3.37102 = (mf)1.77390 + (embed)1.59712\n",
            "Epoch 34 (train)3.6s: train loss = 3.25225 = (mf)1.64479 + (embed)1.60746\n",
            "Epoch 35 (train)4.1s + (eval)0.4s: train loss = 3.16283 = (mf)1.55123 + (embed)1.61160, recall = 0.20845, ndcg = 0.19064\n",
            "Epoch 36 (train)3.8s: train loss = 3.46852 = (mf)1.83655 + (embed)1.63197\n",
            "Epoch 37 (train)3.5s: train loss = 3.26385 = (mf)1.61888 + (embed)1.64496\n",
            "Epoch 38 (train)3.6s: train loss = 3.29692 = (mf)1.63359 + (embed)1.66332\n",
            "Epoch 39 (train)4.3s: train loss = 3.25372 = (mf)1.58313 + (embed)1.67058\n",
            "Epoch 40 (train)3.5s + (eval)0.2s: train loss = 3.16352 = (mf)1.48107 + (embed)1.68245, recall = 0.20832, ndcg = 0.19579\n",
            "Epoch 41 (train)3.5s: train loss = 3.30276 = (mf)1.61367 + (embed)1.68910\n",
            "Epoch 42 (train)4.3s: train loss = 3.20349 = (mf)1.50489 + (embed)1.69860\n",
            "Epoch 43 (train)3.5s: train loss = 3.20238 = (mf)1.49683 + (embed)1.70556\n",
            "Epoch 44 (train)3.5s: train loss = 3.19629 = (mf)1.47489 + (embed)1.72140\n",
            "Epoch 45 (train)4.4s + (eval)0.3s: train loss = 3.09084 = (mf)1.36201 + (embed)1.72883, recall = 0.21071, ndcg = 0.20093\n",
            "Epoch 46 (train)3.6s: train loss = 3.31474 = (mf)1.58128 + (embed)1.73346\n",
            "Epoch 47 (train)3.6s: train loss = 3.33425 = (mf)1.59079 + (embed)1.74345\n",
            "Epoch 48 (train)4.1s: train loss = 3.19434 = (mf)1.43532 + (embed)1.75903\n",
            "Epoch 49 (train)4.0s: train loss = 3.19426 = (mf)1.43015 + (embed)1.76410\n",
            "Epoch 50 (train)3.6s + (eval)0.7s: train loss = 3.22883 = (mf)1.46292 + (embed)1.76590, recall = 0.20434, ndcg = 0.18731\n",
            "Epoch 51 (train)4.3s: train loss = 3.26695 = (mf)1.49219 + (embed)1.77477\n",
            "Epoch 52 (train)4.0s: train loss = 3.33950 = (mf)1.55670 + (embed)1.78280\n",
            "Epoch 53 (train)3.6s: train loss = 3.37722 = (mf)1.59373 + (embed)1.78349\n",
            "Epoch 54 (train)3.6s: train loss = 3.21109 = (mf)1.41169 + (embed)1.79940\n",
            "Epoch 55 (train)4.4s + (eval)0.2s: train loss = 3.28101 = (mf)1.47364 + (embed)1.80738, recall = 0.20326, ndcg = 0.18338\n",
            "Epoch 56 (train)3.6s: train loss = 3.32322 = (mf)1.51210 + (embed)1.81112\n",
            "Epoch 57 (train)3.6s: train loss = 3.33638 = (mf)1.52027 + (embed)1.81611\n",
            "Epoch 58 (train)4.5s: train loss = 3.28325 = (mf)1.46518 + (embed)1.81807\n",
            "Epoch 59 (train)3.6s: train loss = 3.36548 = (mf)1.53891 + (embed)1.82656\n",
            "Epoch 60 (train)3.6s + (eval)0.3s: train loss = 3.20550 = (mf)1.36751 + (embed)1.83799, recall = 0.18974, ndcg = 0.17188\n",
            "Epoch 61 (train)4.5s: train loss = 3.26403 = (mf)1.42069 + (embed)1.84334\n",
            "Epoch 62 (train)3.6s: train loss = 3.31057 = (mf)1.46142 + (embed)1.84915\n",
            "Epoch 63 (train)3.6s: train loss = 3.42787 = (mf)1.57540 + (embed)1.85247\n",
            "Epoch 64 (train)4.2s: train loss = 3.20237 = (mf)1.33871 + (embed)1.86366\n",
            "Epoch 65 (train)3.9s + (eval)0.2s: train loss = 3.37164 = (mf)1.50713 + (embed)1.86452, recall = 0.20464, ndcg = 0.18610\n",
            "Epoch 66 (train)3.6s: train loss = 3.35616 = (mf)1.49207 + (embed)1.86408\n",
            "Epoch 67 (train)3.7s: train loss = 3.51884 = (mf)1.64340 + (embed)1.87545\n",
            "Epoch 68 (train)4.3s: train loss = 3.40311 = (mf)1.51739 + (embed)1.88572\n",
            "Epoch 69 (train)3.6s: train loss = 3.27729 = (mf)1.37891 + (embed)1.89837\n",
            "Epoch 70 (train)3.6s + (eval)0.7s: train loss = 3.28781 = (mf)1.38970 + (embed)1.89812, recall = 0.21193, ndcg = 0.19582\n",
            "Epoch 71 (train)4.4s: train loss = 3.23371 = (mf)1.33719 + (embed)1.89652\n",
            "Epoch 72 (train)3.6s: train loss = 3.40797 = (mf)1.51606 + (embed)1.89192\n",
            "Epoch 73 (train)3.6s: train loss = 3.35360 = (mf)1.45806 + (embed)1.89554\n",
            "Epoch 74 (train)4.4s: train loss = 3.43235 = (mf)1.52764 + (embed)1.90471\n",
            "Epoch 75 (train)3.6s + (eval)0.2s: train loss = 3.41417 = (mf)1.50452 + (embed)1.90965, recall = 0.20052, ndcg = 0.18343\n",
            "Epoch 76 (train)3.7s: train loss = 3.38717 = (mf)1.47821 + (embed)1.90896\n",
            "Epoch 77 (train)4.4s: train loss = 3.42638 = (mf)1.51204 + (embed)1.91434\n",
            "Epoch 78 (train)3.5s: train loss = 3.45567 = (mf)1.53217 + (embed)1.92349\n",
            "Epoch 79 (train)3.6s: train loss = 3.38860 = (mf)1.46572 + (embed)1.92289\n",
            "Epoch 80 (train)3.9s + (eval)0.4s: train loss = 3.29744 = (mf)1.36513 + (embed)1.93231, recall = 0.19615, ndcg = 0.17834\n",
            "Epoch 81 (train)3.8s: train loss = 3.31952 = (mf)1.38796 + (embed)1.93156\n",
            "Epoch 82 (train)3.5s: train loss = 3.37545 = (mf)1.44359 + (embed)1.93185\n",
            "Epoch 83 (train)3.5s: train loss = 3.41657 = (mf)1.47780 + (embed)1.93877\n",
            "Epoch 84 (train)4.4s: train loss = 3.29634 = (mf)1.35264 + (embed)1.94370\n",
            "Epoch 85 (train)3.5s + (eval)0.2s: train loss = 3.41339 = (mf)1.47062 + (embed)1.94277, recall = 0.20554, ndcg = 0.18356\n",
            "Epoch 86 (train)3.5s: train loss = 3.30616 = (mf)1.35746 + (embed)1.94870\n",
            "Epoch 87 (train)4.4s: train loss = 3.53609 = (mf)1.57544 + (embed)1.96064\n",
            "Epoch 88 (train)3.5s: train loss = 3.50827 = (mf)1.54844 + (embed)1.95983\n",
            "Epoch 89 (train)3.5s: train loss = 3.39485 = (mf)1.42358 + (embed)1.97127\n",
            "Epoch 90 (train)4.3s + (eval)0.4s: train loss = 3.40518 = (mf)1.43199 + (embed)1.97319, recall = 0.20040, ndcg = 0.18425\n",
            "Epoch 91 (train)3.5s: train loss = 3.37219 = (mf)1.40207 + (embed)1.97011\n",
            "Epoch 92 (train)3.5s: train loss = 3.45442 = (mf)1.47083 + (embed)1.98359\n",
            "Epoch 93 (train)3.8s: train loss = 3.50147 = (mf)1.52028 + (embed)1.98119\n",
            "Epoch 94 (train)4.1s: train loss = 3.42146 = (mf)1.43397 + (embed)1.98750\n",
            "Epoch 95 (train)3.6s + (eval)0.7s: train loss = 3.41427 = (mf)1.42649 + (embed)1.98777, recall = 0.21230, ndcg = 0.18909\n",
            "Epoch 96 (train)3.5s: train loss = 3.36803 = (mf)1.37756 + (embed)1.99047\n",
            "Epoch 97 (train)4.3s: train loss = 3.45201 = (mf)1.47288 + (embed)1.97913\n",
            "Epoch 98 (train)3.5s: train loss = 3.31947 = (mf)1.33490 + (embed)1.98457\n",
            "Epoch 99 (train)3.5s: train loss = 3.30202 = (mf)1.31780 + (embed)1.98422\n",
            "Epoch 100 (train)4.4s + (eval)0.2s: train loss = 3.46240 = (mf)1.48695 + (embed)1.97545, recall = 0.20765, ndcg = 0.19005\n",
            "Training finished in 394.67 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.19005, Recall@50: 0.20765\n",
            "--- Trying combination: Epochs=100, TopK=50, LearningRate=0.5, Layers=5 ---\n",
            "Already create adjacency matrix.\n",
            "Already normalize adjacency matrix.\n",
            "Using xavier initialization.\n",
            "Epoch 1 (train)6.2s: train loss = 0.34973 = (mf)0.30419 + (embed)0.04554\n",
            "Epoch 2 (train)4.1s: train loss = 0.59202 = (mf)0.44417 + (embed)0.14785\n",
            "Epoch 3 (train)4.9s: train loss = 0.84441 = (mf)0.58477 + (embed)0.25964\n",
            "Epoch 4 (train)4.2s: train loss = 1.08158 = (mf)0.72344 + (embed)0.35814\n",
            "Epoch 5 (train)4.2s + (eval)1.5s: train loss = 1.22913 = (mf)0.78081 + (embed)0.44832, recall = 0.17279, ndcg = 0.16557\n",
            "Epoch 6 (train)4.8s: train loss = 1.32811 = (mf)0.80596 + (embed)0.52216\n",
            "Epoch 7 (train)4.1s: train loss = 1.48722 = (mf)0.89321 + (embed)0.59401\n",
            "Epoch 8 (train)4.7s: train loss = 1.59080 = (mf)0.92945 + (embed)0.66135\n",
            "Epoch 9 (train)4.5s: train loss = 1.65024 = (mf)0.93108 + (embed)0.71916\n",
            "Epoch 10 (train)4.2s + (eval)0.7s: train loss = 1.73360 = (mf)0.96276 + (embed)0.77084, recall = 0.17408, ndcg = 0.16423\n",
            "Epoch 11 (train)5.0s: train loss = 1.84582 = (mf)1.02988 + (embed)0.81593\n",
            "Epoch 12 (train)4.1s: train loss = 1.90636 = (mf)1.04509 + (embed)0.86127\n",
            "Epoch 13 (train)4.2s: train loss = 2.01067 = (mf)1.10934 + (embed)0.90132\n",
            "Epoch 14 (train)4.9s: train loss = 2.00466 = (mf)1.06196 + (embed)0.94270\n",
            "Epoch 15 (train)4.1s + (eval)0.2s: train loss = 2.03209 = (mf)1.05867 + (embed)0.97342, recall = 0.19920, ndcg = 0.18438\n",
            "Epoch 16 (train)4.2s: train loss = 2.05245 = (mf)1.05225 + (embed)1.00020\n",
            "Epoch 17 (train)4.9s: train loss = 2.06326 = (mf)1.04322 + (embed)1.02005\n",
            "Epoch 18 (train)4.1s: train loss = 2.07386 = (mf)1.03348 + (embed)1.04038\n",
            "Epoch 19 (train)4.3s: train loss = 2.11709 = (mf)1.05343 + (embed)1.06366\n",
            "Epoch 20 (train)4.7s + (eval)0.3s: train loss = 2.20690 = (mf)1.11073 + (embed)1.09618, recall = 0.20861, ndcg = 0.19436\n",
            "Epoch 21 (train)4.1s: train loss = 2.22770 = (mf)1.10618 + (embed)1.12152\n",
            "Epoch 22 (train)4.8s: train loss = 2.28050 = (mf)1.13882 + (embed)1.14168\n",
            "Epoch 23 (train)4.3s: train loss = 2.23935 = (mf)1.08327 + (embed)1.15608\n",
            "Epoch 24 (train)4.1s: train loss = 2.21048 = (mf)1.04129 + (embed)1.16919\n",
            "Epoch 25 (train)4.9s + (eval)0.2s: train loss = 2.20044 = (mf)1.01670 + (embed)1.18374, recall = 0.19922, ndcg = 0.18184\n",
            "Epoch 26 (train)4.1s: train loss = 2.22775 = (mf)1.03023 + (embed)1.19752\n",
            "Epoch 27 (train)4.1s: train loss = 2.27358 = (mf)1.06456 + (embed)1.20902\n",
            "Epoch 28 (train)4.9s: train loss = 2.20973 = (mf)0.99134 + (embed)1.21839\n",
            "Epoch 29 (train)4.1s: train loss = 2.12000 = (mf)0.88952 + (embed)1.23048\n",
            "Epoch 30 (train)4.1s + (eval)0.7s: train loss = 2.30512 = (mf)1.06643 + (embed)1.23869, recall = 0.19927, ndcg = 0.17838\n",
            "Epoch 31 (train)4.9s: train loss = 2.35258 = (mf)1.09793 + (embed)1.25465\n",
            "Epoch 32 (train)4.1s: train loss = 2.28614 = (mf)1.01668 + (embed)1.26946\n",
            "Epoch 33 (train)4.3s: train loss = 2.40786 = (mf)1.12384 + (embed)1.28402\n",
            "Epoch 34 (train)4.7s: train loss = 2.40120 = (mf)1.09872 + (embed)1.30247\n",
            "Epoch 35 (train)4.2s + (eval)0.2s: train loss = 2.38480 = (mf)1.06781 + (embed)1.31700, recall = 0.18556, ndcg = 0.16772\n",
            "Epoch 36 (train)4.8s: train loss = 2.38574 = (mf)1.06151 + (embed)1.32423\n",
            "Epoch 37 (train)4.3s: train loss = 2.45177 = (mf)1.11386 + (embed)1.33792\n",
            "Epoch 38 (train)4.1s: train loss = 2.32721 = (mf)0.98098 + (embed)1.34624\n",
            "Epoch 39 (train)4.9s: train loss = 2.39725 = (mf)1.04324 + (embed)1.35400\n",
            "Epoch 40 (train)4.1s + (eval)0.2s: train loss = 2.39329 = (mf)1.03156 + (embed)1.36173, recall = 0.18356, ndcg = 0.16386\n",
            "Epoch 41 (train)4.1s: train loss = 2.46417 = (mf)1.09541 + (embed)1.36875\n",
            "Epoch 42 (train)5.0s: train loss = 2.33965 = (mf)0.96542 + (embed)1.37423\n",
            "Epoch 43 (train)4.2s: train loss = 2.48171 = (mf)1.10365 + (embed)1.37805\n",
            "Epoch 44 (train)4.2s: train loss = 2.36411 = (mf)0.98392 + (embed)1.38019\n",
            "Epoch 45 (train)5.0s + (eval)0.2s: train loss = 2.41754 = (mf)1.03060 + (embed)1.38694, recall = 0.20427, ndcg = 0.18076\n",
            "Epoch 46 (train)4.2s: train loss = 2.37362 = (mf)0.98365 + (embed)1.38998\n",
            "Epoch 47 (train)4.4s: train loss = 2.46613 = (mf)1.06917 + (embed)1.39696\n",
            "Epoch 48 (train)4.9s: train loss = 2.34724 = (mf)0.95323 + (embed)1.39401\n",
            "Epoch 49 (train)4.2s: train loss = 2.36528 = (mf)0.96780 + (embed)1.39748\n",
            "Epoch 50 (train)4.7s + (eval)0.9s: train loss = 2.38454 = (mf)0.98166 + (embed)1.40288, recall = 0.21347, ndcg = 0.19668\n",
            "Epoch 51 (train)4.1s: train loss = 2.38135 = (mf)0.97579 + (embed)1.40555\n",
            "Epoch 52 (train)4.1s: train loss = 2.34741 = (mf)0.94782 + (embed)1.39959\n",
            "Epoch 53 (train)5.0s: train loss = 2.39718 = (mf)0.99863 + (embed)1.39854\n",
            "Epoch 54 (train)4.1s: train loss = 2.39864 = (mf)0.99711 + (embed)1.40153\n",
            "Epoch 55 (train)4.1s + (eval)0.2s: train loss = 2.42314 = (mf)1.01832 + (embed)1.40482, recall = 0.20896, ndcg = 0.18767\n",
            "Epoch 56 (train)5.0s: train loss = 2.43298 = (mf)1.02560 + (embed)1.40738\n",
            "Epoch 57 (train)4.2s: train loss = 2.33731 = (mf)0.92499 + (embed)1.41232\n",
            "Epoch 58 (train)4.2s: train loss = 2.46477 = (mf)1.04363 + (embed)1.42113\n",
            "Epoch 59 (train)5.0s: train loss = 2.32152 = (mf)0.89669 + (embed)1.42483\n",
            "Epoch 60 (train)4.2s + (eval)0.3s: train loss = 2.29520 = (mf)0.86338 + (embed)1.43182, recall = 0.21276, ndcg = 0.19318\n",
            "Epoch 61 (train)4.5s: train loss = 2.46994 = (mf)1.03802 + (embed)1.43191\n",
            "Epoch 62 (train)4.6s: train loss = 2.42522 = (mf)0.98940 + (embed)1.43582\n",
            "Epoch 63 (train)4.1s: train loss = 2.34702 = (mf)0.90200 + (embed)1.44502\n",
            "Epoch 64 (train)4.9s: train loss = 2.35939 = (mf)0.91289 + (embed)1.44650\n",
            "Epoch 65 (train)4.3s + (eval)0.3s: train loss = 2.33588 = (mf)0.89338 + (embed)1.44250, recall = 0.19735, ndcg = 0.17830\n",
            "Epoch 66 (train)4.1s: train loss = 2.42748 = (mf)0.98022 + (embed)1.44727\n",
            "Epoch 67 (train)4.9s: train loss = 2.41508 = (mf)0.96643 + (embed)1.44865\n",
            "Epoch 68 (train)4.1s: train loss = 2.41292 = (mf)0.95513 + (embed)1.45779\n",
            "Epoch 69 (train)4.2s: train loss = 2.39180 = (mf)0.92889 + (embed)1.46291\n",
            "Epoch 70 (train)5.1s + (eval)0.2s: train loss = 2.37799 = (mf)0.91211 + (embed)1.46588, recall = 0.19643, ndcg = 0.17282\n",
            "Epoch 71 (train)4.2s: train loss = 2.39947 = (mf)0.93680 + (embed)1.46267\n",
            "Epoch 72 (train)4.2s: train loss = 2.42188 = (mf)0.95376 + (embed)1.46812\n",
            "Epoch 73 (train)5.0s: train loss = 2.45185 = (mf)0.98295 + (embed)1.46891\n",
            "Epoch 74 (train)4.1s: train loss = 2.46867 = (mf)1.00135 + (embed)1.46732\n",
            "Epoch 75 (train)4.5s + (eval)0.9s: train loss = 2.38413 = (mf)0.91678 + (embed)1.46735, recall = 0.20655, ndcg = 0.18202\n",
            "Epoch 76 (train)4.3s: train loss = 2.37079 = (mf)0.90085 + (embed)1.46994\n",
            "Epoch 77 (train)4.2s: train loss = 2.44583 = (mf)0.96796 + (embed)1.47787\n",
            "Epoch 78 (train)4.9s: train loss = 2.44353 = (mf)0.96240 + (embed)1.48112\n",
            "Epoch 79 (train)4.2s: train loss = 2.45402 = (mf)0.97500 + (embed)1.47902\n",
            "Epoch 80 (train)4.2s + (eval)0.2s: train loss = 2.38319 = (mf)0.90085 + (embed)1.48235, recall = 0.21420, ndcg = 0.18646\n",
            "Epoch 81 (train)5.0s: train loss = 2.40152 = (mf)0.92349 + (embed)1.47803\n",
            "Epoch 82 (train)4.2s: train loss = 2.42980 = (mf)0.95161 + (embed)1.47820\n",
            "Epoch 83 (train)4.2s: train loss = 2.37515 = (mf)0.89284 + (embed)1.48231\n",
            "Epoch 84 (train)5.0s: train loss = 2.46460 = (mf)0.98648 + (embed)1.47812\n",
            "Epoch 85 (train)4.2s + (eval)0.3s: train loss = 2.50690 = (mf)1.02777 + (embed)1.47913, recall = 0.20679, ndcg = 0.18203\n",
            "Epoch 86 (train)4.5s: train loss = 2.41350 = (mf)0.93532 + (embed)1.47817\n",
            "Epoch 87 (train)4.7s: train loss = 2.42231 = (mf)0.94151 + (embed)1.48079\n",
            "Epoch 88 (train)4.4s: train loss = 2.41531 = (mf)0.93513 + (embed)1.48018\n",
            "Epoch 89 (train)5.1s: train loss = 2.40066 = (mf)0.92422 + (embed)1.47644\n",
            "Epoch 90 (train)4.2s + (eval)0.2s: train loss = 2.36773 = (mf)0.89320 + (embed)1.47453, recall = 0.21803, ndcg = 0.19803\n",
            "Epoch 91 (train)4.2s: train loss = 2.33292 = (mf)0.86150 + (embed)1.47142\n",
            "Epoch 92 (train)5.0s: train loss = 2.43130 = (mf)0.96045 + (embed)1.47085\n",
            "Epoch 93 (train)4.3s: train loss = 2.42809 = (mf)0.96658 + (embed)1.46150\n",
            "Epoch 94 (train)4.3s: train loss = 2.38997 = (mf)0.93337 + (embed)1.45660\n",
            "Epoch 95 (train)5.1s + (eval)0.7s: train loss = 2.38032 = (mf)0.91590 + (embed)1.46443, recall = 0.21438, ndcg = 0.19270\n",
            "Epoch 96 (train)4.3s: train loss = 2.33401 = (mf)0.86626 + (embed)1.46776\n",
            "Epoch 97 (train)4.6s: train loss = 2.44221 = (mf)0.96875 + (embed)1.47346\n",
            "Epoch 98 (train)4.5s: train loss = 2.35661 = (mf)0.88025 + (embed)1.47636\n",
            "Epoch 99 (train)4.2s: train loss = 2.36197 = (mf)0.88381 + (embed)1.47817\n",
            "Epoch 100 (train)4.9s + (eval)0.3s: train loss = 2.39387 = (mf)0.92204 + (embed)1.47183, recall = 0.17857, ndcg = 0.15304\n",
            "Training finished in 455.43 seconds.\n",
            "Evaluation results for this combination: NDCG@50: 0.15304, Recall@50: 0.17857\n",
            "\n",
            "===== Hyperparameter Tuning Summary =====\n",
            "Epochs=25_TopK=10_LR=0.005_Layers=3: NDCG=0.30806, Recall=0.12771, TrainTime=79.58s\n",
            "Epochs=25_TopK=10_LR=0.005_Layers=4: NDCG=0.30374, Recall=0.12191, TrainTime=93.55s\n",
            "Epochs=25_TopK=10_LR=0.005_Layers=5: NDCG=0.28401, Recall=0.11195, TrainTime=114.17s\n",
            "Epochs=25_TopK=10_LR=0.05_Layers=3: NDCG=0.23464, Recall=0.09822, TrainTime=80.12s\n",
            "Epochs=25_TopK=10_LR=0.05_Layers=4: NDCG=0.26803, Recall=0.11576, TrainTime=96.24s\n",
            "Epochs=25_TopK=10_LR=0.05_Layers=5: NDCG=0.29538, Recall=0.12626, TrainTime=113.12s\n",
            "Epochs=25_TopK=10_LR=0.5_Layers=3: NDCG=0.17540, Recall=0.06688, TrainTime=81.57s\n",
            "Epochs=25_TopK=10_LR=0.5_Layers=4: NDCG=0.16227, Recall=0.06389, TrainTime=97.13s\n",
            "Epochs=25_TopK=10_LR=0.5_Layers=5: NDCG=0.15081, Recall=0.05915, TrainTime=110.58s\n",
            "Epochs=25_TopK=20_LR=0.005_Layers=3: NDCG=0.29306, Recall=0.19078, TrainTime=78.73s\n",
            "Epochs=25_TopK=20_LR=0.005_Layers=4: NDCG=0.28353, Recall=0.18090, TrainTime=95.80s\n",
            "Epochs=25_TopK=20_LR=0.005_Layers=5: NDCG=0.27136, Recall=0.17164, TrainTime=112.62s\n",
            "Epochs=25_TopK=20_LR=0.05_Layers=3: NDCG=0.22852, Recall=0.15599, TrainTime=80.04s\n",
            "Epochs=25_TopK=20_LR=0.05_Layers=4: NDCG=0.25638, Recall=0.17744, TrainTime=96.13s\n",
            "Epochs=25_TopK=20_LR=0.05_Layers=5: NDCG=0.28961, Recall=0.20159, TrainTime=111.16s\n",
            "Epochs=25_TopK=20_LR=0.5_Layers=3: NDCG=0.17950, Recall=0.11597, TrainTime=80.86s\n",
            "Epochs=25_TopK=20_LR=0.5_Layers=4: NDCG=0.16083, Recall=0.10255, TrainTime=96.94s\n",
            "Epochs=25_TopK=20_LR=0.5_Layers=5: NDCG=0.17018, Recall=0.11463, TrainTime=111.34s\n",
            "Epochs=25_TopK=50_LR=0.005_Layers=3: NDCG=0.32382, Recall=0.32402, TrainTime=80.00s\n",
            "Epochs=25_TopK=50_LR=0.005_Layers=4: NDCG=0.31098, Recall=0.31367, TrainTime=96.11s\n",
            "Epochs=25_TopK=50_LR=0.005_Layers=5: NDCG=0.29350, Recall=0.29744, TrainTime=111.05s\n",
            "Epochs=25_TopK=50_LR=0.05_Layers=3: NDCG=0.25440, Recall=0.26899, TrainTime=80.75s\n",
            "Epochs=25_TopK=50_LR=0.05_Layers=4: NDCG=0.27888, Recall=0.29653, TrainTime=95.32s\n",
            "Epochs=25_TopK=50_LR=0.05_Layers=5: NDCG=0.30670, Recall=0.32293, TrainTime=114.94s\n",
            "Epochs=25_TopK=50_LR=0.5_Layers=3: NDCG=0.20119, Recall=0.21425, TrainTime=81.27s\n",
            "Epochs=25_TopK=50_LR=0.5_Layers=4: NDCG=0.19238, Recall=0.19633, TrainTime=97.34s\n",
            "Epochs=25_TopK=50_LR=0.5_Layers=5: NDCG=0.18467, Recall=0.20011, TrainTime=111.75s\n",
            "Epochs=50_TopK=10_LR=0.005_Layers=3: NDCG=0.33644, Recall=0.13652, TrainTime=160.00s\n",
            "Epochs=50_TopK=10_LR=0.005_Layers=4: NDCG=0.34517, Recall=0.14272, TrainTime=189.78s\n",
            "Epochs=50_TopK=10_LR=0.005_Layers=5: NDCG=0.33520, Recall=0.13744, TrainTime=219.63s\n",
            "Epochs=50_TopK=10_LR=0.05_Layers=3: NDCG=0.21913, Recall=0.09439, TrainTime=160.90s\n",
            "Epochs=50_TopK=10_LR=0.05_Layers=4: NDCG=0.25399, Recall=0.11299, TrainTime=189.79s\n",
            "Epochs=50_TopK=10_LR=0.05_Layers=5: NDCG=0.26352, Recall=0.11745, TrainTime=222.36s\n",
            "Epochs=50_TopK=10_LR=0.5_Layers=3: NDCG=0.16721, Recall=0.06714, TrainTime=160.55s\n",
            "Epochs=50_TopK=10_LR=0.5_Layers=4: NDCG=0.17884, Recall=0.06911, TrainTime=190.97s\n",
            "Epochs=50_TopK=10_LR=0.5_Layers=5: NDCG=0.15371, Recall=0.05933, TrainTime=223.54s\n",
            "Epochs=50_TopK=20_LR=0.005_Layers=3: NDCG=0.33967, Recall=0.22460, TrainTime=163.97s\n",
            "Epochs=50_TopK=20_LR=0.005_Layers=4: NDCG=0.32928, Recall=0.21602, TrainTime=194.71s\n",
            "Epochs=50_TopK=20_LR=0.005_Layers=5: NDCG=0.32889, Recall=0.21072, TrainTime=227.03s\n",
            "Epochs=50_TopK=20_LR=0.05_Layers=3: NDCG=0.20895, Recall=0.14761, TrainTime=160.91s\n",
            "Epochs=50_TopK=20_LR=0.05_Layers=4: NDCG=0.24919, Recall=0.18006, TrainTime=190.04s\n",
            "Epochs=50_TopK=20_LR=0.05_Layers=5: NDCG=0.28521, Recall=0.19939, TrainTime=221.35s\n",
            "Epochs=50_TopK=20_LR=0.5_Layers=3: NDCG=0.16916, Recall=0.11713, TrainTime=167.58s\n",
            "Epochs=50_TopK=20_LR=0.5_Layers=4: NDCG=0.16787, Recall=0.10470, TrainTime=194.68s\n",
            "Epochs=50_TopK=20_LR=0.5_Layers=5: NDCG=0.16239, Recall=0.11288, TrainTime=224.07s\n",
            "Epochs=50_TopK=50_LR=0.005_Layers=3: NDCG=0.34917, Recall=0.35275, TrainTime=164.87s\n",
            "Epochs=50_TopK=50_LR=0.005_Layers=4: NDCG=0.34956, Recall=0.34589, TrainTime=195.26s\n",
            "Epochs=50_TopK=50_LR=0.005_Layers=5: NDCG=0.34061, Recall=0.33589, TrainTime=226.10s\n",
            "Epochs=50_TopK=50_LR=0.05_Layers=3: NDCG=0.23604, Recall=0.25733, TrainTime=165.18s\n",
            "Epochs=50_TopK=50_LR=0.05_Layers=4: NDCG=0.27868, Recall=0.29356, TrainTime=195.90s\n",
            "Epochs=50_TopK=50_LR=0.05_Layers=5: NDCG=0.30381, Recall=0.31466, TrainTime=227.04s\n",
            "Epochs=50_TopK=50_LR=0.5_Layers=3: NDCG=0.19736, Recall=0.21100, TrainTime=162.59s\n",
            "Epochs=50_TopK=50_LR=0.5_Layers=4: NDCG=0.19091, Recall=0.21052, TrainTime=192.85s\n",
            "Epochs=50_TopK=50_LR=0.5_Layers=5: NDCG=0.19102, Recall=0.20760, TrainTime=226.61s\n",
            "Epochs=100_TopK=10_LR=0.005_Layers=3: NDCG=0.33575, Recall=0.14265, TrainTime=323.68s\n",
            "Epochs=100_TopK=10_LR=0.005_Layers=4: NDCG=0.34077, Recall=0.14263, TrainTime=383.00s\n",
            "Epochs=100_TopK=10_LR=0.005_Layers=5: NDCG=0.35768, Recall=0.14723, TrainTime=446.96s\n",
            "Epochs=100_TopK=10_LR=0.05_Layers=3: NDCG=0.21611, Recall=0.09183, TrainTime=323.37s\n",
            "Epochs=100_TopK=10_LR=0.05_Layers=4: NDCG=0.24070, Recall=0.10285, TrainTime=384.41s\n",
            "Epochs=100_TopK=10_LR=0.05_Layers=5: NDCG=0.26850, Recall=0.11568, TrainTime=443.88s\n",
            "Epochs=100_TopK=10_LR=0.5_Layers=3: NDCG=0.17146, Recall=0.07174, TrainTime=322.44s\n",
            "Epochs=100_TopK=10_LR=0.5_Layers=4: NDCG=0.14221, Recall=0.05581, TrainTime=389.08s\n",
            "Epochs=100_TopK=10_LR=0.5_Layers=5: NDCG=0.14152, Recall=0.05681, TrainTime=447.52s\n",
            "Epochs=100_TopK=20_LR=0.005_Layers=3: NDCG=0.33565, Recall=0.22394, TrainTime=348.91s\n",
            "Epochs=100_TopK=20_LR=0.005_Layers=4: NDCG=0.33632, Recall=0.22555, TrainTime=390.03s\n",
            "Epochs=100_TopK=20_LR=0.005_Layers=5: NDCG=0.34202, Recall=0.22492, TrainTime=451.46s\n",
            "Epochs=100_TopK=20_LR=0.05_Layers=3: NDCG=0.20429, Recall=0.14454, TrainTime=327.86s\n",
            "Epochs=100_TopK=20_LR=0.05_Layers=4: NDCG=0.24735, Recall=0.18013, TrainTime=389.79s\n",
            "Epochs=100_TopK=20_LR=0.05_Layers=5: NDCG=0.26070, Recall=0.18836, TrainTime=452.34s\n",
            "Epochs=100_TopK=20_LR=0.5_Layers=3: NDCG=0.15253, Recall=0.10382, TrainTime=327.37s\n",
            "Epochs=100_TopK=20_LR=0.5_Layers=4: NDCG=0.16058, Recall=0.11254, TrainTime=387.97s\n",
            "Epochs=100_TopK=20_LR=0.5_Layers=5: NDCG=0.13977, Recall=0.09561, TrainTime=449.80s\n",
            "Epochs=100_TopK=50_LR=0.005_Layers=3: NDCG=0.34929, Recall=0.35445, TrainTime=332.16s\n",
            "Epochs=100_TopK=50_LR=0.005_Layers=4: NDCG=0.35889, Recall=0.35932, TrainTime=392.09s\n",
            "Epochs=100_TopK=50_LR=0.005_Layers=5: NDCG=0.35971, Recall=0.35957, TrainTime=449.98s\n",
            "Epochs=100_TopK=50_LR=0.05_Layers=3: NDCG=0.22850, Recall=0.25045, TrainTime=331.77s\n",
            "Epochs=100_TopK=50_LR=0.05_Layers=4: NDCG=0.27478, Recall=0.29327, TrainTime=391.04s\n",
            "Epochs=100_TopK=50_LR=0.05_Layers=5: NDCG=0.29452, Recall=0.30995, TrainTime=455.51s\n",
            "Epochs=100_TopK=50_LR=0.5_Layers=3: NDCG=0.19274, Recall=0.20925, TrainTime=332.38s\n",
            "Epochs=100_TopK=50_LR=0.5_Layers=4: NDCG=0.19005, Recall=0.20765, TrainTime=394.67s\n",
            "Epochs=100_TopK=50_LR=0.5_Layers=5: NDCG=0.15304, Recall=0.17857, TrainTime=455.43s\n",
            "\n",
            "Best combination based on NDCG: Epochs=100_TopK=50_LR=0.005_Layers=5\n",
            "NDCG: 0.35971\n",
            "Recall: 0.35957\n"
          ]
        }
      ]
    }
  ]
}
